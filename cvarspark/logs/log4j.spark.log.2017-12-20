17/12/20 16:28:28 INFO SparkContext: Running Spark version 2.1.0
17/12/20 16:28:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/12/20 16:28:28 INFO SecurityManager: Changing view acls to: conan
17/12/20 16:28:28 INFO SecurityManager: Changing modify acls to: conan
17/12/20 16:28:28 INFO SecurityManager: Changing view acls groups to: 
17/12/20 16:28:28 INFO SecurityManager: Changing modify acls groups to: 
17/12/20 16:28:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(conan); groups with view permissions: Set(); users  with modify permissions: Set(conan); groups with modify permissions: Set()
17/12/20 16:28:29 INFO Utils: Successfully started service 'sparkDriver' on port 57757.
17/12/20 16:28:29 INFO SparkEnv: Registering MapOutputTracker
17/12/20 16:28:29 INFO SparkEnv: Registering BlockManagerMaster
17/12/20 16:28:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/12/20 16:28:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/12/20 16:28:29 INFO DiskBlockManager: Created local directory at C:\Users\conan\AppData\Local\Temp\blockmgr-5511a3c8-2b1e-4631-b801-6b85811a57e5
17/12/20 16:28:29 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
17/12/20 16:28:29 INFO SparkEnv: Registering OutputCommitCoordinator
17/12/20 16:28:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/12/20 16:28:29 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
17/12/20 16:28:29 INFO SparkContext: Added JAR file:/C:/Users/conan/Documents/R/win-library/3.4/sparklyr/java/sparklyr-2.1-2.11.jar at spark://127.0.0.1:57757/jars/sparklyr-2.1-2.11.jar with timestamp 1513787309419
17/12/20 16:28:29 INFO Executor: Starting executor ID driver on host localhost
17/12/20 16:28:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57778.
17/12/20 16:28:29 INFO NettyBlockTransferService: Server created on 127.0.0.1:57778
17/12/20 16:28:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/12/20 16:28:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57778, None)
17/12/20 16:28:29 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:57778 with 2004.6 MB RAM, BlockManagerId(driver, 127.0.0.1, 57778, None)
17/12/20 16:28:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57778, None)
17/12/20 16:28:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 57778, None)
17/12/20 16:28:30 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
17/12/20 16:28:30 INFO SharedState: Warehouse path is 'C:UsersconanAppDataLocalsparkspark-2.1.0-bin-hadoop2.7	mphive'.
17/12/20 16:28:30 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/12/20 16:28:31 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/12/20 16:28:31 INFO ObjectStore: ObjectStore, initialize called
17/12/20 16:28:31 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/12/20 16:28:31 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/12/20 16:28:32 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/12/20 16:28:33 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 16:28:33 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 16:28:34 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 16:28:34 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 16:28:34 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/12/20 16:28:34 INFO ObjectStore: Initialized ObjectStore
17/12/20 16:28:34 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/12/20 16:28:35 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/12/20 16:28:35 INFO HiveMetaStore: Added admin role in metastore
17/12/20 16:28:35 INFO HiveMetaStore: Added public role in metastore
17/12/20 16:28:36 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/12/20 16:28:36 INFO HiveMetaStore: 0: get_all_databases
17/12/20 16:28:36 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_all_databases	
17/12/20 16:28:36 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/12/20 16:28:36 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/12/20 16:28:36 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 16:28:36 INFO SessionState: Created local directory: C:/Users/conan/AppData/Local/Temp/27965f60-2048-4451-b6e1-81a9fb478aa9_resources
17/12/20 16:28:36 INFO SessionState: Created HDFS directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/conan/27965f60-2048-4451-b6e1-81a9fb478aa9
17/12/20 16:28:36 INFO SessionState: Created local directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/27965f60-2048-4451-b6e1-81a9fb478aa9
17/12/20 16:28:36 INFO SessionState: Created HDFS directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/conan/27965f60-2048-4451-b6e1-81a9fb478aa9/_tmp_space.db
17/12/20 16:28:36 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:UsersconanAppDataLocalsparkspark-2.1.0-bin-hadoop2.7	mphive
17/12/20 16:28:37 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:37 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:37 INFO HiveMetaStore: 0: get_database: global_temp
17/12/20 16:28:37 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: global_temp	
17/12/20 16:28:37 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
17/12/20 16:28:37 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:28:41 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:41 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:41 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:41 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:41 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:28:41 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:28:43 INFO CodeGenerator: Code generated in 559.739952 ms
17/12/20 16:28:43 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 16:28:43 INFO DAGScheduler: Got job 0 (collect at utils.scala:58) with 1 output partitions
17/12/20 16:28:43 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:58)
17/12/20 16:28:43 INFO DAGScheduler: Parents of final stage: List()
17/12/20 16:28:43 INFO DAGScheduler: Missing parents: List()
17/12/20 16:28:44 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at map at utils.scala:55), which has no missing parents
17/12/20 16:28:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 8.7 KB, free 2004.6 MB)
17/12/20 16:28:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2004.6 MB)
17/12/20 16:28:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:57778 (size: 4.6 KB, free: 2004.6 MB)
17/12/20 16:28:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996
17/12/20 16:28:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at map at utils.scala:55)
17/12/20 16:28:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/12/20 16:28:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6041 bytes)
17/12/20 16:28:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/12/20 16:28:46 INFO Executor: Fetching spark://127.0.0.1:57757/jars/sparklyr-2.1-2.11.jar with timestamp 1513787309419
17/12/20 16:28:46 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:57757 after 43 ms (0 ms spent in bootstraps)
17/12/20 16:28:46 INFO Utils: Fetching spark://127.0.0.1:57757/jars/sparklyr-2.1-2.11.jar to C:\Users\conan\AppData\Local\Temp\spark-c9f66a4c-d291-401e-b364-ad360d69d99e\userFiles-df266b3f-a103-4069-88e4-2af02b2fa0ff\fetchFileTemp6636624866208511275.tmp
17/12/20 16:28:46 INFO Executor: Adding file:/C:/Users/conan/AppData/Local/Temp/spark-c9f66a4c-d291-401e-b364-ad360d69d99e/userFiles-df266b3f-a103-4069-88e4-2af02b2fa0ff/sparklyr-2.1-2.11.jar to class loader
17/12/20 16:28:47 INFO CodeGenerator: Code generated in 22.094919 ms
17/12/20 16:28:47 INFO CodeGenerator: Code generated in 17.819945 ms
17/12/20 16:28:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1318 bytes result sent to driver
17/12/20 16:28:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1211 ms on localhost (executor driver) (1/1)
17/12/20 16:28:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/12/20 16:28:47 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:58) finished in 1.293 s
17/12/20 16:28:47 INFO DAGScheduler: Job 0 finished: collect at utils.scala:58, took 3.504579 s
17/12/20 16:28:47 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:48 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:28:48 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:48 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 16:28:48 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 16:28:48 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 16:28:48 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 16:28:48 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 16:28:48 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 16:28:48 INFO CodeGenerator: Code generated in 9.100839 ms
17/12/20 16:28:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 293.7 KB, free 2004.3 MB)
17/12/20 16:28:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2004.3 MB)
17/12/20 16:28:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57778 (size: 24.0 KB, free: 2004.6 MB)
17/12/20 16:28:49 INFO SparkContext: Created broadcast 1 from sql at <unknown>:0
17/12/20 16:28:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 16:28:49 INFO CodeGenerator: Code generated in 36.706178 ms
17/12/20 16:28:49 INFO CodeGenerator: Code generated in 19.701432 ms
17/12/20 16:28:49 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 16:28:49 INFO DAGScheduler: Registering RDD 9 (sql at <unknown>:0)
17/12/20 16:28:49 INFO DAGScheduler: Registering RDD 14 (sql at <unknown>:0)
17/12/20 16:28:49 INFO DAGScheduler: Got job 1 (sql at <unknown>:0) with 1 output partitions
17/12/20 16:28:49 INFO DAGScheduler: Final stage: ResultStage 3 (sql at <unknown>:0)
17/12/20 16:28:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
17/12/20 16:28:49 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
17/12/20 16:28:49 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[9] at sql at <unknown>:0), which has no missing parents
17/12/20 16:28:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.1 KB, free 2004.3 MB)
17/12/20 16:28:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2004.3 MB)
17/12/20 16:28:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:57778 (size: 7.5 KB, free: 2004.6 MB)
17/12/20 16:28:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/12/20 16:28:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[9] at sql at <unknown>:0)
17/12/20 16:28:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/12/20 16:28:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 6676 bytes)
17/12/20 16:28:49 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/12/20 16:28:49 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/RtmpYBQ1ot/spark_serialize_1c858b11ab1ba98a083c00616fb0f69d37780e348a5fcdb957644271af826373.csv, range: 0-1940, partition values: [empty row]
17/12/20 16:28:50 INFO CodeGenerator: Code generated in 24.255816 ms
17/12/20 16:28:50 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1809 bytes result sent to driver
17/12/20 16:28:50 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 677 ms on localhost (executor driver) (1/1)
17/12/20 16:28:50 INFO DAGScheduler: ShuffleMapStage 1 (sql at <unknown>:0) finished in 0.677 s
17/12/20 16:28:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/12/20 16:28:50 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:28:50 INFO DAGScheduler: running: Set()
17/12/20 16:28:50 INFO DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)
17/12/20 16:28:50 INFO DAGScheduler: failed: Set()
17/12/20 16:28:50 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[14] at sql at <unknown>:0), which has no missing parents
17/12/20 16:28:50 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.3 KB, free 2004.2 MB)
17/12/20 16:28:50 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2004.2 MB)
17/12/20 16:28:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:57778 (size: 6.9 KB, free: 2004.6 MB)
17/12/20 16:28:50 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/12/20 16:28:50 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[14] at sql at <unknown>:0)
17/12/20 16:28:50 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
17/12/20 16:28:50 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5942 bytes)
17/12/20 16:28:50 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3, localhost, executor driver, partition 1, ANY, 5942 bytes)
17/12/20 16:28:50 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/12/20 16:28:50 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)
17/12/20 16:28:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 16:28:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 16:28:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 28 ms
17/12/20 16:28:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 28 ms
17/12/20 16:28:50 INFO MemoryStore: Block rdd_11_0 stored as values in memory (estimated size 1512.0 B, free 2004.2 MB)
17/12/20 16:28:50 INFO MemoryStore: Block rdd_11_1 stored as values in memory (estimated size 1512.0 B, free 2004.2 MB)
17/12/20 16:28:50 INFO BlockManagerInfo: Added rdd_11_0 in memory on 127.0.0.1:57778 (size: 1512.0 B, free: 2004.6 MB)
17/12/20 16:28:50 INFO BlockManagerInfo: Added rdd_11_1 in memory on 127.0.0.1:57778 (size: 1512.0 B, free: 2004.6 MB)
17/12/20 16:28:50 INFO CodeGenerator: Code generated in 13.268203 ms
17/12/20 16:28:51 INFO CodeGenerator: Code generated in 53.393001 ms
17/12/20 16:28:51 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 3151 bytes result sent to driver
17/12/20 16:28:51 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3064 bytes result sent to driver
17/12/20 16:28:51 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 440 ms on localhost (executor driver) (1/2)
17/12/20 16:28:51 INFO DAGScheduler: ShuffleMapStage 2 (sql at <unknown>:0) finished in 0.440 s
17/12/20 16:28:51 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:28:51 INFO DAGScheduler: running: Set()
17/12/20 16:28:51 INFO DAGScheduler: waiting: Set(ResultStage 3)
17/12/20 16:28:51 INFO DAGScheduler: failed: Set()
17/12/20 16:28:51 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at sql at <unknown>:0), which has no missing parents
17/12/20 16:28:51 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 2004.2 MB)
17/12/20 16:28:51 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2004.2 MB)
17/12/20 16:28:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:57778 (size: 3.7 KB, free: 2004.6 MB)
17/12/20 16:28:51 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:996
17/12/20 16:28:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at sql at <unknown>:0)
17/12/20 16:28:51 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/12/20 16:28:51 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 440 ms on localhost (executor driver) (2/2)
17/12/20 16:28:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/12/20 16:28:51 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4, localhost, executor driver, partition 0, ANY, 5953 bytes)
17/12/20 16:28:51 INFO Executor: Running task 0.0 in stage 3.0 (TID 4)
17/12/20 16:28:51 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 16:28:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 16:28:51 INFO Executor: Finished task 0.0 in stage 3.0 (TID 4). 1952 bytes result sent to driver
17/12/20 16:28:51 INFO DAGScheduler: ResultStage 3 (sql at <unknown>:0) finished in 0.015 s
17/12/20 16:28:51 INFO DAGScheduler: Job 1 finished: sql at <unknown>:0, took 1.326795 s
17/12/20 16:28:51 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 15 ms on localhost (executor driver) (1/1)
17/12/20 16:28:51 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/12/20 16:28:51 INFO CodeGenerator: Code generated in 10.815058 ms
17/12/20 16:28:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:51 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 16:28:51 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 16:28:51 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 145 bytes
17/12/20 16:28:51 INFO DAGScheduler: Registering RDD 21 (collect at utils.scala:196)
17/12/20 16:28:51 INFO DAGScheduler: Got job 2 (collect at utils.scala:196) with 1 output partitions
17/12/20 16:28:51 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:196)
17/12/20 16:28:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/12/20 16:28:51 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/12/20 16:28:51 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[21] at collect at utils.scala:196), which has no missing parents
17/12/20 16:28:51 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.3 KB, free 2004.2 MB)
17/12/20 16:28:51 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2004.2 MB)
17/12/20 16:28:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:57778 (size: 6.9 KB, free: 2004.5 MB)
17/12/20 16:28:51 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:996
17/12/20 16:28:51 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[21] at collect at utils.scala:196)
17/12/20 16:28:51 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/12/20 16:28:51 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 5934 bytes)
17/12/20 16:28:51 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6, localhost, executor driver, partition 1, PROCESS_LOCAL, 5934 bytes)
17/12/20 16:28:51 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
17/12/20 16:28:51 INFO Executor: Running task 1.0 in stage 5.0 (TID 6)
17/12/20 16:28:51 INFO BlockManager: Found block rdd_11_1 locally
17/12/20 16:28:51 INFO BlockManager: Found block rdd_11_0 locally
17/12/20 16:28:51 INFO Executor: Finished task 1.0 in stage 5.0 (TID 6). 1950 bytes result sent to driver
17/12/20 16:28:51 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1958 bytes result sent to driver
17/12/20 16:28:51 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 34 ms on localhost (executor driver) (1/2)
17/12/20 16:28:51 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 35 ms on localhost (executor driver) (2/2)
17/12/20 16:28:51 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:196) finished in 0.036 s
17/12/20 16:28:51 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:28:51 INFO DAGScheduler: running: Set()
17/12/20 16:28:51 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/12/20 16:28:51 INFO DAGScheduler: failed: Set()
17/12/20 16:28:51 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[24] at collect at utils.scala:196), which has no missing parents
17/12/20 16:28:51 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.0 KB, free 2004.2 MB)
17/12/20 16:28:51 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2004.2 MB)
17/12/20 16:28:51 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/12/20 16:28:51 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:57778 (size: 3.7 KB, free: 2004.5 MB)
17/12/20 16:28:51 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:996
17/12/20 16:28:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[24] at collect at utils.scala:196)
17/12/20 16:28:51 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/12/20 16:28:51 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 7, localhost, executor driver, partition 0, ANY, 5945 bytes)
17/12/20 16:28:51 INFO Executor: Running task 0.0 in stage 6.0 (TID 7)
17/12/20 16:28:51 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 16:28:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 16:28:51 INFO Executor: Finished task 0.0 in stage 6.0 (TID 7). 1707 bytes result sent to driver
17/12/20 16:28:51 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:196) finished in 0.000 s
17/12/20 16:28:51 INFO DAGScheduler: Job 2 finished: collect at utils.scala:196, took 0.112937 s
17/12/20 16:28:51 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 7) in 0 ms on localhost (executor driver) (1/1)
17/12/20 16:28:51 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/12/20 16:28:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:51 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz1`
WHERE (0 = 1)
17/12/20 16:28:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:51 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S1` + 0.9 * RANDN() AS `V1`, `S7` + 0.92195445 * RANDN() AS `V2`, `S5` + 0.94339811 * RANDN() AS `V3`, `S8` + 0.92736185 * RANDN() AS `V4`, `S7` + 0.92195445 * RANDN() AS `V5`, `S10` + 0.90553851 * RANDN() AS `V6`, `S1` + 0.9 * RANDN() AS `V7`, `S9` + 0.9486833 * RANDN() AS `V8`, `S1` + 0.9 * RANDN() AS `V9`, `S10` + 0.90553851 * RANDN() AS `V10`
FROM `analyis_tbl`) `qmyglfzpai`
17/12/20 16:28:52 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:28:52 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:52 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz2`
WHERE (0 = 1)
17/12/20 16:28:52 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:52 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 16:28:52 INFO CodeGenerator: Code generated in 55.451951 ms
17/12/20 16:28:54 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 16:28:54 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 145 bytes
17/12/20 16:28:54 INFO DAGScheduler: Got job 3 (take at <unknown>:0) with 1 output partitions
17/12/20 16:28:54 INFO DAGScheduler: Final stage: ResultStage 8 (take at <unknown>:0)
17/12/20 16:28:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
17/12/20 16:28:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:57778 in memory (size: 4.6 KB, free: 2004.5 MB)
17/12/20 16:28:54 INFO DAGScheduler: Missing parents: List()
17/12/20 16:28:54 INFO DAGScheduler: Submitting ResultStage 8 (WorkerRDD[30] at RDD at rdd.scala:18), which has no missing parents
17/12/20 16:28:54 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 119.2 KB, free 2004.1 MB)
17/12/20 16:28:54 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 45.7 KB, free 2004.0 MB)
17/12/20 16:28:54 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57778 (size: 45.7 KB, free: 2004.5 MB)
17/12/20 16:28:54 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:996
17/12/20 16:28:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (WorkerRDD[30] at RDD at rdd.scala:18)
17/12/20 16:28:54 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/12/20 16:28:54 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5913 bytes)
17/12/20 16:28:54 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
17/12/20 16:28:54 INFO BlockManager: Found block rdd_11_0 locally
17/12/20 16:28:54 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 127.0.0.1:57778 in memory (size: 3.7 KB, free: 2004.5 MB)
17/12/20 16:28:54 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:57778 in memory (size: 6.9 KB, free: 2004.5 MB)
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 50
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 51
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 57
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 58
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 59
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 60
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 61
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 62
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 63
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 64
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 65
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 66
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 67
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 68
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 69
17/12/20 16:28:54 INFO ContextCleaner: Cleaned shuffle 1
17/12/20 16:28:54 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:57778 in memory (size: 7.5 KB, free: 2004.5 MB)
17/12/20 16:28:54 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:57778 in memory (size: 6.9 KB, free: 2004.5 MB)
17/12/20 16:28:54 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:57778 in memory (size: 3.7 KB, free: 2004.5 MB)
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 238
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 1
17/12/20 16:28:54 INFO ContextCleaner: Cleaned accumulator 0
17/12/20 16:28:54 INFO CodeGenerator: Code generated in 87.089102 ms
17/12/20 16:28:54 INFO CodeGenerator: Code generated in 20.539662 ms
17/12/20 16:28:56 INFO MemoryStore: Block rdd_30_0 stored as values in memory (estimated size 1600.0 B, free 2004.1 MB)
17/12/20 16:28:56 INFO BlockManagerInfo: Added rdd_30_0 in memory on 127.0.0.1:57778 (size: 1600.0 B, free: 2004.5 MB)
17/12/20 16:28:56 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 3082 bytes result sent to driver
17/12/20 16:28:56 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 1394 ms on localhost (executor driver) (1/1)
17/12/20 16:28:56 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/12/20 16:28:56 INFO DAGScheduler: ResultStage 8 (take at <unknown>:0) finished in 1.394 s
17/12/20 16:28:56 INFO DAGScheduler: Job 3 finished: take at <unknown>:0, took 1.422887 s
17/12/20 16:28:56 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 16:28:56 INFO DAGScheduler: Got job 4 (take at <unknown>:0) with 1 output partitions
17/12/20 16:28:56 INFO DAGScheduler: Final stage: ResultStage 10 (take at <unknown>:0)
17/12/20 16:28:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
17/12/20 16:28:56 INFO DAGScheduler: Missing parents: List()
17/12/20 16:28:56 INFO DAGScheduler: Submitting ResultStage 10 (WorkerRDD[30] at RDD at rdd.scala:18), which has no missing parents
17/12/20 16:28:56 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 119.2 KB, free 2004.0 MB)
17/12/20 16:28:56 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 45.7 KB, free 2004.0 MB)
17/12/20 16:28:56 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:57778 (size: 45.7 KB, free: 2004.5 MB)
17/12/20 16:28:56 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:996
17/12/20 16:28:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (WorkerRDD[30] at RDD at rdd.scala:18)
17/12/20 16:28:56 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
17/12/20 16:28:56 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 5913 bytes)
17/12/20 16:28:56 INFO Executor: Running task 0.0 in stage 10.0 (TID 9)
17/12/20 16:28:56 INFO BlockManager: Found block rdd_11_1 locally
17/12/20 16:28:57 INFO MemoryStore: Block rdd_30_1 stored as values in memory (estimated size 1600.0 B, free 2004.0 MB)
17/12/20 16:28:57 INFO BlockManagerInfo: Added rdd_30_1 in memory on 127.0.0.1:57778 (size: 1600.0 B, free: 2004.5 MB)
17/12/20 16:28:57 WARN Executor: 1 block locks were not released by TID = 9:
[rdd_30_1]
17/12/20 16:28:57 INFO Executor: Finished task 0.0 in stage 10.0 (TID 9). 3082 bytes result sent to driver
17/12/20 16:28:57 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 1134 ms on localhost (executor driver) (1/1)
17/12/20 16:28:57 INFO DAGScheduler: ResultStage 10 (take at <unknown>:0) finished in 1.135 s
17/12/20 16:28:57 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
17/12/20 16:28:57 INFO DAGScheduler: Job 4 finished: take at <unknown>:0, took 1.150754 s
17/12/20 16:28:57 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:57 INFO SparkSqlParser: Parsing command: sparklyr_tmp_163cffe1a80
17/12/20 16:28:57 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:57 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_163cffe1a80` AS `zzz3`
WHERE (0 = 1)
17/12/20 16:28:57 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:57 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_163cffe1a80`
17/12/20 16:28:57 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:28:57 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:57 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz4`
WHERE (0 = 1)
17/12/20 16:28:57 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:57 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.1) AS `V1`, (`V2` < 0.009) AS `V2`, (`V3` < 0.1) AS `V3`, (`V4` < 0.15) AS `V4`, (`V5` < 0.0185) AS `V5`, (`V6` < 0.0018) AS `V6`, (`V7` < 0.0045) AS `V7`, (`V8` < 0.0045) AS `V8`, (`V9` < 0.0035) AS `V9`, (`V10` < 0.026) AS `V10`
FROM `analyis_tbl`
17/12/20 16:28:57 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:28:57 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:57 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz5`
WHERE (0 = 1)
17/12/20 16:28:57 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:57 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 16:28:58 INFO CodeGenerator: Code generated in 34.268514 ms
17/12/20 16:28:58 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 16:28:58 INFO DAGScheduler: Got job 5 (collect at utils.scala:196) with 2 output partitions
17/12/20 16:28:58 INFO DAGScheduler: Final stage: ResultStage 12 (collect at utils.scala:196)
17/12/20 16:28:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
17/12/20 16:28:58 INFO DAGScheduler: Missing parents: List()
17/12/20 16:28:58 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[37] at collect at utils.scala:196), which has no missing parents
17/12/20 16:28:58 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 132.0 KB, free 2003.8 MB)
17/12/20 16:28:58 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 50.3 KB, free 2003.8 MB)
17/12/20 16:28:58 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 127.0.0.1:57778 (size: 50.3 KB, free: 2004.4 MB)
17/12/20 16:28:58 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:996
17/12/20 16:28:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[37] at collect at utils.scala:196)
17/12/20 16:28:58 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks
17/12/20 16:28:58 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 5945 bytes)
17/12/20 16:28:58 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 11, localhost, executor driver, partition 1, PROCESS_LOCAL, 5945 bytes)
17/12/20 16:28:58 INFO Executor: Running task 0.0 in stage 12.0 (TID 10)
17/12/20 16:28:58 INFO Executor: Running task 1.0 in stage 12.0 (TID 11)
17/12/20 16:28:58 INFO BlockManager: Found block rdd_30_1 locally
17/12/20 16:28:58 INFO BlockManager: Found block rdd_30_0 locally
17/12/20 16:28:58 INFO CodeGenerator: Code generated in 113.413685 ms
17/12/20 16:28:58 INFO Executor: Finished task 0.0 in stage 12.0 (TID 10). 1690 bytes result sent to driver
17/12/20 16:28:58 INFO Executor: Finished task 1.0 in stage 12.0 (TID 11). 1690 bytes result sent to driver
17/12/20 16:28:58 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 162 ms on localhost (executor driver) (1/2)
17/12/20 16:28:58 INFO DAGScheduler: ResultStage 12 (collect at utils.scala:196) finished in 0.165 s
17/12/20 16:28:58 INFO DAGScheduler: Job 5 finished: collect at utils.scala:196, took 0.177466 s
17/12/20 16:28:58 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 11) in 159 ms on localhost (executor driver) (2/2)
17/12/20 16:28:58 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
17/12/20 16:28:58 INFO CodeGenerator: Code generated in 15.560498 ms
17/12/20 16:28:58 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:58 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:28:58 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:58 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:58 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:58 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:58 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:28:58 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:28:58 INFO CodeGenerator: Code generated in 10.400851 ms
17/12/20 16:28:58 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:58 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:28:58 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:58 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:59 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:59 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:59 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:28:59 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:28:59 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:59 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:28:59 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:59 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:59 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:59 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:59 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:28:59 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:28:59 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:59 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:28:59 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:59 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:59 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:59 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:59 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:28:59 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:28:59 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:59 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:28:59 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:59 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:59 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:59 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:59 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:28:59 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:28:59 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:28:59 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:28:59 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:59 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:59 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:28:59 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:28:59 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:28:59 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:36:17 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:17 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:36:17 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:36:17 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:36:17 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:36:17 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:36:17 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:36:17 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:36:17 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 16:36:17 INFO DAGScheduler: Got job 6 (collect at utils.scala:58) with 1 output partitions
17/12/20 16:36:17 INFO DAGScheduler: Final stage: ResultStage 13 (collect at utils.scala:58)
17/12/20 16:36:17 INFO DAGScheduler: Parents of final stage: List()
17/12/20 16:36:17 INFO DAGScheduler: Missing parents: List()
17/12/20 16:36:17 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[49] at map at utils.scala:55), which has no missing parents
17/12/20 16:36:17 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 8.7 KB, free 2003.8 MB)
17/12/20 16:36:17 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2003.8 MB)
17/12/20 16:36:17 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 127.0.0.1:57778 (size: 4.6 KB, free: 2004.4 MB)
17/12/20 16:36:17 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:996
17/12/20 16:36:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[49] at map at utils.scala:55)
17/12/20 16:36:17 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
17/12/20 16:36:17 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 6432 bytes)
17/12/20 16:36:17 INFO Executor: Running task 0.0 in stage 13.0 (TID 12)
17/12/20 16:36:17 INFO Executor: Finished task 0.0 in stage 13.0 (TID 12). 1103 bytes result sent to driver
17/12/20 16:36:17 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 12) in 20 ms on localhost (executor driver) (1/1)
17/12/20 16:36:17 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
17/12/20 16:36:17 INFO DAGScheduler: ResultStage 13 (collect at utils.scala:58) finished in 0.020 s
17/12/20 16:36:17 INFO DAGScheduler: Job 6 finished: collect at utils.scala:58, took 0.024636 s
17/12/20 16:36:17 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:17 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:17 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:36:17 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:17 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 16:36:17 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 16:36:17 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 16:36:17 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 16:36:17 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 16:36:17 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 16:36:17 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 293.7 KB, free 2003.5 MB)
17/12/20 16:36:17 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2003.5 MB)
17/12/20 16:36:17 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 127.0.0.1:57778 (size: 24.0 KB, free: 2004.4 MB)
17/12/20 16:36:17 INFO SparkContext: Created broadcast 11 from sql at <unknown>:0
17/12/20 16:36:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 16:36:17 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 16:36:17 INFO DAGScheduler: Registering RDD 53 (sql at <unknown>:0)
17/12/20 16:36:17 INFO DAGScheduler: Registering RDD 58 (sql at <unknown>:0)
17/12/20 16:36:17 INFO DAGScheduler: Got job 7 (sql at <unknown>:0) with 1 output partitions
17/12/20 16:36:17 INFO DAGScheduler: Final stage: ResultStage 16 (sql at <unknown>:0)
17/12/20 16:36:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)
17/12/20 16:36:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 15)
17/12/20 16:36:17 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[53] at sql at <unknown>:0), which has no missing parents
17/12/20 16:36:17 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 13.1 KB, free 2003.4 MB)
17/12/20 16:36:18 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2003.4 MB)
17/12/20 16:36:18 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 127.0.0.1:57778 (size: 7.5 KB, free: 2004.4 MB)
17/12/20 16:36:18 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:996
17/12/20 16:36:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[53] at sql at <unknown>:0)
17/12/20 16:36:18 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
17/12/20 16:36:18 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 6677 bytes)
17/12/20 16:36:18 INFO Executor: Running task 0.0 in stage 14.0 (TID 13)
17/12/20 16:36:18 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/RtmpYBQ1ot/spark_serialize_9d0a637f4cc2e80bb2933a59d2bb6e314905142e1a2697d22cdb7f045941d82f.csv, range: 0-1924, partition values: [empty row]
17/12/20 16:36:18 INFO Executor: Finished task 0.0 in stage 14.0 (TID 13). 1719 bytes result sent to driver
17/12/20 16:36:18 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 13) in 68 ms on localhost (executor driver) (1/1)
17/12/20 16:36:18 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
17/12/20 16:36:18 INFO DAGScheduler: ShuffleMapStage 14 (sql at <unknown>:0) finished in 0.068 s
17/12/20 16:36:18 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:36:18 INFO DAGScheduler: running: Set()
17/12/20 16:36:18 INFO DAGScheduler: waiting: Set(ShuffleMapStage 15, ResultStage 16)
17/12/20 16:36:18 INFO DAGScheduler: failed: Set()
17/12/20 16:36:18 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[58] at sql at <unknown>:0), which has no missing parents
17/12/20 16:36:18 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 15.3 KB, free 2003.4 MB)
17/12/20 16:36:18 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2003.4 MB)
17/12/20 16:36:18 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 127.0.0.1:57778 (size: 6.9 KB, free: 2004.4 MB)
17/12/20 16:36:18 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:996
17/12/20 16:36:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[58] at sql at <unknown>:0)
17/12/20 16:36:18 INFO TaskSchedulerImpl: Adding task set 15.0 with 2 tasks
17/12/20 16:36:18 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 14, localhost, executor driver, partition 0, ANY, 5943 bytes)
17/12/20 16:36:18 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 15, localhost, executor driver, partition 1, ANY, 5943 bytes)
17/12/20 16:36:18 INFO Executor: Running task 0.0 in stage 15.0 (TID 14)
17/12/20 16:36:18 INFO Executor: Running task 1.0 in stage 15.0 (TID 15)
17/12/20 16:36:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 16:36:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 16:36:18 INFO MemoryStore: Block rdd_55_0 stored as values in memory (estimated size 1512.0 B, free 2003.4 MB)
17/12/20 16:36:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 16:36:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 16:36:18 INFO BlockManagerInfo: Added rdd_55_0 in memory on 127.0.0.1:57778 (size: 1512.0 B, free: 2004.4 MB)
17/12/20 16:36:18 INFO MemoryStore: Block rdd_55_1 stored as values in memory (estimated size 1512.0 B, free 2003.4 MB)
17/12/20 16:36:18 INFO BlockManagerInfo: Added rdd_55_1 in memory on 127.0.0.1:57778 (size: 1512.0 B, free: 2004.4 MB)
17/12/20 16:36:18 INFO Executor: Finished task 0.0 in stage 15.0 (TID 14). 3162 bytes result sent to driver
17/12/20 16:36:18 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 14) in 21 ms on localhost (executor driver) (1/2)
17/12/20 16:36:18 INFO Executor: Finished task 1.0 in stage 15.0 (TID 15). 3241 bytes result sent to driver
17/12/20 16:36:18 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 15) in 27 ms on localhost (executor driver) (2/2)
17/12/20 16:36:18 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
17/12/20 16:36:18 INFO DAGScheduler: ShuffleMapStage 15 (sql at <unknown>:0) finished in 0.027 s
17/12/20 16:36:18 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:36:18 INFO DAGScheduler: running: Set()
17/12/20 16:36:18 INFO DAGScheduler: waiting: Set(ResultStage 16)
17/12/20 16:36:18 INFO DAGScheduler: failed: Set()
17/12/20 16:36:18 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[61] at sql at <unknown>:0), which has no missing parents
17/12/20 16:36:18 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 7.0 KB, free 2003.4 MB)
17/12/20 16:36:18 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.4 MB)
17/12/20 16:36:18 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 127.0.0.1:57778 (size: 3.7 KB, free: 2004.4 MB)
17/12/20 16:36:18 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:996
17/12/20 16:36:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[61] at sql at <unknown>:0)
17/12/20 16:36:18 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
17/12/20 16:36:18 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, ANY, 5954 bytes)
17/12/20 16:36:18 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
17/12/20 16:36:18 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 16:36:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 16:36:18 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 1873 bytes result sent to driver
17/12/20 16:36:18 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 6 ms on localhost (executor driver) (1/1)
17/12/20 16:36:18 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
17/12/20 16:36:18 INFO DAGScheduler: ResultStage 16 (sql at <unknown>:0) finished in 0.006 s
17/12/20 16:36:18 INFO DAGScheduler: Job 7 finished: sql at <unknown>:0, took 0.116297 s
17/12/20 16:36:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:18 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 16:36:18 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 16:36:18 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 145 bytes
17/12/20 16:36:18 INFO DAGScheduler: Registering RDD 65 (collect at utils.scala:196)
17/12/20 16:36:18 INFO DAGScheduler: Got job 8 (collect at utils.scala:196) with 1 output partitions
17/12/20 16:36:18 INFO DAGScheduler: Final stage: ResultStage 19 (collect at utils.scala:196)
17/12/20 16:36:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
17/12/20 16:36:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 18)
17/12/20 16:36:18 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[65] at collect at utils.scala:196), which has no missing parents
17/12/20 16:36:18 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 15.3 KB, free 2003.4 MB)
17/12/20 16:36:18 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2003.4 MB)
17/12/20 16:36:18 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 127.0.0.1:57778 (size: 6.9 KB, free: 2004.4 MB)
17/12/20 16:36:18 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:996
17/12/20 16:36:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[65] at collect at utils.scala:196)
17/12/20 16:36:18 INFO TaskSchedulerImpl: Adding task set 18.0 with 2 tasks
17/12/20 16:36:18 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 5936 bytes)
17/12/20 16:36:18 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 18, localhost, executor driver, partition 1, PROCESS_LOCAL, 5936 bytes)
17/12/20 16:36:18 INFO Executor: Running task 0.0 in stage 18.0 (TID 17)
17/12/20 16:36:18 INFO Executor: Running task 1.0 in stage 18.0 (TID 18)
17/12/20 16:36:18 INFO BlockManager: Found block rdd_55_0 locally
17/12/20 16:36:18 INFO BlockManager: Found block rdd_55_1 locally
17/12/20 16:36:18 INFO Executor: Finished task 1.0 in stage 18.0 (TID 18). 1950 bytes result sent to driver
17/12/20 16:36:18 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 18) in 12 ms on localhost (executor driver) (1/2)
17/12/20 16:36:18 INFO Executor: Finished task 0.0 in stage 18.0 (TID 17). 1958 bytes result sent to driver
17/12/20 16:36:18 INFO DAGScheduler: ShuffleMapStage 18 (collect at utils.scala:196) finished in 0.016 s
17/12/20 16:36:18 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:36:18 INFO DAGScheduler: running: Set()
17/12/20 16:36:18 INFO DAGScheduler: waiting: Set(ResultStage 19)
17/12/20 16:36:18 INFO DAGScheduler: failed: Set()
17/12/20 16:36:18 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[68] at collect at utils.scala:196), which has no missing parents
17/12/20 16:36:18 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 17) in 15 ms on localhost (executor driver) (2/2)
17/12/20 16:36:18 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
17/12/20 16:36:18 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 7.0 KB, free 2003.4 MB)
17/12/20 16:36:18 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.4 MB)
17/12/20 16:36:18 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 127.0.0.1:57778 (size: 3.7 KB, free: 2004.4 MB)
17/12/20 16:36:18 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:996
17/12/20 16:36:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[68] at collect at utils.scala:196)
17/12/20 16:36:18 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
17/12/20 16:36:18 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, ANY, 5947 bytes)
17/12/20 16:36:18 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
17/12/20 16:36:18 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 16:36:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 16:36:18 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 1786 bytes result sent to driver
17/12/20 16:36:18 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 5 ms on localhost (executor driver) (1/1)
17/12/20 16:36:18 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
17/12/20 16:36:18 INFO DAGScheduler: ResultStage 19 (collect at utils.scala:196) finished in 0.005 s
17/12/20 16:36:18 INFO DAGScheduler: Job 8 finished: collect at utils.scala:196, took 0.032547 s
17/12/20 16:36:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:18 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz6`
WHERE (0 = 1)
17/12/20 16:36:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:18 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S9` + 0.94339811 * RANDN() AS `V1`, `S2` + 0.93273791 * RANDN() AS `V2`, `S3` + 0.94339811 * RANDN() AS `V3`, `S5` + 0.94339811 * RANDN() AS `V4`, `S10` + 0.93273791 * RANDN() AS `V5`, `S3` + 0.94339811 * RANDN() AS `V6`, `S8` + 0.93273791 * RANDN() AS `V7`, `S3` + 0.94339811 * RANDN() AS `V8`, `S10` + 0.93273791 * RANDN() AS `V9`, `S8` + 0.93273791 * RANDN() AS `V10`
FROM `analyis_tbl`) `xzneuqmsev`
17/12/20 16:36:18 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:36:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:18 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz7`
WHERE (0 = 1)
17/12/20 16:36:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:18 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 16:36:18 INFO CodeGenerator: Code generated in 14.261619 ms
17/12/20 16:36:18 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 16:36:18 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 145 bytes
17/12/20 16:36:18 INFO DAGScheduler: Got job 9 (take at <unknown>:0) with 1 output partitions
17/12/20 16:36:18 INFO DAGScheduler: Final stage: ResultStage 21 (take at <unknown>:0)
17/12/20 16:36:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
17/12/20 16:36:18 INFO DAGScheduler: Missing parents: List()
17/12/20 16:36:18 INFO DAGScheduler: Submitting ResultStage 21 (WorkerRDD[74] at RDD at rdd.scala:18), which has no missing parents
17/12/20 16:36:18 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 122.1 KB, free 2003.3 MB)
17/12/20 16:36:18 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 46.6 KB, free 2003.2 MB)
17/12/20 16:36:18 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 127.0.0.1:57778 (size: 46.6 KB, free: 2004.3 MB)
17/12/20 16:36:18 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:996
17/12/20 16:36:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (WorkerRDD[74] at RDD at rdd.scala:18)
17/12/20 16:36:18 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
17/12/20 16:36:18 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 5914 bytes)
17/12/20 16:36:18 INFO Executor: Running task 0.0 in stage 21.0 (TID 20)
17/12/20 16:36:18 INFO BlockManager: Found block rdd_55_0 locally
17/12/20 16:36:19 INFO MemoryStore: Block rdd_74_0 stored as values in memory (estimated size 1600.0 B, free 2003.2 MB)
17/12/20 16:36:19 INFO BlockManagerInfo: Added rdd_74_0 in memory on 127.0.0.1:57778 (size: 1600.0 B, free: 2004.3 MB)
17/12/20 16:36:19 INFO Executor: Finished task 0.0 in stage 21.0 (TID 20). 3090 bytes result sent to driver
17/12/20 16:36:19 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 20) in 693 ms on localhost (executor driver) (1/1)
17/12/20 16:36:19 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
17/12/20 16:36:19 INFO DAGScheduler: ResultStage 21 (take at <unknown>:0) finished in 0.693 s
17/12/20 16:36:19 INFO DAGScheduler: Job 9 finished: take at <unknown>:0, took 0.699569 s
17/12/20 16:36:19 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 16:36:19 INFO DAGScheduler: Got job 10 (take at <unknown>:0) with 1 output partitions
17/12/20 16:36:19 INFO DAGScheduler: Final stage: ResultStage 23 (take at <unknown>:0)
17/12/20 16:36:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
17/12/20 16:36:19 INFO DAGScheduler: Missing parents: List()
17/12/20 16:36:19 INFO DAGScheduler: Submitting ResultStage 23 (WorkerRDD[74] at RDD at rdd.scala:18), which has no missing parents
17/12/20 16:36:19 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 122.1 KB, free 2003.1 MB)
17/12/20 16:36:19 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 46.6 KB, free 2003.0 MB)
17/12/20 16:36:19 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 127.0.0.1:57778 (size: 46.6 KB, free: 2004.3 MB)
17/12/20 16:36:19 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:996
17/12/20 16:36:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (WorkerRDD[74] at RDD at rdd.scala:18)
17/12/20 16:36:19 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
17/12/20 16:36:19 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 21, localhost, executor driver, partition 1, PROCESS_LOCAL, 5914 bytes)
17/12/20 16:36:19 INFO Executor: Running task 0.0 in stage 23.0 (TID 21)
17/12/20 16:36:19 INFO BlockManager: Found block rdd_55_1 locally
17/12/20 16:36:20 INFO MemoryStore: Block rdd_74_1 stored as values in memory (estimated size 1600.0 B, free 2003.0 MB)
17/12/20 16:36:20 INFO BlockManagerInfo: Added rdd_74_1 in memory on 127.0.0.1:57778 (size: 1600.0 B, free: 2004.3 MB)
17/12/20 16:36:20 WARN Executor: 1 block locks were not released by TID = 21:
[rdd_74_1]
17/12/20 16:36:20 INFO Executor: Finished task 0.0 in stage 23.0 (TID 21). 3169 bytes result sent to driver
17/12/20 16:36:20 INFO DAGScheduler: ResultStage 23 (take at <unknown>:0) finished in 0.815 s
17/12/20 16:36:20 INFO DAGScheduler: Job 10 finished: take at <unknown>:0, took 0.823816 s
17/12/20 16:36:20 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 21) in 815 ms on localhost (executor driver) (1/1)
17/12/20 16:36:20 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
17/12/20 16:36:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:20 INFO SparkSqlParser: Parsing command: sparklyr_tmp_163c26f72
17/12/20 16:36:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:20 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_163c26f72` AS `zzz8`
WHERE (0 = 1)
17/12/20 16:36:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:20 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_163c26f72`
17/12/20 16:36:20 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:36:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:20 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz9`
WHERE (0 = 1)
17/12/20 16:36:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:20 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.0375) AS `V1`, (`V2` < 0.0055) AS `V2`, (`V3` < 0.3) AS `V3`, (`V4` < 0.0375) AS `V4`, (`V5` < 0.009) AS `V5`, (`V6` < 0.075) AS `V6`, (`V7` < 0.0138) AS `V7`, (`V8` < 0.0375) AS `V8`, (`V9` < 0.0045) AS `V9`, (`V10` < 0.0185) AS `V10`
FROM `analyis_tbl`
17/12/20 16:36:20 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:36:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:20 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz10`
WHERE (0 = 1)
17/12/20 16:36:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:20 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 16:36:20 INFO CodeGenerator: Code generated in 23.536902 ms
17/12/20 16:36:20 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 16:36:20 INFO DAGScheduler: Got job 11 (collect at utils.scala:196) with 2 output partitions
17/12/20 16:36:20 INFO DAGScheduler: Final stage: ResultStage 25 (collect at utils.scala:196)
17/12/20 16:36:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)
17/12/20 16:36:20 INFO DAGScheduler: Missing parents: List()
17/12/20 16:36:20 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[81] at collect at utils.scala:196), which has no missing parents
17/12/20 16:36:20 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 134.9 KB, free 2002.9 MB)
17/12/20 16:36:20 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 51.1 KB, free 2002.9 MB)
17/12/20 16:36:20 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 127.0.0.1:57778 (size: 51.1 KB, free: 2004.2 MB)
17/12/20 16:36:20 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:996
17/12/20 16:36:20 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 25 (MapPartitionsRDD[81] at collect at utils.scala:196)
17/12/20 16:36:20 INFO TaskSchedulerImpl: Adding task set 25.0 with 2 tasks
17/12/20 16:36:20 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 5947 bytes)
17/12/20 16:36:20 INFO TaskSetManager: Starting task 1.0 in stage 25.0 (TID 23, localhost, executor driver, partition 1, PROCESS_LOCAL, 5947 bytes)
17/12/20 16:36:20 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 127.0.0.1:57778 in memory (size: 3.7 KB, free: 2004.2 MB)
17/12/20 16:36:20 INFO Executor: Running task 0.0 in stage 25.0 (TID 22)
17/12/20 16:36:20 INFO Executor: Running task 1.0 in stage 25.0 (TID 23)
17/12/20 16:36:20 INFO BlockManager: Found block rdd_74_1 locally
17/12/20 16:36:20 INFO BlockManager: Found block rdd_74_0 locally
17/12/20 16:36:20 INFO Executor: Finished task 0.0 in stage 25.0 (TID 22). 1542 bytes result sent to driver
17/12/20 16:36:20 INFO Executor: Finished task 1.0 in stage 25.0 (TID 23). 1720 bytes result sent to driver
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 781
17/12/20 16:36:20 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 22) in 40 ms on localhost (executor driver) (1/2)
17/12/20 16:36:20 INFO TaskSetManager: Finished task 1.0 in stage 25.0 (TID 23) in 29 ms on localhost (executor driver) (2/2)
17/12/20 16:36:20 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
17/12/20 16:36:20 INFO DAGScheduler: ResultStage 25 (collect at utils.scala:196) finished in 0.041 s
17/12/20 16:36:20 INFO DAGScheduler: Job 11 finished: collect at utils.scala:196, took 0.048956 s
17/12/20 16:36:20 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 127.0.0.1:57778 in memory (size: 6.9 KB, free: 2004.2 MB)
17/12/20 16:36:20 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 127.0.0.1:57778 in memory (size: 3.7 KB, free: 2004.2 MB)
17/12/20 16:36:20 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 127.0.0.1:57778 in memory (size: 46.6 KB, free: 2004.3 MB)
17/12/20 16:36:20 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 127.0.0.1:57778 in memory (size: 46.6 KB, free: 2004.3 MB)
17/12/20 16:36:20 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 127.0.0.1:57778 in memory (size: 45.7 KB, free: 2004.4 MB)
17/12/20 16:36:20 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 127.0.0.1:57778 in memory (size: 45.7 KB, free: 2004.4 MB)
17/12/20 16:36:20 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 127.0.0.1:57778 in memory (size: 50.3 KB, free: 2004.5 MB)
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 543
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 544
17/12/20 16:36:20 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 127.0.0.1:57778 in memory (size: 4.6 KB, free: 2004.5 MB)
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 593
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 594
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 600
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 601
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 602
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 603
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 604
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 605
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 606
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 607
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 608
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 609
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 610
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 611
17/12/20 16:36:20 INFO ContextCleaner: Cleaned accumulator 612
17/12/20 16:36:20 INFO ContextCleaner: Cleaned shuffle 4
17/12/20 16:36:20 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 127.0.0.1:57778 in memory (size: 7.5 KB, free: 2004.5 MB)
17/12/20 16:36:20 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 127.0.0.1:57778 in memory (size: 6.9 KB, free: 2004.5 MB)
17/12/20 16:36:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:20 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:36:20 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:36:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:36:20 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:36:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:36:20 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:36:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:36:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:20 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:36:20 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:36:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:36:20 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:36:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:36:20 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:36:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:36:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:20 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:36:20 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:36:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:36:20 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:36:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:36:20 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:36:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:36:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:36:20 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:36:20 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:36:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:36:20 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:36:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:36:20 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:36:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:44:12 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:12 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:44:12 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:44:12 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:44:12 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:44:12 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:44:12 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:44:12 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:44:12 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 16:44:12 INFO DAGScheduler: Got job 12 (collect at utils.scala:58) with 1 output partitions
17/12/20 16:44:12 INFO DAGScheduler: Final stage: ResultStage 26 (collect at utils.scala:58)
17/12/20 16:44:12 INFO DAGScheduler: Parents of final stage: List()
17/12/20 16:44:12 INFO DAGScheduler: Missing parents: List()
17/12/20 16:44:12 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[91] at map at utils.scala:55), which has no missing parents
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 8.7 KB, free 2003.8 MB)
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2003.8 MB)
17/12/20 16:44:12 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 127.0.0.1:57778 (size: 4.6 KB, free: 2004.5 MB)
17/12/20 16:44:12 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:996
17/12/20 16:44:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[91] at map at utils.scala:55)
17/12/20 16:44:12 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
17/12/20 16:44:12 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 6501 bytes)
17/12/20 16:44:12 INFO Executor: Running task 0.0 in stage 26.0 (TID 24)
17/12/20 16:44:12 INFO Executor: Finished task 0.0 in stage 26.0 (TID 24). 1128 bytes result sent to driver
17/12/20 16:44:12 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 24) in 8 ms on localhost (executor driver) (1/1)
17/12/20 16:44:12 INFO DAGScheduler: ResultStage 26 (collect at utils.scala:58) finished in 0.008 s
17/12/20 16:44:12 INFO DAGScheduler: Job 12 finished: collect at utils.scala:58, took 0.014924 s
17/12/20 16:44:12 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
17/12/20 16:44:12 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:12 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:12 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:44:12 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:12 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 16:44:12 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 16:44:12 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 16:44:12 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 16:44:12 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 16:44:12 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 293.7 KB, free 2003.5 MB)
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2003.5 MB)
17/12/20 16:44:12 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 127.0.0.1:57778 (size: 24.0 KB, free: 2004.5 MB)
17/12/20 16:44:12 INFO SparkContext: Created broadcast 21 from sql at <unknown>:0
17/12/20 16:44:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 16:44:12 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 16:44:12 INFO DAGScheduler: Registering RDD 95 (sql at <unknown>:0)
17/12/20 16:44:12 INFO DAGScheduler: Registering RDD 100 (sql at <unknown>:0)
17/12/20 16:44:12 INFO DAGScheduler: Got job 13 (sql at <unknown>:0) with 1 output partitions
17/12/20 16:44:12 INFO DAGScheduler: Final stage: ResultStage 29 (sql at <unknown>:0)
17/12/20 16:44:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
17/12/20 16:44:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 28)
17/12/20 16:44:12 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[95] at sql at <unknown>:0), which has no missing parents
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 13.1 KB, free 2003.4 MB)
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2003.4 MB)
17/12/20 16:44:12 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 127.0.0.1:57778 (size: 7.5 KB, free: 2004.5 MB)
17/12/20 16:44:12 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:996
17/12/20 16:44:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[95] at sql at <unknown>:0)
17/12/20 16:44:12 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
17/12/20 16:44:12 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 6678 bytes)
17/12/20 16:44:12 INFO Executor: Running task 0.0 in stage 27.0 (TID 25)
17/12/20 16:44:12 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/RtmpYBQ1ot/spark_serialize_03230dbea8c6afc0c45c04f42f5c0000d9c923cc7cee9087438e26b0c66a584f.csv, range: 0-1920, partition values: [empty row]
17/12/20 16:44:12 INFO Executor: Finished task 0.0 in stage 27.0 (TID 25). 1719 bytes result sent to driver
17/12/20 16:44:12 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 25) in 35 ms on localhost (executor driver) (1/1)
17/12/20 16:44:12 INFO DAGScheduler: ShuffleMapStage 27 (sql at <unknown>:0) finished in 0.035 s
17/12/20 16:44:12 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:44:12 INFO DAGScheduler: running: Set()
17/12/20 16:44:12 INFO DAGScheduler: waiting: Set(ShuffleMapStage 28, ResultStage 29)
17/12/20 16:44:12 INFO DAGScheduler: failed: Set()
17/12/20 16:44:12 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
17/12/20 16:44:12 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[100] at sql at <unknown>:0), which has no missing parents
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 15.3 KB, free 2003.4 MB)
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2003.4 MB)
17/12/20 16:44:12 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 127.0.0.1:57778 (size: 6.9 KB, free: 2004.4 MB)
17/12/20 16:44:12 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:996
17/12/20 16:44:12 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[100] at sql at <unknown>:0)
17/12/20 16:44:12 INFO TaskSchedulerImpl: Adding task set 28.0 with 2 tasks
17/12/20 16:44:12 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 26, localhost, executor driver, partition 0, ANY, 5944 bytes)
17/12/20 16:44:12 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 27, localhost, executor driver, partition 1, ANY, 5944 bytes)
17/12/20 16:44:12 INFO Executor: Running task 0.0 in stage 28.0 (TID 26)
17/12/20 16:44:12 INFO Executor: Running task 1.0 in stage 28.0 (TID 27)
17/12/20 16:44:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 16:44:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/12/20 16:44:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 16:44:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 16:44:12 INFO MemoryStore: Block rdd_97_0 stored as values in memory (estimated size 1512.0 B, free 2003.4 MB)
17/12/20 16:44:12 INFO BlockManagerInfo: Added rdd_97_0 in memory on 127.0.0.1:57778 (size: 1512.0 B, free: 2004.4 MB)
17/12/20 16:44:12 INFO MemoryStore: Block rdd_97_1 stored as values in memory (estimated size 1512.0 B, free 2003.4 MB)
17/12/20 16:44:12 INFO BlockManagerInfo: Added rdd_97_1 in memory on 127.0.0.1:57778 (size: 1512.0 B, free: 2004.4 MB)
17/12/20 16:44:12 INFO Executor: Finished task 1.0 in stage 28.0 (TID 27). 3072 bytes result sent to driver
17/12/20 16:44:12 INFO Executor: Finished task 0.0 in stage 28.0 (TID 26). 3151 bytes result sent to driver
17/12/20 16:44:12 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 27) in 22 ms on localhost (executor driver) (1/2)
17/12/20 16:44:12 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 26) in 23 ms on localhost (executor driver) (2/2)
17/12/20 16:44:12 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
17/12/20 16:44:12 INFO DAGScheduler: ShuffleMapStage 28 (sql at <unknown>:0) finished in 0.026 s
17/12/20 16:44:12 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:44:12 INFO DAGScheduler: running: Set()
17/12/20 16:44:12 INFO DAGScheduler: waiting: Set(ResultStage 29)
17/12/20 16:44:12 INFO DAGScheduler: failed: Set()
17/12/20 16:44:12 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[103] at sql at <unknown>:0), which has no missing parents
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 7.0 KB, free 2003.4 MB)
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.4 MB)
17/12/20 16:44:12 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 127.0.0.1:57778 (size: 3.7 KB, free: 2004.4 MB)
17/12/20 16:44:12 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:996
17/12/20 16:44:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[103] at sql at <unknown>:0)
17/12/20 16:44:12 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
17/12/20 16:44:12 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 28, localhost, executor driver, partition 0, ANY, 5955 bytes)
17/12/20 16:44:12 INFO Executor: Running task 0.0 in stage 29.0 (TID 28)
17/12/20 16:44:12 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 16:44:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 16:44:12 INFO Executor: Finished task 0.0 in stage 29.0 (TID 28). 1873 bytes result sent to driver
17/12/20 16:44:12 INFO DAGScheduler: ResultStage 29 (sql at <unknown>:0) finished in 0.006 s
17/12/20 16:44:12 INFO DAGScheduler: Job 13 finished: sql at <unknown>:0, took 0.079113 s
17/12/20 16:44:12 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 28) in 6 ms on localhost (executor driver) (1/1)
17/12/20 16:44:12 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
17/12/20 16:44:12 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:12 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 16:44:12 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 16:44:12 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 145 bytes
17/12/20 16:44:12 INFO DAGScheduler: Registering RDD 107 (collect at utils.scala:196)
17/12/20 16:44:12 INFO DAGScheduler: Got job 14 (collect at utils.scala:196) with 1 output partitions
17/12/20 16:44:12 INFO DAGScheduler: Final stage: ResultStage 32 (collect at utils.scala:196)
17/12/20 16:44:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
17/12/20 16:44:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 31)
17/12/20 16:44:12 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[107] at collect at utils.scala:196), which has no missing parents
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.3 KB, free 2003.4 MB)
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2003.4 MB)
17/12/20 16:44:12 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 127.0.0.1:57778 (size: 6.9 KB, free: 2004.4 MB)
17/12/20 16:44:12 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:996
17/12/20 16:44:12 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[107] at collect at utils.scala:196)
17/12/20 16:44:12 INFO TaskSchedulerImpl: Adding task set 31.0 with 2 tasks
17/12/20 16:44:12 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 29, localhost, executor driver, partition 0, PROCESS_LOCAL, 5936 bytes)
17/12/20 16:44:12 INFO TaskSetManager: Starting task 1.0 in stage 31.0 (TID 30, localhost, executor driver, partition 1, PROCESS_LOCAL, 5936 bytes)
17/12/20 16:44:12 INFO Executor: Running task 0.0 in stage 31.0 (TID 29)
17/12/20 16:44:12 INFO Executor: Running task 1.0 in stage 31.0 (TID 30)
17/12/20 16:44:12 INFO BlockManager: Found block rdd_97_0 locally
17/12/20 16:44:12 INFO BlockManager: Found block rdd_97_1 locally
17/12/20 16:44:12 INFO Executor: Finished task 0.0 in stage 31.0 (TID 29). 1871 bytes result sent to driver
17/12/20 16:44:12 INFO Executor: Finished task 1.0 in stage 31.0 (TID 30). 1871 bytes result sent to driver
17/12/20 16:44:12 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 29) in 15 ms on localhost (executor driver) (1/2)
17/12/20 16:44:12 INFO DAGScheduler: ShuffleMapStage 31 (collect at utils.scala:196) finished in 0.015 s
17/12/20 16:44:12 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:44:12 INFO DAGScheduler: running: Set()
17/12/20 16:44:12 INFO DAGScheduler: waiting: Set(ResultStage 32)
17/12/20 16:44:12 INFO DAGScheduler: failed: Set()
17/12/20 16:44:12 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[110] at collect at utils.scala:196), which has no missing parents
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 7.0 KB, free 2003.4 MB)
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.4 MB)
17/12/20 16:44:12 INFO TaskSetManager: Finished task 1.0 in stage 31.0 (TID 30) in 12 ms on localhost (executor driver) (2/2)
17/12/20 16:44:12 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
17/12/20 16:44:12 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 127.0.0.1:57778 (size: 3.7 KB, free: 2004.4 MB)
17/12/20 16:44:12 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:996
17/12/20 16:44:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[110] at collect at utils.scala:196)
17/12/20 16:44:12 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
17/12/20 16:44:12 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 31, localhost, executor driver, partition 0, ANY, 5947 bytes)
17/12/20 16:44:12 INFO Executor: Running task 0.0 in stage 32.0 (TID 31)
17/12/20 16:44:12 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 16:44:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 16:44:12 INFO Executor: Finished task 0.0 in stage 32.0 (TID 31). 1963 bytes result sent to driver
17/12/20 16:44:12 INFO DAGScheduler: ResultStage 32 (collect at utils.scala:196) finished in 0.010 s
17/12/20 16:44:12 INFO DAGScheduler: Job 14 finished: collect at utils.scala:196, took 0.029783 s
17/12/20 16:44:12 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 31) in 10 ms on localhost (executor driver) (1/1)
17/12/20 16:44:12 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
17/12/20 16:44:12 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:12 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz11`
WHERE (0 = 1)
17/12/20 16:44:12 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:12 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S9` + 0.94339811 * RANDN() AS `V1`, `S2` + 0.93273791 * RANDN() AS `V2`, `S3` + 0.94339811 * RANDN() AS `V3`, `S5` + 0.94339811 * RANDN() AS `V4`, `S10` + 0.93273791 * RANDN() AS `V5`, `S3` + 0.94339811 * RANDN() AS `V6`, `S8` + 0.93273791 * RANDN() AS `V7`, `S3` + 0.94339811 * RANDN() AS `V8`, `S10` + 0.93273791 * RANDN() AS `V9`, `S8` + 0.93273791 * RANDN() AS `V10`
FROM `analyis_tbl`) `mfabjiivcx`
17/12/20 16:44:12 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:44:12 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:12 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz12`
WHERE (0 = 1)
17/12/20 16:44:12 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:12 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 16:44:12 INFO CodeGenerator: Code generated in 13.112262 ms
17/12/20 16:44:12 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 16:44:12 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 145 bytes
17/12/20 16:44:12 INFO DAGScheduler: Got job 15 (take at <unknown>:0) with 1 output partitions
17/12/20 16:44:12 INFO DAGScheduler: Final stage: ResultStage 34 (take at <unknown>:0)
17/12/20 16:44:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)
17/12/20 16:44:12 INFO DAGScheduler: Missing parents: List()
17/12/20 16:44:12 INFO DAGScheduler: Submitting ResultStage 34 (WorkerRDD[116] at RDD at rdd.scala:18), which has no missing parents
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 121.8 KB, free 2003.3 MB)
17/12/20 16:44:12 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 46.5 KB, free 2003.2 MB)
17/12/20 16:44:12 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 127.0.0.1:57778 (size: 46.5 KB, free: 2004.4 MB)
17/12/20 16:44:12 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:996
17/12/20 16:44:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (WorkerRDD[116] at RDD at rdd.scala:18)
17/12/20 16:44:12 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks
17/12/20 16:44:12 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 32, localhost, executor driver, partition 0, PROCESS_LOCAL, 5914 bytes)
17/12/20 16:44:12 INFO Executor: Running task 0.0 in stage 34.0 (TID 32)
17/12/20 16:44:12 INFO BlockManager: Found block rdd_97_0 locally
17/12/20 16:44:13 INFO MemoryStore: Block rdd_116_0 stored as values in memory (estimated size 1600.0 B, free 2003.2 MB)
17/12/20 16:44:13 INFO BlockManagerInfo: Added rdd_116_0 in memory on 127.0.0.1:57778 (size: 1600.0 B, free: 2004.4 MB)
17/12/20 16:44:13 INFO Executor: Finished task 0.0 in stage 34.0 (TID 32). 3180 bytes result sent to driver
17/12/20 16:44:13 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 32) in 745 ms on localhost (executor driver) (1/1)
17/12/20 16:44:13 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
17/12/20 16:44:13 INFO DAGScheduler: ResultStage 34 (take at <unknown>:0) finished in 0.745 s
17/12/20 16:44:13 INFO DAGScheduler: Job 15 finished: take at <unknown>:0, took 0.749778 s
17/12/20 16:44:13 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 16:44:13 INFO DAGScheduler: Got job 16 (take at <unknown>:0) with 1 output partitions
17/12/20 16:44:13 INFO DAGScheduler: Final stage: ResultStage 36 (take at <unknown>:0)
17/12/20 16:44:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)
17/12/20 16:44:13 INFO DAGScheduler: Missing parents: List()
17/12/20 16:44:13 INFO DAGScheduler: Submitting ResultStage 36 (WorkerRDD[116] at RDD at rdd.scala:18), which has no missing parents
17/12/20 16:44:13 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 121.8 KB, free 2003.1 MB)
17/12/20 16:44:13 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 46.5 KB, free 2003.0 MB)
17/12/20 16:44:13 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 127.0.0.1:57778 (size: 46.5 KB, free: 2004.3 MB)
17/12/20 16:44:13 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:996
17/12/20 16:44:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (WorkerRDD[116] at RDD at rdd.scala:18)
17/12/20 16:44:13 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks
17/12/20 16:44:13 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 33, localhost, executor driver, partition 1, PROCESS_LOCAL, 5914 bytes)
17/12/20 16:44:13 INFO Executor: Running task 0.0 in stage 36.0 (TID 33)
17/12/20 16:44:13 INFO BlockManager: Found block rdd_97_1 locally
17/12/20 16:44:14 INFO MemoryStore: Block rdd_116_1 stored as values in memory (estimated size 1600.0 B, free 2003.0 MB)
17/12/20 16:44:14 INFO BlockManagerInfo: Added rdd_116_1 in memory on 127.0.0.1:57778 (size: 1600.0 B, free: 2004.3 MB)
17/12/20 16:44:14 WARN Executor: 1 block locks were not released by TID = 33:
[rdd_116_1]
17/12/20 16:44:14 INFO Executor: Finished task 0.0 in stage 36.0 (TID 33). 3090 bytes result sent to driver
17/12/20 16:44:14 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 33) in 738 ms on localhost (executor driver) (1/1)
17/12/20 16:44:14 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
17/12/20 16:44:14 INFO DAGScheduler: ResultStage 36 (take at <unknown>:0) finished in 0.739 s
17/12/20 16:44:14 INFO DAGScheduler: Job 16 finished: take at <unknown>:0, took 0.743721 s
17/12/20 16:44:14 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:14 INFO SparkSqlParser: Parsing command: sparklyr_tmp_163c43bc357c
17/12/20 16:44:14 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:14 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_163c43bc357c` AS `zzz13`
WHERE (0 = 1)
17/12/20 16:44:14 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:14 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_163c43bc357c`
17/12/20 16:44:14 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:44:14 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:14 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz14`
WHERE (0 = 1)
17/12/20 16:44:14 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:14 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.0375) AS `V1`, (`V2` < 0.0055) AS `V2`, (`V3` < 0.3) AS `V3`, (`V4` < 0.0375) AS `V4`, (`V5` < 0.009) AS `V5`, (`V6` < 0.075) AS `V6`, (`V7` < 0.0138) AS `V7`, (`V8` < 0.0375) AS `V8`, (`V9` < 0.0045) AS `V9`, (`V10` < 0.0185) AS `V10`
FROM `analyis_tbl`
17/12/20 16:44:14 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:44:14 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:14 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz15`
WHERE (0 = 1)
17/12/20 16:44:14 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:14 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 16:44:14 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 16:44:14 INFO DAGScheduler: Got job 17 (collect at utils.scala:196) with 2 output partitions
17/12/20 16:44:14 INFO DAGScheduler: Final stage: ResultStage 38 (collect at utils.scala:196)
17/12/20 16:44:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
17/12/20 16:44:14 INFO DAGScheduler: Missing parents: List()
17/12/20 16:44:14 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[123] at collect at utils.scala:196), which has no missing parents
17/12/20 16:44:14 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 134.6 KB, free 2002.9 MB)
17/12/20 16:44:14 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 51.1 KB, free 2002.9 MB)
17/12/20 16:44:14 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 127.0.0.1:57778 (size: 51.1 KB, free: 2004.3 MB)
17/12/20 16:44:14 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:996
17/12/20 16:44:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 38 (MapPartitionsRDD[123] at collect at utils.scala:196)
17/12/20 16:44:14 INFO TaskSchedulerImpl: Adding task set 38.0 with 2 tasks
17/12/20 16:44:14 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 5947 bytes)
17/12/20 16:44:14 INFO TaskSetManager: Starting task 1.0 in stage 38.0 (TID 35, localhost, executor driver, partition 1, PROCESS_LOCAL, 5947 bytes)
17/12/20 16:44:14 INFO Executor: Running task 0.0 in stage 38.0 (TID 34)
17/12/20 16:44:14 INFO Executor: Running task 1.0 in stage 38.0 (TID 35)
17/12/20 16:44:14 INFO BlockManager: Found block rdd_116_0 locally
17/12/20 16:44:14 INFO BlockManager: Found block rdd_116_1 locally
17/12/20 16:44:14 INFO Executor: Finished task 1.0 in stage 38.0 (TID 35). 1746 bytes result sent to driver
17/12/20 16:44:14 INFO Executor: Finished task 0.0 in stage 38.0 (TID 34). 1720 bytes result sent to driver
17/12/20 16:44:14 INFO TaskSetManager: Finished task 1.0 in stage 38.0 (TID 35) in 12 ms on localhost (executor driver) (1/2)
17/12/20 16:44:14 INFO DAGScheduler: ResultStage 38 (collect at utils.scala:196) finished in 0.013 s
17/12/20 16:44:14 INFO DAGScheduler: Job 17 finished: collect at utils.scala:196, took 0.019606 s
17/12/20 16:44:14 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 34) in 12 ms on localhost (executor driver) (2/2)
17/12/20 16:44:14 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
17/12/20 16:44:14 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:14 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:44:14 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:44:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:44:14 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:44:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:44:14 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:44:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:44:14 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:14 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:44:14 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:44:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:44:14 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:44:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:44:14 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:44:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:44:14 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:14 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:44:14 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:44:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:44:14 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:44:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:44:14 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:44:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:44:14 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:44:14 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:44:14 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:44:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:44:14 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:44:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:44:14 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:44:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:45:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:01 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:45:01 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:45:01 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:45:01 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:45:01 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:45:01 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:45:01 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:45:01 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 16:45:01 INFO DAGScheduler: Got job 18 (collect at utils.scala:58) with 1 output partitions
17/12/20 16:45:01 INFO DAGScheduler: Final stage: ResultStage 39 (collect at utils.scala:58)
17/12/20 16:45:01 INFO DAGScheduler: Parents of final stage: List()
17/12/20 16:45:01 INFO DAGScheduler: Missing parents: List()
17/12/20 16:45:01 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[133] at map at utils.scala:55), which has no missing parents
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 8.7 KB, free 2002.9 MB)
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2002.8 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 127.0.0.1:57778 (size: 4.6 KB, free: 2004.3 MB)
17/12/20 16:45:01 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:996
17/12/20 16:45:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[133] at map at utils.scala:55)
17/12/20 16:45:01 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks
17/12/20 16:45:01 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 6573 bytes)
17/12/20 16:45:01 INFO Executor: Running task 0.0 in stage 39.0 (TID 36)
17/12/20 16:45:01 INFO Executor: Finished task 0.0 in stage 39.0 (TID 36). 1246 bytes result sent to driver
17/12/20 16:45:01 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 36) in 8 ms on localhost (executor driver) (1/1)
17/12/20 16:45:01 INFO DAGScheduler: ResultStage 39 (collect at utils.scala:58) finished in 0.008 s
17/12/20 16:45:01 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
17/12/20 16:45:01 INFO DAGScheduler: Job 18 finished: collect at utils.scala:58, took 0.015501 s
17/12/20 16:45:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:01 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:45:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:01 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 16:45:01 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 16:45:01 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 16:45:01 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 16:45:01 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 16:45:01 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 293.7 KB, free 2002.6 MB)
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1143
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1144
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1145
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1146
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1147
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1148
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1149
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1150
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1151
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1152
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1153
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1154
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1155
17/12/20 16:45:01 INFO ContextCleaner: Cleaned shuffle 7
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2002.5 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 127.0.0.1:57778 in memory (size: 7.5 KB, free: 2004.3 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 127.0.0.1:57778 (size: 24.0 KB, free: 2004.3 MB)
17/12/20 16:45:01 INFO SparkContext: Created broadcast 31 from sql at <unknown>:0
17/12/20 16:45:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 16:45:01 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 127.0.0.1:57778 in memory (size: 6.9 KB, free: 2004.3 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 127.0.0.1:57778 in memory (size: 3.7 KB, free: 2004.3 MB)
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1324
17/12/20 16:45:01 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 127.0.0.1:57778 in memory (size: 6.9 KB, free: 2004.3 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 127.0.0.1:57778 in memory (size: 3.7 KB, free: 2004.3 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 127.0.0.1:57778 in memory (size: 46.5 KB, free: 2004.3 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 127.0.0.1:57778 in memory (size: 46.5 KB, free: 2004.4 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 127.0.0.1:57778 in memory (size: 51.1 KB, free: 2004.4 MB)
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1629
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1630
17/12/20 16:45:01 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 127.0.0.1:57778 in memory (size: 4.6 KB, free: 2004.4 MB)
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1679
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1680
17/12/20 16:45:01 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 127.0.0.1:57778 in memory (size: 51.1 KB, free: 2004.5 MB)
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1086
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1087
17/12/20 16:45:01 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 127.0.0.1:57778 in memory (size: 4.6 KB, free: 2004.5 MB)
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1136
17/12/20 16:45:01 INFO ContextCleaner: Cleaned accumulator 1137
17/12/20 16:45:01 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 16:45:01 INFO DAGScheduler: Registering RDD 137 (sql at <unknown>:0)
17/12/20 16:45:01 INFO DAGScheduler: Registering RDD 142 (sql at <unknown>:0)
17/12/20 16:45:01 INFO DAGScheduler: Got job 19 (sql at <unknown>:0) with 1 output partitions
17/12/20 16:45:01 INFO DAGScheduler: Final stage: ResultStage 42 (sql at <unknown>:0)
17/12/20 16:45:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)
17/12/20 16:45:01 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 41)
17/12/20 16:45:01 INFO DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[137] at sql at <unknown>:0), which has no missing parents
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 13.1 KB, free 2003.3 MB)
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2003.3 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 127.0.0.1:57778 (size: 7.5 KB, free: 2004.5 MB)
17/12/20 16:45:01 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:996
17/12/20 16:45:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[137] at sql at <unknown>:0)
17/12/20 16:45:01 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks
17/12/20 16:45:01 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 37, localhost, executor driver, partition 0, PROCESS_LOCAL, 6678 bytes)
17/12/20 16:45:01 INFO Executor: Running task 0.0 in stage 40.0 (TID 37)
17/12/20 16:45:01 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/RtmpYBQ1ot/spark_serialize_491b97433a8b622983c8750b694b20eec9bd24dc924079ea35edea13c727fc44.csv, range: 0-1918, partition values: [empty row]
17/12/20 16:45:01 INFO Executor: Finished task 0.0 in stage 40.0 (TID 37). 1719 bytes result sent to driver
17/12/20 16:45:01 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 37) in 34 ms on localhost (executor driver) (1/1)
17/12/20 16:45:01 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
17/12/20 16:45:01 INFO DAGScheduler: ShuffleMapStage 40 (sql at <unknown>:0) finished in 0.034 s
17/12/20 16:45:01 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:45:01 INFO DAGScheduler: running: Set()
17/12/20 16:45:01 INFO DAGScheduler: waiting: Set(ResultStage 42, ShuffleMapStage 41)
17/12/20 16:45:01 INFO DAGScheduler: failed: Set()
17/12/20 16:45:01 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[142] at sql at <unknown>:0), which has no missing parents
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 15.3 KB, free 2003.3 MB)
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2003.3 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 127.0.0.1:57778 (size: 6.9 KB, free: 2004.5 MB)
17/12/20 16:45:01 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:996
17/12/20 16:45:01 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[142] at sql at <unknown>:0)
17/12/20 16:45:01 INFO TaskSchedulerImpl: Adding task set 41.0 with 2 tasks
17/12/20 16:45:01 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 38, localhost, executor driver, partition 0, ANY, 5944 bytes)
17/12/20 16:45:01 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 39, localhost, executor driver, partition 1, ANY, 5944 bytes)
17/12/20 16:45:01 INFO Executor: Running task 0.0 in stage 41.0 (TID 38)
17/12/20 16:45:01 INFO Executor: Running task 1.0 in stage 41.0 (TID 39)
17/12/20 16:45:01 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 16:45:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 16:45:01 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 16:45:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 16:45:01 INFO MemoryStore: Block rdd_139_0 stored as values in memory (estimated size 1512.0 B, free 2003.3 MB)
17/12/20 16:45:01 INFO MemoryStore: Block rdd_139_1 stored as values in memory (estimated size 1512.0 B, free 2003.3 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Added rdd_139_0 in memory on 127.0.0.1:57778 (size: 1512.0 B, free: 2004.5 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Added rdd_139_1 in memory on 127.0.0.1:57778 (size: 1512.0 B, free: 2004.5 MB)
17/12/20 16:45:01 INFO Executor: Finished task 0.0 in stage 41.0 (TID 38). 3072 bytes result sent to driver
17/12/20 16:45:01 INFO Executor: Finished task 1.0 in stage 41.0 (TID 39). 3072 bytes result sent to driver
17/12/20 16:45:01 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 38) in 23 ms on localhost (executor driver) (1/2)
17/12/20 16:45:01 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 39) in 21 ms on localhost (executor driver) (2/2)
17/12/20 16:45:01 INFO DAGScheduler: ShuffleMapStage 41 (sql at <unknown>:0) finished in 0.023 s
17/12/20 16:45:01 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:45:01 INFO DAGScheduler: running: Set()
17/12/20 16:45:01 INFO DAGScheduler: waiting: Set(ResultStage 42)
17/12/20 16:45:01 INFO DAGScheduler: failed: Set()
17/12/20 16:45:01 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[145] at sql at <unknown>:0), which has no missing parents
17/12/20 16:45:01 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 7.0 KB, free 2003.3 MB)
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.3 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 127.0.0.1:57778 (size: 3.7 KB, free: 2004.5 MB)
17/12/20 16:45:01 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:996
17/12/20 16:45:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[145] at sql at <unknown>:0)
17/12/20 16:45:01 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
17/12/20 16:45:01 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 40, localhost, executor driver, partition 0, ANY, 5955 bytes)
17/12/20 16:45:01 INFO Executor: Running task 0.0 in stage 42.0 (TID 40)
17/12/20 16:45:01 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 16:45:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 16:45:01 INFO Executor: Finished task 0.0 in stage 42.0 (TID 40). 1952 bytes result sent to driver
17/12/20 16:45:01 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 40) in 3 ms on localhost (executor driver) (1/1)
17/12/20 16:45:01 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
17/12/20 16:45:01 INFO DAGScheduler: ResultStage 42 (sql at <unknown>:0) finished in 0.004 s
17/12/20 16:45:01 INFO DAGScheduler: Job 19 finished: sql at <unknown>:0, took 0.074754 s
17/12/20 16:45:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:01 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 16:45:01 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 16:45:01 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 145 bytes
17/12/20 16:45:01 INFO DAGScheduler: Registering RDD 149 (collect at utils.scala:196)
17/12/20 16:45:01 INFO DAGScheduler: Got job 20 (collect at utils.scala:196) with 1 output partitions
17/12/20 16:45:01 INFO DAGScheduler: Final stage: ResultStage 45 (collect at utils.scala:196)
17/12/20 16:45:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)
17/12/20 16:45:01 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 44)
17/12/20 16:45:01 INFO DAGScheduler: Submitting ShuffleMapStage 44 (MapPartitionsRDD[149] at collect at utils.scala:196), which has no missing parents
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 15.3 KB, free 2003.3 MB)
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 7.0 KB, free 2003.3 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 127.0.0.1:57778 (size: 7.0 KB, free: 2004.5 MB)
17/12/20 16:45:01 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:996
17/12/20 16:45:01 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 44 (MapPartitionsRDD[149] at collect at utils.scala:196)
17/12/20 16:45:01 INFO TaskSchedulerImpl: Adding task set 44.0 with 2 tasks
17/12/20 16:45:01 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 41, localhost, executor driver, partition 0, PROCESS_LOCAL, 5936 bytes)
17/12/20 16:45:01 INFO TaskSetManager: Starting task 1.0 in stage 44.0 (TID 42, localhost, executor driver, partition 1, PROCESS_LOCAL, 5936 bytes)
17/12/20 16:45:01 INFO Executor: Running task 0.0 in stage 44.0 (TID 41)
17/12/20 16:45:01 INFO Executor: Running task 1.0 in stage 44.0 (TID 42)
17/12/20 16:45:01 INFO BlockManager: Found block rdd_139_1 locally
17/12/20 16:45:01 INFO BlockManager: Found block rdd_139_0 locally
17/12/20 16:45:01 INFO Executor: Finished task 0.0 in stage 44.0 (TID 41). 1871 bytes result sent to driver
17/12/20 16:45:01 INFO Executor: Finished task 1.0 in stage 44.0 (TID 42). 1871 bytes result sent to driver
17/12/20 16:45:01 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 41) in 15 ms on localhost (executor driver) (1/2)
17/12/20 16:45:01 INFO DAGScheduler: ShuffleMapStage 44 (collect at utils.scala:196) finished in 0.017 s
17/12/20 16:45:01 INFO TaskSetManager: Finished task 1.0 in stage 44.0 (TID 42) in 15 ms on localhost (executor driver) (2/2)
17/12/20 16:45:01 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:45:01 INFO DAGScheduler: running: Set()
17/12/20 16:45:01 INFO DAGScheduler: waiting: Set(ResultStage 45)
17/12/20 16:45:01 INFO DAGScheduler: failed: Set()
17/12/20 16:45:01 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
17/12/20 16:45:01 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[152] at collect at utils.scala:196), which has no missing parents
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 7.0 KB, free 2003.3 MB)
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.3 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 127.0.0.1:57778 (size: 3.7 KB, free: 2004.5 MB)
17/12/20 16:45:01 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:996
17/12/20 16:45:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[152] at collect at utils.scala:196)
17/12/20 16:45:01 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks
17/12/20 16:45:01 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 43, localhost, executor driver, partition 0, ANY, 5947 bytes)
17/12/20 16:45:01 INFO Executor: Running task 0.0 in stage 45.0 (TID 43)
17/12/20 16:45:01 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 16:45:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 16:45:01 INFO Executor: Finished task 0.0 in stage 45.0 (TID 43). 1794 bytes result sent to driver
17/12/20 16:45:01 INFO DAGScheduler: ResultStage 45 (collect at utils.scala:196) finished in 0.002 s
17/12/20 16:45:01 INFO DAGScheduler: Job 20 finished: collect at utils.scala:196, took 0.029041 s
17/12/20 16:45:01 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 43) in 2 ms on localhost (executor driver) (1/1)
17/12/20 16:45:01 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
17/12/20 16:45:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:01 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz16`
WHERE (0 = 1)
17/12/20 16:45:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:01 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S9` + 0.94339811 * RANDN() AS `V1`, `S2` + 0.93273791 * RANDN() AS `V2`, `S3` + 0.94339811 * RANDN() AS `V3`, `S5` + 0.94339811 * RANDN() AS `V4`, `S10` + 0.93273791 * RANDN() AS `V5`, `S3` + 0.94339811 * RANDN() AS `V6`, `S8` + 0.93273791 * RANDN() AS `V7`, `S3` + 0.94339811 * RANDN() AS `V8`, `S10` + 0.93273791 * RANDN() AS `V9`, `S8` + 0.93273791 * RANDN() AS `V10`
FROM `analyis_tbl`) `pnnyjakbvw`
17/12/20 16:45:01 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:45:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:01 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz17`
WHERE (0 = 1)
17/12/20 16:45:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:01 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 16:45:01 INFO CodeGenerator: Code generated in 14.525926 ms
17/12/20 16:45:01 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 16:45:01 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 145 bytes
17/12/20 16:45:01 INFO DAGScheduler: Got job 21 (take at <unknown>:0) with 1 output partitions
17/12/20 16:45:01 INFO DAGScheduler: Final stage: ResultStage 47 (take at <unknown>:0)
17/12/20 16:45:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 46)
17/12/20 16:45:01 INFO DAGScheduler: Missing parents: List()
17/12/20 16:45:01 INFO DAGScheduler: Submitting ResultStage 47 (WorkerRDD[158] at RDD at rdd.scala:18), which has no missing parents
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 121.8 KB, free 2003.1 MB)
17/12/20 16:45:01 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 46.5 KB, free 2003.1 MB)
17/12/20 16:45:01 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 127.0.0.1:57778 (size: 46.5 KB, free: 2004.4 MB)
17/12/20 16:45:01 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:996
17/12/20 16:45:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (WorkerRDD[158] at RDD at rdd.scala:18)
17/12/20 16:45:01 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks
17/12/20 16:45:01 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 44, localhost, executor driver, partition 0, PROCESS_LOCAL, 5914 bytes)
17/12/20 16:45:01 INFO Executor: Running task 0.0 in stage 47.0 (TID 44)
17/12/20 16:45:01 INFO BlockManager: Found block rdd_139_0 locally
17/12/20 16:45:02 INFO MemoryStore: Block rdd_158_0 stored as values in memory (estimated size 1600.0 B, free 2003.1 MB)
17/12/20 16:45:02 INFO BlockManagerInfo: Added rdd_158_0 in memory on 127.0.0.1:57778 (size: 1600.0 B, free: 2004.4 MB)
17/12/20 16:45:02 INFO Executor: Finished task 0.0 in stage 47.0 (TID 44). 3090 bytes result sent to driver
17/12/20 16:45:02 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 44) in 701 ms on localhost (executor driver) (1/1)
17/12/20 16:45:02 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
17/12/20 16:45:02 INFO DAGScheduler: ResultStage 47 (take at <unknown>:0) finished in 0.701 s
17/12/20 16:45:02 INFO DAGScheduler: Job 21 finished: take at <unknown>:0, took 0.703909 s
17/12/20 16:45:02 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 16:45:02 INFO DAGScheduler: Got job 22 (take at <unknown>:0) with 1 output partitions
17/12/20 16:45:02 INFO DAGScheduler: Final stage: ResultStage 49 (take at <unknown>:0)
17/12/20 16:45:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)
17/12/20 16:45:02 INFO DAGScheduler: Missing parents: List()
17/12/20 16:45:02 INFO DAGScheduler: Submitting ResultStage 49 (WorkerRDD[158] at RDD at rdd.scala:18), which has no missing parents
17/12/20 16:45:02 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 121.8 KB, free 2003.0 MB)
17/12/20 16:45:02 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 46.5 KB, free 2002.9 MB)
17/12/20 16:45:02 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 127.0.0.1:57778 (size: 46.5 KB, free: 2004.4 MB)
17/12/20 16:45:02 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:996
17/12/20 16:45:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (WorkerRDD[158] at RDD at rdd.scala:18)
17/12/20 16:45:02 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks
17/12/20 16:45:02 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 45, localhost, executor driver, partition 1, PROCESS_LOCAL, 5914 bytes)
17/12/20 16:45:02 INFO Executor: Running task 0.0 in stage 49.0 (TID 45)
17/12/20 16:45:02 INFO BlockManager: Found block rdd_139_1 locally
17/12/20 16:45:03 INFO MemoryStore: Block rdd_158_1 stored as values in memory (estimated size 1600.0 B, free 2002.9 MB)
17/12/20 16:45:03 INFO BlockManagerInfo: Added rdd_158_1 in memory on 127.0.0.1:57778 (size: 1600.0 B, free: 2004.4 MB)
17/12/20 16:45:03 WARN Executor: 1 block locks were not released by TID = 45:
[rdd_158_1]
17/12/20 16:45:03 INFO Executor: Finished task 0.0 in stage 49.0 (TID 45). 3090 bytes result sent to driver
17/12/20 16:45:03 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 45) in 723 ms on localhost (executor driver) (1/1)
17/12/20 16:45:03 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool 
17/12/20 16:45:03 INFO DAGScheduler: ResultStage 49 (take at <unknown>:0) finished in 0.723 s
17/12/20 16:45:03 INFO DAGScheduler: Job 22 finished: take at <unknown>:0, took 0.727250 s
17/12/20 16:45:03 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:03 INFO SparkSqlParser: Parsing command: sparklyr_tmp_163c79ec2063
17/12/20 16:45:03 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:03 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_163c79ec2063` AS `zzz18`
WHERE (0 = 1)
17/12/20 16:45:03 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:03 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_163c79ec2063`
17/12/20 16:45:03 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:45:03 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:03 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz19`
WHERE (0 = 1)
17/12/20 16:45:03 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:03 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.0375) AS `V1`, (`V2` < 0.0055) AS `V2`, (`V3` < 0.3) AS `V3`, (`V4` < 0.0375) AS `V4`, (`V5` < 0.009) AS `V5`, (`V6` < 0.075) AS `V6`, (`V7` < 0.0138) AS `V7`, (`V8` < 0.0375) AS `V8`, (`V9` < 0.0045) AS `V9`, (`V10` < 0.0185) AS `V10`
FROM `analyis_tbl`
17/12/20 16:45:03 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 16:45:03 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:03 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz20`
WHERE (0 = 1)
17/12/20 16:45:03 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:03 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 16:45:03 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 16:45:03 INFO DAGScheduler: Got job 23 (collect at utils.scala:196) with 2 output partitions
17/12/20 16:45:03 INFO DAGScheduler: Final stage: ResultStage 51 (collect at utils.scala:196)
17/12/20 16:45:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 50)
17/12/20 16:45:03 INFO DAGScheduler: Missing parents: List()
17/12/20 16:45:03 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[165] at collect at utils.scala:196), which has no missing parents
17/12/20 16:45:03 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 134.6 KB, free 2002.8 MB)
17/12/20 16:45:03 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 51.1 KB, free 2002.7 MB)
17/12/20 16:45:03 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 127.0.0.1:57778 (size: 51.1 KB, free: 2004.3 MB)
17/12/20 16:45:03 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:996
17/12/20 16:45:03 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 51 (MapPartitionsRDD[165] at collect at utils.scala:196)
17/12/20 16:45:03 INFO TaskSchedulerImpl: Adding task set 51.0 with 2 tasks
17/12/20 16:45:03 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 46, localhost, executor driver, partition 0, PROCESS_LOCAL, 5947 bytes)
17/12/20 16:45:03 INFO TaskSetManager: Starting task 1.0 in stage 51.0 (TID 47, localhost, executor driver, partition 1, PROCESS_LOCAL, 5947 bytes)
17/12/20 16:45:03 INFO Executor: Running task 0.0 in stage 51.0 (TID 46)
17/12/20 16:45:03 INFO Executor: Running task 1.0 in stage 51.0 (TID 47)
17/12/20 16:45:03 INFO BlockManager: Found block rdd_158_1 locally
17/12/20 16:45:03 INFO BlockManager: Found block rdd_158_0 locally
17/12/20 16:45:03 INFO Executor: Finished task 1.0 in stage 51.0 (TID 47). 1724 bytes result sent to driver
17/12/20 16:45:03 INFO Executor: Finished task 0.0 in stage 51.0 (TID 46). 1722 bytes result sent to driver
17/12/20 16:45:03 INFO TaskSetManager: Finished task 1.0 in stage 51.0 (TID 47) in 9 ms on localhost (executor driver) (1/2)
17/12/20 16:45:03 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 46) in 11 ms on localhost (executor driver) (2/2)
17/12/20 16:45:03 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
17/12/20 16:45:03 INFO DAGScheduler: ResultStage 51 (collect at utils.scala:196) finished in 0.011 s
17/12/20 16:45:03 INFO DAGScheduler: Job 23 finished: collect at utils.scala:196, took 0.017482 s
17/12/20 16:45:03 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:03 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:45:03 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:45:03 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:45:03 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:45:03 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:45:03 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:45:03 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:45:03 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:03 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:45:03 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:45:03 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:45:03 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:45:03 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:45:03 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:45:03 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:45:03 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:03 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:45:03 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:45:03 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:45:03 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:45:03 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:45:03 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:45:03 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:45:03 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 16:45:03 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 16:45:03 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:45:03 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:45:03 INFO HiveMetaStore: 0: get_database: default
17/12/20 16:45:03 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 16:45:03 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 16:45:03 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 16:58:29 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 127.0.0.1:57778 in memory (size: 7.5 KB, free: 2004.3 MB)
17/12/20 16:58:29 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 127.0.0.1:57778 in memory (size: 51.1 KB, free: 2004.4 MB)
17/12/20 16:58:29 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 127.0.0.1:57778 in memory (size: 46.5 KB, free: 2004.4 MB)
17/12/20 16:58:29 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 127.0.0.1:57778 in memory (size: 46.5 KB, free: 2004.5 MB)
17/12/20 16:58:29 INFO ContextCleaner: Cleaned accumulator 1686
17/12/20 16:58:29 INFO ContextCleaner: Cleaned accumulator 1687
17/12/20 16:58:29 INFO ContextCleaner: Cleaned accumulator 1688
17/12/20 16:58:29 INFO ContextCleaner: Cleaned accumulator 1689
17/12/20 16:58:29 INFO ContextCleaner: Cleaned accumulator 1690
17/12/20 16:58:29 INFO ContextCleaner: Cleaned accumulator 1691
17/12/20 16:58:29 INFO ContextCleaner: Cleaned accumulator 1692
17/12/20 16:58:29 INFO ContextCleaner: Cleaned accumulator 1693
17/12/20 16:58:29 INFO ContextCleaner: Cleaned accumulator 1694
17/12/20 16:58:29 INFO ContextCleaner: Cleaned accumulator 1695
17/12/20 16:58:29 INFO ContextCleaner: Cleaned accumulator 1696
17/12/20 16:58:29 INFO ContextCleaner: Cleaned accumulator 1697
17/12/20 16:58:29 INFO ContextCleaner: Cleaned accumulator 1698
17/12/20 16:58:29 INFO ContextCleaner: Cleaned shuffle 10
17/12/20 16:58:29 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 127.0.0.1:57778 in memory (size: 6.9 KB, free: 2004.5 MB)
17/12/20 16:58:29 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 127.0.0.1:57778 in memory (size: 3.7 KB, free: 2004.5 MB)
17/12/20 16:58:29 INFO ContextCleaner: Cleaned accumulator 1867
17/12/20 16:58:29 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 127.0.0.1:57778 in memory (size: 7.0 KB, free: 2004.5 MB)
17/12/20 16:58:29 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 127.0.0.1:57778 in memory (size: 3.7 KB, free: 2004.5 MB)
17/12/20 17:28:12 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@dd1e083,BlockManagerId(driver, 127.0.0.1, 57778, None))] in 1 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:538)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:567)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:567)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:567)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:567)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.runAndReset(Unknown Source)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(Unknown Source)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:81)
	... 14 more
17/12/20 17:28:25 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@dd1e083,BlockManagerId(driver, 127.0.0.1, 57778, None))] in 2 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:538)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:567)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:567)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:567)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:567)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.runAndReset(Unknown Source)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(Unknown Source)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:81)
	... 14 more
17/12/20 17:28:28 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
17/12/20 17:28:28 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
17/12/20 18:16:11 INFO SparkContext: Running Spark version 2.1.0
17/12/20 18:16:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/12/20 18:16:11 INFO SecurityManager: Changing view acls to: conan
17/12/20 18:16:11 INFO SecurityManager: Changing modify acls to: conan
17/12/20 18:16:11 INFO SecurityManager: Changing view acls groups to: 
17/12/20 18:16:11 INFO SecurityManager: Changing modify acls groups to: 
17/12/20 18:16:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(conan); groups with view permissions: Set(); users  with modify permissions: Set(conan); groups with modify permissions: Set()
17/12/20 18:16:11 INFO Utils: Successfully started service 'sparkDriver' on port 64647.
17/12/20 18:16:11 INFO SparkEnv: Registering MapOutputTracker
17/12/20 18:16:11 INFO SparkEnv: Registering BlockManagerMaster
17/12/20 18:16:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/12/20 18:16:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/12/20 18:16:11 INFO DiskBlockManager: Created local directory at C:\Users\conan\AppData\Local\Temp\blockmgr-a41c1419-6879-4bfe-848d-33d2ca3d20ff
17/12/20 18:16:11 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
17/12/20 18:16:11 INFO SparkEnv: Registering OutputCommitCoordinator
17/12/20 18:16:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/12/20 18:16:12 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
17/12/20 18:16:12 INFO SparkContext: Added JAR file:/C:/Users/conan/Documents/R/win-library/3.4/sparklyr/java/sparklyr-2.1-2.11.jar at spark://127.0.0.1:64647/jars/sparklyr-2.1-2.11.jar with timestamp 1513793772107
17/12/20 18:16:12 INFO Executor: Starting executor ID driver on host localhost
17/12/20 18:16:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64668.
17/12/20 18:16:12 INFO NettyBlockTransferService: Server created on 127.0.0.1:64668
17/12/20 18:16:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/12/20 18:16:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 64668, None)
17/12/20 18:16:12 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:64668 with 2004.6 MB RAM, BlockManagerId(driver, 127.0.0.1, 64668, None)
17/12/20 18:16:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 64668, None)
17/12/20 18:16:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 64668, None)
17/12/20 18:16:13 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
17/12/20 18:16:13 INFO SharedState: Warehouse path is 'C:UsersconanAppDataLocalsparkspark-2.1.0-bin-hadoop2.7	mphive'.
17/12/20 18:16:13 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/12/20 18:16:14 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/12/20 18:16:14 INFO ObjectStore: ObjectStore, initialize called
17/12/20 18:16:15 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/12/20 18:16:15 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/12/20 18:16:17 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/12/20 18:16:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 18:16:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 18:16:20 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 18:16:20 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 18:16:20 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/12/20 18:16:20 INFO ObjectStore: Initialized ObjectStore
17/12/20 18:16:20 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/12/20 18:16:21 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/12/20 18:16:22 INFO HiveMetaStore: Added admin role in metastore
17/12/20 18:16:22 INFO HiveMetaStore: Added public role in metastore
17/12/20 18:16:22 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/12/20 18:16:22 INFO HiveMetaStore: 0: get_all_databases
17/12/20 18:16:22 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_all_databases	
17/12/20 18:16:22 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/12/20 18:16:22 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/12/20 18:16:22 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 18:16:22 INFO SessionState: Created local directory: C:/Users/conan/AppData/Local/Temp/3b073ed7-44b1-41da-b2ad-c11970a73c01_resources
17/12/20 18:16:23 INFO SessionState: Created HDFS directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/conan/3b073ed7-44b1-41da-b2ad-c11970a73c01
17/12/20 18:16:23 INFO SessionState: Created local directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/3b073ed7-44b1-41da-b2ad-c11970a73c01
17/12/20 18:16:23 INFO SessionState: Created HDFS directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/conan/3b073ed7-44b1-41da-b2ad-c11970a73c01/_tmp_space.db
17/12/20 18:16:23 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:UsersconanAppDataLocalsparkspark-2.1.0-bin-hadoop2.7	mphive
17/12/20 18:16:23 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:23 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:23 INFO HiveMetaStore: 0: get_database: global_temp
17/12/20 18:16:23 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: global_temp	
17/12/20 18:16:23 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
17/12/20 18:16:23 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:16:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:28 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:16:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:16:29 INFO CodeGenerator: Code generated in 575.417066 ms
17/12/20 18:16:30 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 18:16:30 INFO DAGScheduler: Got job 0 (collect at utils.scala:58) with 1 output partitions
17/12/20 18:16:30 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:58)
17/12/20 18:16:30 INFO DAGScheduler: Parents of final stage: List()
17/12/20 18:16:30 INFO DAGScheduler: Missing parents: List()
17/12/20 18:16:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at map at utils.scala:55), which has no missing parents
17/12/20 18:16:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 8.7 KB, free 2004.6 MB)
17/12/20 18:16:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2004.6 MB)
17/12/20 18:16:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:64668 (size: 4.6 KB, free: 2004.6 MB)
17/12/20 18:16:31 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996
17/12/20 18:16:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at map at utils.scala:55)
17/12/20 18:16:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/12/20 18:16:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6041 bytes)
17/12/20 18:16:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/12/20 18:16:32 INFO Executor: Fetching spark://127.0.0.1:64647/jars/sparklyr-2.1-2.11.jar with timestamp 1513793772107
17/12/20 18:16:32 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:64647 after 42 ms (0 ms spent in bootstraps)
17/12/20 18:16:32 INFO Utils: Fetching spark://127.0.0.1:64647/jars/sparklyr-2.1-2.11.jar to C:\Users\conan\AppData\Local\Temp\spark-6996707d-1ff2-4999-a3cb-15fb5dfb9c61\userFiles-f1cd7b40-4ded-4bf1-89c7-4d02f67e2f29\fetchFileTemp4941153269696611893.tmp
17/12/20 18:16:32 INFO Executor: Adding file:/C:/Users/conan/AppData/Local/Temp/spark-6996707d-1ff2-4999-a3cb-15fb5dfb9c61/userFiles-f1cd7b40-4ded-4bf1-89c7-4d02f67e2f29/sparklyr-2.1-2.11.jar to class loader
17/12/20 18:16:32 INFO CodeGenerator: Code generated in 28.439017 ms
17/12/20 18:16:32 INFO CodeGenerator: Code generated in 27.508657 ms
17/12/20 18:16:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1231 bytes result sent to driver
17/12/20 18:16:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 872 ms on localhost (executor driver) (1/1)
17/12/20 18:16:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/12/20 18:16:32 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:58) finished in 0.937 s
17/12/20 18:16:32 INFO DAGScheduler: Job 0 finished: collect at utils.scala:58, took 2.862272 s
17/12/20 18:16:33 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:64668 in memory (size: 4.6 KB, free: 2004.6 MB)
17/12/20 18:16:33 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:16:33 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:33 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 18:16:33 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 18:16:33 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 18:16:33 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 18:16:33 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 18:16:33 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 18:16:33 INFO CodeGenerator: Code generated in 12.104111 ms
17/12/20 18:16:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 293.7 KB, free 2004.3 MB)
17/12/20 18:16:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2004.3 MB)
17/12/20 18:16:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:64668 (size: 24.0 KB, free: 2004.6 MB)
17/12/20 18:16:34 INFO SparkContext: Created broadcast 1 from sql at <unknown>:0
17/12/20 18:16:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 18:16:34 INFO CodeGenerator: Code generated in 33.631131 ms
17/12/20 18:16:34 INFO CodeGenerator: Code generated in 20.032933 ms
17/12/20 18:16:34 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 18:16:34 INFO DAGScheduler: Registering RDD 9 (sql at <unknown>:0)
17/12/20 18:16:34 INFO DAGScheduler: Registering RDD 14 (sql at <unknown>:0)
17/12/20 18:16:34 INFO DAGScheduler: Got job 1 (sql at <unknown>:0) with 1 output partitions
17/12/20 18:16:34 INFO DAGScheduler: Final stage: ResultStage 3 (sql at <unknown>:0)
17/12/20 18:16:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
17/12/20 18:16:34 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
17/12/20 18:16:34 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[9] at sql at <unknown>:0), which has no missing parents
17/12/20 18:16:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.1 KB, free 2004.3 MB)
17/12/20 18:16:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2004.3 MB)
17/12/20 18:16:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:64668 (size: 7.5 KB, free: 2004.6 MB)
17/12/20 18:16:34 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/12/20 18:16:34 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[9] at sql at <unknown>:0)
17/12/20 18:16:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/12/20 18:16:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 6676 bytes)
17/12/20 18:16:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/12/20 18:16:35 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/RtmpusRKRl/spark_serialize_26492c7f0ec316381bb5d11641e5091169731063140d11120160d1977c186101.csv, range: 0-1910, partition values: [empty row]
17/12/20 18:16:35 INFO CodeGenerator: Code generated in 23.877461 ms
17/12/20 18:16:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1896 bytes result sent to driver
17/12/20 18:16:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 531 ms on localhost (executor driver) (1/1)
17/12/20 18:16:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/12/20 18:16:35 INFO DAGScheduler: ShuffleMapStage 1 (sql at <unknown>:0) finished in 0.531 s
17/12/20 18:16:35 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:16:35 INFO DAGScheduler: running: Set()
17/12/20 18:16:35 INFO DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)
17/12/20 18:16:35 INFO DAGScheduler: failed: Set()
17/12/20 18:16:35 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[14] at sql at <unknown>:0), which has no missing parents
17/12/20 18:16:35 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.3 KB, free 2004.3 MB)
17/12/20 18:16:35 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2004.2 MB)
17/12/20 18:16:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:64668 (size: 6.9 KB, free: 2004.6 MB)
17/12/20 18:16:35 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/12/20 18:16:35 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[14] at sql at <unknown>:0)
17/12/20 18:16:35 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
17/12/20 18:16:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5942 bytes)
17/12/20 18:16:35 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3, localhost, executor driver, partition 1, ANY, 5942 bytes)
17/12/20 18:16:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/12/20 18:16:35 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)
17/12/20 18:16:35 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 18:16:35 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 18:16:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
17/12/20 18:16:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
17/12/20 18:16:35 INFO MemoryStore: Block rdd_11_1 stored as values in memory (estimated size 1512.0 B, free 2004.2 MB)
17/12/20 18:16:35 INFO MemoryStore: Block rdd_11_0 stored as values in memory (estimated size 1512.0 B, free 2004.2 MB)
17/12/20 18:16:35 INFO BlockManagerInfo: Added rdd_11_1 in memory on 127.0.0.1:64668 (size: 1512.0 B, free: 2004.6 MB)
17/12/20 18:16:35 INFO BlockManagerInfo: Added rdd_11_0 in memory on 127.0.0.1:64668 (size: 1512.0 B, free: 2004.6 MB)
17/12/20 18:16:35 INFO CodeGenerator: Code generated in 8.848609 ms
17/12/20 18:16:35 INFO CodeGenerator: Code generated in 42.729321 ms
17/12/20 18:16:35 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 3241 bytes result sent to driver
17/12/20 18:16:35 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3241 bytes result sent to driver
17/12/20 18:16:35 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 304 ms on localhost (executor driver) (1/2)
17/12/20 18:16:35 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 323 ms on localhost (executor driver) (2/2)
17/12/20 18:16:35 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/12/20 18:16:35 INFO DAGScheduler: ShuffleMapStage 2 (sql at <unknown>:0) finished in 0.323 s
17/12/20 18:16:35 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:16:35 INFO DAGScheduler: running: Set()
17/12/20 18:16:35 INFO DAGScheduler: waiting: Set(ResultStage 3)
17/12/20 18:16:35 INFO DAGScheduler: failed: Set()
17/12/20 18:16:35 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at sql at <unknown>:0), which has no missing parents
17/12/20 18:16:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 2004.2 MB)
17/12/20 18:16:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2004.2 MB)
17/12/20 18:16:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:64668 (size: 3.7 KB, free: 2004.6 MB)
17/12/20 18:16:35 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:996
17/12/20 18:16:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at sql at <unknown>:0)
17/12/20 18:16:35 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/12/20 18:16:35 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4, localhost, executor driver, partition 0, ANY, 5953 bytes)
17/12/20 18:16:35 INFO Executor: Running task 0.0 in stage 3.0 (TID 4)
17/12/20 18:16:35 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 18:16:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:16:35 INFO Executor: Finished task 0.0 in stage 3.0 (TID 4). 1786 bytes result sent to driver
17/12/20 18:16:35 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 11 ms on localhost (executor driver) (1/1)
17/12/20 18:16:35 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/12/20 18:16:35 INFO DAGScheduler: ResultStage 3 (sql at <unknown>:0) finished in 0.011 s
17/12/20 18:16:35 INFO DAGScheduler: Job 1 finished: sql at <unknown>:0, took 0.998396 s
17/12/20 18:16:35 INFO CodeGenerator: Code generated in 11.327805 ms
17/12/20 18:16:35 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:35 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 18:16:36 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 18:16:36 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 145 bytes
17/12/20 18:16:36 INFO DAGScheduler: Registering RDD 21 (collect at utils.scala:196)
17/12/20 18:16:36 INFO DAGScheduler: Got job 2 (collect at utils.scala:196) with 1 output partitions
17/12/20 18:16:36 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:196)
17/12/20 18:16:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/12/20 18:16:36 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/12/20 18:16:36 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[21] at collect at utils.scala:196), which has no missing parents
17/12/20 18:16:36 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.3 KB, free 2004.2 MB)
17/12/20 18:16:36 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2004.2 MB)
17/12/20 18:16:36 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:64668 (size: 6.9 KB, free: 2004.5 MB)
17/12/20 18:16:36 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:996
17/12/20 18:16:36 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[21] at collect at utils.scala:196)
17/12/20 18:16:36 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/12/20 18:16:36 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 5934 bytes)
17/12/20 18:16:36 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6, localhost, executor driver, partition 1, PROCESS_LOCAL, 5934 bytes)
17/12/20 18:16:36 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
17/12/20 18:16:36 INFO Executor: Running task 1.0 in stage 5.0 (TID 6)
17/12/20 18:16:36 INFO BlockManager: Found block rdd_11_1 locally
17/12/20 18:16:36 INFO BlockManager: Found block rdd_11_0 locally
17/12/20 18:16:36 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1950 bytes result sent to driver
17/12/20 18:16:36 INFO Executor: Finished task 1.0 in stage 5.0 (TID 6). 1950 bytes result sent to driver
17/12/20 18:16:36 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 36 ms on localhost (executor driver) (1/2)
17/12/20 18:16:36 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:196) finished in 0.036 s
17/12/20 18:16:36 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:16:36 INFO DAGScheduler: running: Set()
17/12/20 18:16:36 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/12/20 18:16:36 INFO DAGScheduler: failed: Set()
17/12/20 18:16:36 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[24] at collect at utils.scala:196), which has no missing parents
17/12/20 18:16:36 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.0 KB, free 2004.2 MB)
17/12/20 18:16:36 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 28 ms on localhost (executor driver) (2/2)
17/12/20 18:16:36 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/12/20 18:16:36 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2004.2 MB)
17/12/20 18:16:36 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:64668 (size: 3.7 KB, free: 2004.5 MB)
17/12/20 18:16:36 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:996
17/12/20 18:16:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[24] at collect at utils.scala:196)
17/12/20 18:16:36 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/12/20 18:16:36 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 7, localhost, executor driver, partition 0, ANY, 5945 bytes)
17/12/20 18:16:36 INFO Executor: Running task 0.0 in stage 6.0 (TID 7)
17/12/20 18:16:36 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 18:16:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:16:36 INFO Executor: Finished task 0.0 in stage 6.0 (TID 7). 1794 bytes result sent to driver
17/12/20 18:16:36 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:196) finished in 0.014 s
17/12/20 18:16:36 INFO DAGScheduler: Job 2 finished: collect at utils.scala:196, took 0.098535 s
17/12/20 18:16:36 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 7) in 13 ms on localhost (executor driver) (1/1)
17/12/20 18:16:36 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/12/20 18:16:36 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:36 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz1`
WHERE (0 = 1)
17/12/20 18:16:36 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:36 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S3` + 0.91651514 * RANDN() AS `V1`, `S10` + 0.9486833 * RANDN() AS `V2`, `S10` + 0.9486833 * RANDN() AS `V3`, `S10` + 0.9486833 * RANDN() AS `V4`, `S5` + 0.9 * RANDN() AS `V5`, `S2` + 0.9486833 * RANDN() AS `V6`, `S8` + 0.9 * RANDN() AS `V7`, `S7` + 0.91104336 * RANDN() AS `V8`, `S4` + 0.93808315 * RANDN() AS `V9`, `S8` + 0.9 * RANDN() AS `V10`
FROM `analyis_tbl`) `rfkphjezax`
17/12/20 18:16:36 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:16:36 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:36 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz2`
WHERE (0 = 1)
17/12/20 18:16:37 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:37 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 50
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 51
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 57
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 58
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 59
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 60
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 61
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 62
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 63
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 64
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 65
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 66
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 67
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 68
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 69
17/12/20 18:16:37 INFO ContextCleaner: Cleaned shuffle 1
17/12/20 18:16:37 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:64668 in memory (size: 7.5 KB, free: 2004.6 MB)
17/12/20 18:16:37 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:64668 in memory (size: 6.9 KB, free: 2004.6 MB)
17/12/20 18:16:37 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:64668 in memory (size: 3.7 KB, free: 2004.6 MB)
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 238
17/12/20 18:16:37 INFO CodeGenerator: Code generated in 53.891368 ms
17/12/20 18:16:37 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:64668 in memory (size: 6.9 KB, free: 2004.6 MB)
17/12/20 18:16:37 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 127.0.0.1:64668 in memory (size: 3.7 KB, free: 2004.6 MB)
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 1
17/12/20 18:16:37 INFO ContextCleaner: Cleaned accumulator 0
17/12/20 18:16:39 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 18:16:39 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 145 bytes
17/12/20 18:16:39 INFO DAGScheduler: Got job 3 (take at <unknown>:0) with 1 output partitions
17/12/20 18:16:39 INFO DAGScheduler: Final stage: ResultStage 8 (take at <unknown>:0)
17/12/20 18:16:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
17/12/20 18:16:39 INFO DAGScheduler: Missing parents: List()
17/12/20 18:16:39 INFO DAGScheduler: Submitting ResultStage 8 (WorkerRDD[30] at RDD at rdd.scala:18), which has no missing parents
17/12/20 18:16:39 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 119.5 KB, free 2004.2 MB)
17/12/20 18:16:39 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 45.6 KB, free 2004.1 MB)
17/12/20 18:16:39 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:64668 (size: 45.6 KB, free: 2004.5 MB)
17/12/20 18:16:39 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:996
17/12/20 18:16:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (WorkerRDD[30] at RDD at rdd.scala:18)
17/12/20 18:16:39 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/12/20 18:16:39 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5913 bytes)
17/12/20 18:16:39 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
17/12/20 18:16:39 INFO BlockManager: Found block rdd_11_0 locally
17/12/20 18:16:39 INFO CodeGenerator: Code generated in 70.809635 ms
17/12/20 18:16:39 INFO CodeGenerator: Code generated in 22.184766 ms
17/12/20 18:16:40 INFO MemoryStore: Block rdd_30_0 stored as values in memory (estimated size 1600.0 B, free 2004.1 MB)
17/12/20 18:16:40 INFO BlockManagerInfo: Added rdd_30_0 in memory on 127.0.0.1:64668 (size: 1600.0 B, free: 2004.5 MB)
17/12/20 18:16:40 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 3256 bytes result sent to driver
17/12/20 18:16:40 INFO DAGScheduler: ResultStage 8 (take at <unknown>:0) finished in 1.379 s
17/12/20 18:16:40 INFO DAGScheduler: Job 3 finished: take at <unknown>:0, took 1.401375 s
17/12/20 18:16:40 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 1379 ms on localhost (executor driver) (1/1)
17/12/20 18:16:40 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/12/20 18:16:40 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 18:16:40 INFO DAGScheduler: Got job 4 (take at <unknown>:0) with 1 output partitions
17/12/20 18:16:40 INFO DAGScheduler: Final stage: ResultStage 10 (take at <unknown>:0)
17/12/20 18:16:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
17/12/20 18:16:40 INFO DAGScheduler: Missing parents: List()
17/12/20 18:16:40 INFO DAGScheduler: Submitting ResultStage 10 (WorkerRDD[30] at RDD at rdd.scala:18), which has no missing parents
17/12/20 18:16:40 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 119.5 KB, free 2004.0 MB)
17/12/20 18:16:40 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 45.6 KB, free 2004.0 MB)
17/12/20 18:16:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:64668 (size: 45.6 KB, free: 2004.5 MB)
17/12/20 18:16:40 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:996
17/12/20 18:16:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (WorkerRDD[30] at RDD at rdd.scala:18)
17/12/20 18:16:40 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
17/12/20 18:16:40 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 5913 bytes)
17/12/20 18:16:40 INFO Executor: Running task 0.0 in stage 10.0 (TID 9)
17/12/20 18:16:40 INFO BlockManager: Found block rdd_11_1 locally
17/12/20 18:16:41 INFO MemoryStore: Block rdd_30_1 stored as values in memory (estimated size 1600.0 B, free 2004.0 MB)
17/12/20 18:16:41 INFO BlockManagerInfo: Added rdd_30_1 in memory on 127.0.0.1:64668 (size: 1600.0 B, free: 2004.5 MB)
17/12/20 18:16:41 WARN Executor: 1 block locks were not released by TID = 9:
[rdd_30_1]
17/12/20 18:16:41 INFO Executor: Finished task 0.0 in stage 10.0 (TID 9). 3082 bytes result sent to driver
17/12/20 18:16:41 INFO DAGScheduler: ResultStage 10 (take at <unknown>:0) finished in 1.082 s
17/12/20 18:16:41 INFO DAGScheduler: Job 4 finished: take at <unknown>:0, took 1.101901 s
17/12/20 18:16:41 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 1082 ms on localhost (executor driver) (1/1)
17/12/20 18:16:41 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
17/12/20 18:16:41 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:41 INFO SparkSqlParser: Parsing command: sparklyr_tmp_1c0c7f567c88
17/12/20 18:16:41 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:41 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0c7f567c88` AS `zzz3`
WHERE (0 = 1)
17/12/20 18:16:41 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:41 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0c7f567c88`
17/12/20 18:16:41 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:16:41 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:41 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz4`
WHERE (0 = 1)
17/12/20 18:16:41 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:41 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.0023) AS `V1`, (`V2` < 0.0138) AS `V2`, (`V3` < 0.0185) AS `V3`, (`V4` < 0.009) AS `V4`, (`V5` < 0.0375) AS `V5`, (`V6` < 0.054) AS `V6`, (`V7` < 0.054) AS `V7`, (`V8` < 0.0185) AS `V8`, (`V9` < 0.026) AS `V9`, (`V10` < 0.026) AS `V10`
FROM `analyis_tbl`
17/12/20 18:16:42 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:16:42 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:42 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz5`
WHERE (0 = 1)
17/12/20 18:16:42 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:42 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 18:16:42 INFO CodeGenerator: Code generated in 35.116535 ms
17/12/20 18:16:42 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 18:16:42 INFO DAGScheduler: Got job 5 (collect at utils.scala:196) with 2 output partitions
17/12/20 18:16:42 INFO DAGScheduler: Final stage: ResultStage 12 (collect at utils.scala:196)
17/12/20 18:16:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
17/12/20 18:16:42 INFO DAGScheduler: Missing parents: List()
17/12/20 18:16:42 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[37] at collect at utils.scala:196), which has no missing parents
17/12/20 18:16:42 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 132.3 KB, free 2003.8 MB)
17/12/20 18:16:42 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 50.4 KB, free 2003.8 MB)
17/12/20 18:16:42 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 127.0.0.1:64668 (size: 50.4 KB, free: 2004.4 MB)
17/12/20 18:16:42 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:996
17/12/20 18:16:42 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[37] at collect at utils.scala:196)
17/12/20 18:16:42 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks
17/12/20 18:16:42 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 5945 bytes)
17/12/20 18:16:42 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 11, localhost, executor driver, partition 1, PROCESS_LOCAL, 5945 bytes)
17/12/20 18:16:42 INFO Executor: Running task 0.0 in stage 12.0 (TID 10)
17/12/20 18:16:42 INFO Executor: Running task 1.0 in stage 12.0 (TID 11)
17/12/20 18:16:42 INFO BlockManager: Found block rdd_30_0 locally
17/12/20 18:16:42 INFO BlockManager: Found block rdd_30_1 locally
17/12/20 18:16:42 INFO CodeGenerator: Code generated in 115.883356 ms
17/12/20 18:16:42 INFO Executor: Finished task 1.0 in stage 12.0 (TID 11). 1775 bytes result sent to driver
17/12/20 18:16:42 INFO Executor: Finished task 0.0 in stage 12.0 (TID 10). 1796 bytes result sent to driver
17/12/20 18:16:42 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 11) in 156 ms on localhost (executor driver) (1/2)
17/12/20 18:16:42 INFO DAGScheduler: ResultStage 12 (collect at utils.scala:196) finished in 0.156 s
17/12/20 18:16:42 INFO DAGScheduler: Job 5 finished: collect at utils.scala:196, took 0.179503 s
17/12/20 18:16:42 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 156 ms on localhost (executor driver) (2/2)
17/12/20 18:16:42 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
17/12/20 18:16:42 INFO CodeGenerator: Code generated in 16.590528 ms
17/12/20 18:16:42 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:42 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:16:42 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:42 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:42 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:42 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:42 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:16:42 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:16:42 INFO CodeGenerator: Code generated in 10.622483 ms
17/12/20 18:16:42 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:42 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:16:42 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:42 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:42 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:42 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:42 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:16:42 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:16:43 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:43 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:16:43 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:43 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:43 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:43 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:43 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:16:43 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:16:43 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:43 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:16:43 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:43 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:43 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:43 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:43 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:16:43 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:16:43 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:43 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:16:43 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:43 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:43 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:43 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:43 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:16:43 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:16:43 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:16:43 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:16:43 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:43 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:43 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:16:43 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:16:43 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:16:43 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:18:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:18 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:18:18 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:18:18 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:18:18 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:18:18 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:18:18 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:18:18 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:18:18 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 18:18:18 INFO DAGScheduler: Got job 6 (collect at utils.scala:58) with 1 output partitions
17/12/20 18:18:18 INFO DAGScheduler: Final stage: ResultStage 13 (collect at utils.scala:58)
17/12/20 18:18:18 INFO DAGScheduler: Parents of final stage: List()
17/12/20 18:18:18 INFO DAGScheduler: Missing parents: List()
17/12/20 18:18:18 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[49] at map at utils.scala:55), which has no missing parents
17/12/20 18:18:18 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 8.7 KB, free 2003.8 MB)
17/12/20 18:18:18 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2003.8 MB)
17/12/20 18:18:18 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 127.0.0.1:64668 (size: 4.6 KB, free: 2004.4 MB)
17/12/20 18:18:18 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:996
17/12/20 18:18:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[49] at map at utils.scala:55)
17/12/20 18:18:18 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
17/12/20 18:18:18 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 6433 bytes)
17/12/20 18:18:18 INFO Executor: Running task 0.0 in stage 13.0 (TID 12)
17/12/20 18:18:18 INFO Executor: Finished task 0.0 in stage 13.0 (TID 12). 1194 bytes result sent to driver
17/12/20 18:18:18 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 12) in 16 ms on localhost (executor driver) (1/1)
17/12/20 18:18:18 INFO DAGScheduler: ResultStage 13 (collect at utils.scala:58) finished in 0.018 s
17/12/20 18:18:18 INFO DAGScheduler: Job 6 finished: collect at utils.scala:58, took 0.024070 s
17/12/20 18:18:18 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
17/12/20 18:18:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:18 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:18:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:18 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 18:18:18 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 18:18:18 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 18:18:18 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 18:18:18 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 18:18:18 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 18:18:18 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 293.7 KB, free 2003.5 MB)
17/12/20 18:18:18 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2003.5 MB)
17/12/20 18:18:18 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 127.0.0.1:64668 (size: 24.0 KB, free: 2004.4 MB)
17/12/20 18:18:18 INFO SparkContext: Created broadcast 11 from sql at <unknown>:0
17/12/20 18:18:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 18:18:18 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 18:18:18 INFO DAGScheduler: Registering RDD 53 (sql at <unknown>:0)
17/12/20 18:18:18 INFO DAGScheduler: Registering RDD 58 (sql at <unknown>:0)
17/12/20 18:18:18 INFO DAGScheduler: Got job 7 (sql at <unknown>:0) with 1 output partitions
17/12/20 18:18:18 INFO DAGScheduler: Final stage: ResultStage 16 (sql at <unknown>:0)
17/12/20 18:18:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)
17/12/20 18:18:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 15)
17/12/20 18:18:18 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[53] at sql at <unknown>:0), which has no missing parents
17/12/20 18:18:18 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 13.1 KB, free 2003.4 MB)
17/12/20 18:18:18 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2003.4 MB)
17/12/20 18:18:18 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 127.0.0.1:64668 (size: 7.5 KB, free: 2004.4 MB)
17/12/20 18:18:18 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:996
17/12/20 18:18:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[53] at sql at <unknown>:0)
17/12/20 18:18:18 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
17/12/20 18:18:18 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 6677 bytes)
17/12/20 18:18:18 INFO Executor: Running task 0.0 in stage 14.0 (TID 13)
17/12/20 18:18:18 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/RtmpusRKRl/spark_serialize_e4bfe492797ca777fcb922b70de3141fcc1c452b95d5c71c8cfcd5657a934ecf.csv, range: 0-1922, partition values: [empty row]
17/12/20 18:18:18 INFO Executor: Finished task 0.0 in stage 14.0 (TID 13). 1809 bytes result sent to driver
17/12/20 18:18:18 INFO DAGScheduler: ShuffleMapStage 14 (sql at <unknown>:0) finished in 0.043 s
17/12/20 18:18:18 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:18:18 INFO DAGScheduler: running: Set()
17/12/20 18:18:18 INFO DAGScheduler: waiting: Set(ShuffleMapStage 15, ResultStage 16)
17/12/20 18:18:18 INFO DAGScheduler: failed: Set()
17/12/20 18:18:18 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 13) in 43 ms on localhost (executor driver) (1/1)
17/12/20 18:18:18 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
17/12/20 18:18:18 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[58] at sql at <unknown>:0), which has no missing parents
17/12/20 18:18:18 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 15.3 KB, free 2003.4 MB)
17/12/20 18:18:18 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2003.4 MB)
17/12/20 18:18:18 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 127.0.0.1:64668 (size: 6.9 KB, free: 2004.4 MB)
17/12/20 18:18:18 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:996
17/12/20 18:18:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[58] at sql at <unknown>:0)
17/12/20 18:18:18 INFO TaskSchedulerImpl: Adding task set 15.0 with 2 tasks
17/12/20 18:18:18 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 14, localhost, executor driver, partition 0, ANY, 5943 bytes)
17/12/20 18:18:18 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 15, localhost, executor driver, partition 1, ANY, 5943 bytes)
17/12/20 18:18:18 INFO Executor: Running task 0.0 in stage 15.0 (TID 14)
17/12/20 18:18:18 INFO Executor: Running task 1.0 in stage 15.0 (TID 15)
17/12/20 18:18:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 18:18:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:18:18 INFO MemoryStore: Block rdd_55_0 stored as values in memory (estimated size 1512.0 B, free 2003.4 MB)
17/12/20 18:18:18 INFO BlockManagerInfo: Added rdd_55_0 in memory on 127.0.0.1:64668 (size: 1512.0 B, free: 2004.4 MB)
17/12/20 18:18:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 18:18:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:18:18 INFO MemoryStore: Block rdd_55_1 stored as values in memory (estimated size 1512.0 B, free 2003.4 MB)
17/12/20 18:18:18 INFO BlockManagerInfo: Added rdd_55_1 in memory on 127.0.0.1:64668 (size: 1512.0 B, free: 2004.4 MB)
17/12/20 18:18:18 INFO Executor: Finished task 0.0 in stage 15.0 (TID 14). 3072 bytes result sent to driver
17/12/20 18:18:18 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 14) in 21 ms on localhost (executor driver) (1/2)
17/12/20 18:18:18 INFO Executor: Finished task 1.0 in stage 15.0 (TID 15). 3064 bytes result sent to driver
17/12/20 18:18:18 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 15) in 23 ms on localhost (executor driver) (2/2)
17/12/20 18:18:18 INFO DAGScheduler: ShuffleMapStage 15 (sql at <unknown>:0) finished in 0.024 s
17/12/20 18:18:18 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:18:18 INFO DAGScheduler: running: Set()
17/12/20 18:18:18 INFO DAGScheduler: waiting: Set(ResultStage 16)
17/12/20 18:18:18 INFO DAGScheduler: failed: Set()
17/12/20 18:18:18 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
17/12/20 18:18:18 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[61] at sql at <unknown>:0), which has no missing parents
17/12/20 18:18:18 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 7.0 KB, free 2003.4 MB)
17/12/20 18:18:18 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.4 MB)
17/12/20 18:18:18 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 127.0.0.1:64668 (size: 3.7 KB, free: 2004.4 MB)
17/12/20 18:18:18 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:996
17/12/20 18:18:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[61] at sql at <unknown>:0)
17/12/20 18:18:18 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
17/12/20 18:18:18 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, ANY, 5954 bytes)
17/12/20 18:18:18 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
17/12/20 18:18:18 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 18:18:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/12/20 18:18:18 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 1873 bytes result sent to driver
17/12/20 18:18:18 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 6 ms on localhost (executor driver) (1/1)
17/12/20 18:18:18 INFO DAGScheduler: ResultStage 16 (sql at <unknown>:0) finished in 0.006 s
17/12/20 18:18:18 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
17/12/20 18:18:18 INFO DAGScheduler: Job 7 finished: sql at <unknown>:0, took 0.086857 s
17/12/20 18:18:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:18 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 18:18:18 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 18:18:18 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 145 bytes
17/12/20 18:18:18 INFO DAGScheduler: Registering RDD 65 (collect at utils.scala:196)
17/12/20 18:18:18 INFO DAGScheduler: Got job 8 (collect at utils.scala:196) with 1 output partitions
17/12/20 18:18:18 INFO DAGScheduler: Final stage: ResultStage 19 (collect at utils.scala:196)
17/12/20 18:18:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
17/12/20 18:18:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 18)
17/12/20 18:18:18 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[65] at collect at utils.scala:196), which has no missing parents
17/12/20 18:18:18 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 15.3 KB, free 2003.4 MB)
17/12/20 18:18:18 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2003.4 MB)
17/12/20 18:18:18 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 127.0.0.1:64668 (size: 6.9 KB, free: 2004.4 MB)
17/12/20 18:18:18 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:996
17/12/20 18:18:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[65] at collect at utils.scala:196)
17/12/20 18:18:18 INFO TaskSchedulerImpl: Adding task set 18.0 with 2 tasks
17/12/20 18:18:18 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 5936 bytes)
17/12/20 18:18:18 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 18, localhost, executor driver, partition 1, PROCESS_LOCAL, 5936 bytes)
17/12/20 18:18:18 INFO Executor: Running task 0.0 in stage 18.0 (TID 17)
17/12/20 18:18:18 INFO Executor: Running task 1.0 in stage 18.0 (TID 18)
17/12/20 18:18:18 INFO BlockManager: Found block rdd_55_1 locally
17/12/20 18:18:18 INFO BlockManager: Found block rdd_55_0 locally
17/12/20 18:18:18 INFO Executor: Finished task 1.0 in stage 18.0 (TID 18). 1958 bytes result sent to driver
17/12/20 18:18:18 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 18) in 14 ms on localhost (executor driver) (1/2)
17/12/20 18:18:18 INFO Executor: Finished task 0.0 in stage 18.0 (TID 17). 2037 bytes result sent to driver
17/12/20 18:18:18 INFO DAGScheduler: ShuffleMapStage 18 (collect at utils.scala:196) finished in 0.023 s
17/12/20 18:18:18 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:18:18 INFO DAGScheduler: running: Set()
17/12/20 18:18:18 INFO DAGScheduler: waiting: Set(ResultStage 19)
17/12/20 18:18:18 INFO DAGScheduler: failed: Set()
17/12/20 18:18:18 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[68] at collect at utils.scala:196), which has no missing parents
17/12/20 18:18:18 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 7.0 KB, free 2003.4 MB)
17/12/20 18:18:18 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 17) in 23 ms on localhost (executor driver) (2/2)
17/12/20 18:18:18 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
17/12/20 18:18:18 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.4 MB)
17/12/20 18:18:18 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 127.0.0.1:64668 (size: 3.7 KB, free: 2004.4 MB)
17/12/20 18:18:18 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:996
17/12/20 18:18:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[68] at collect at utils.scala:196)
17/12/20 18:18:18 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
17/12/20 18:18:18 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, ANY, 5947 bytes)
17/12/20 18:18:18 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
17/12/20 18:18:18 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 18:18:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:18:18 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 1794 bytes result sent to driver
17/12/20 18:18:18 INFO DAGScheduler: ResultStage 19 (collect at utils.scala:196) finished in 0.002 s
17/12/20 18:18:18 INFO DAGScheduler: Job 8 finished: collect at utils.scala:196, took 0.036841 s
17/12/20 18:18:18 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 2 ms on localhost (executor driver) (1/1)
17/12/20 18:18:18 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
17/12/20 18:18:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:18 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz6`
WHERE (0 = 1)
17/12/20 18:18:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:18 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S3` + 0.91651514 * RANDN() AS `V1`, `S10` + 0.9486833 * RANDN() AS `V2`, `S10` + 0.9486833 * RANDN() AS `V3`, `S10` + 0.9486833 * RANDN() AS `V4`, `S5` + 0.9 * RANDN() AS `V5`, `S2` + 0.9486833 * RANDN() AS `V6`, `S8` + 0.9 * RANDN() AS `V7`, `S7` + 0.91104336 * RANDN() AS `V8`, `S4` + 0.93808315 * RANDN() AS `V9`, `S8` + 0.9 * RANDN() AS `V10`
FROM `analyis_tbl`) `olkmvoflyy`
17/12/20 18:18:18 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:18:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:18 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz7`
WHERE (0 = 1)
17/12/20 18:18:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:18 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 18:18:18 INFO CodeGenerator: Code generated in 13.942175 ms
17/12/20 18:18:19 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 18:18:19 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 145 bytes
17/12/20 18:18:19 INFO DAGScheduler: Got job 9 (take at <unknown>:0) with 1 output partitions
17/12/20 18:18:19 INFO DAGScheduler: Final stage: ResultStage 21 (take at <unknown>:0)
17/12/20 18:18:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
17/12/20 18:18:19 INFO DAGScheduler: Missing parents: List()
17/12/20 18:18:19 INFO DAGScheduler: Submitting ResultStage 21 (WorkerRDD[74] at RDD at rdd.scala:18), which has no missing parents
17/12/20 18:18:19 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 119.5 KB, free 2003.3 MB)
17/12/20 18:18:19 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 45.7 KB, free 2003.2 MB)
17/12/20 18:18:19 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 127.0.0.1:64668 (size: 45.7 KB, free: 2004.3 MB)
17/12/20 18:18:19 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:996
17/12/20 18:18:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (WorkerRDD[74] at RDD at rdd.scala:18)
17/12/20 18:18:19 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
17/12/20 18:18:19 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 5914 bytes)
17/12/20 18:18:19 INFO Executor: Running task 0.0 in stage 21.0 (TID 20)
17/12/20 18:18:19 INFO BlockManager: Found block rdd_55_0 locally
17/12/20 18:18:19 INFO MemoryStore: Block rdd_74_0 stored as values in memory (estimated size 1600.0 B, free 2003.2 MB)
17/12/20 18:18:19 INFO BlockManagerInfo: Added rdd_74_0 in memory on 127.0.0.1:64668 (size: 1600.0 B, free: 2004.3 MB)
17/12/20 18:18:19 INFO Executor: Finished task 0.0 in stage 21.0 (TID 20). 3259 bytes result sent to driver
17/12/20 18:18:19 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 20) in 726 ms on localhost (executor driver) (1/1)
17/12/20 18:18:19 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
17/12/20 18:18:19 INFO DAGScheduler: ResultStage 21 (take at <unknown>:0) finished in 0.726 s
17/12/20 18:18:19 INFO DAGScheduler: Job 9 finished: take at <unknown>:0, took 0.733818 s
17/12/20 18:18:19 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 18:18:19 INFO DAGScheduler: Got job 10 (take at <unknown>:0) with 1 output partitions
17/12/20 18:18:19 INFO DAGScheduler: Final stage: ResultStage 23 (take at <unknown>:0)
17/12/20 18:18:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
17/12/20 18:18:19 INFO DAGScheduler: Missing parents: List()
17/12/20 18:18:19 INFO DAGScheduler: Submitting ResultStage 23 (WorkerRDD[74] at RDD at rdd.scala:18), which has no missing parents
17/12/20 18:18:19 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 119.5 KB, free 2003.1 MB)
17/12/20 18:18:19 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 45.7 KB, free 2003.0 MB)
17/12/20 18:18:19 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 127.0.0.1:64668 (size: 45.7 KB, free: 2004.3 MB)
17/12/20 18:18:19 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:996
17/12/20 18:18:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (WorkerRDD[74] at RDD at rdd.scala:18)
17/12/20 18:18:19 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
17/12/20 18:18:19 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 21, localhost, executor driver, partition 1, PROCESS_LOCAL, 5914 bytes)
17/12/20 18:18:19 INFO Executor: Running task 0.0 in stage 23.0 (TID 21)
17/12/20 18:18:19 INFO BlockManager: Found block rdd_55_1 locally
17/12/20 18:18:20 INFO MemoryStore: Block rdd_74_1 stored as values in memory (estimated size 1600.0 B, free 2003.0 MB)
17/12/20 18:18:20 INFO BlockManagerInfo: Added rdd_74_1 in memory on 127.0.0.1:64668 (size: 1600.0 B, free: 2004.3 MB)
17/12/20 18:18:20 WARN Executor: 1 block locks were not released by TID = 21:
[rdd_74_1]
17/12/20 18:18:20 INFO Executor: Finished task 0.0 in stage 23.0 (TID 21). 3090 bytes result sent to driver
17/12/20 18:18:20 INFO DAGScheduler: ResultStage 23 (take at <unknown>:0) finished in 0.731 s
17/12/20 18:18:20 INFO DAGScheduler: Job 10 finished: take at <unknown>:0, took 0.736826 s
17/12/20 18:18:20 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 21) in 730 ms on localhost (executor driver) (1/1)
17/12/20 18:18:20 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
17/12/20 18:18:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:20 INFO SparkSqlParser: Parsing command: sparklyr_tmp_1c0c55bc5559
17/12/20 18:18:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:20 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0c55bc5559` AS `zzz8`
WHERE (0 = 1)
17/12/20 18:18:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:20 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0c55bc5559`
17/12/20 18:18:20 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:18:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:20 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz9`
WHERE (0 = 1)
17/12/20 18:18:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:20 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.0023) AS `V1`, (`V2` < 0.0138) AS `V2`, (`V3` < 0.0185) AS `V3`, (`V4` < 0.009) AS `V4`, (`V5` < 0.0375) AS `V5`, (`V6` < 0.054) AS `V6`, (`V7` < 0.054) AS `V7`, (`V8` < 0.0185) AS `V8`, (`V9` < 0.026) AS `V9`, (`V10` < 0.026) AS `V10`
FROM `analyis_tbl`
17/12/20 18:18:20 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:18:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:20 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz10`
WHERE (0 = 1)
17/12/20 18:18:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:20 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 606
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 607
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 608
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 609
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 610
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 611
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 612
17/12/20 18:18:20 INFO ContextCleaner: Cleaned shuffle 4
17/12/20 18:18:20 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 18:18:20 INFO DAGScheduler: Got job 11 (collect at utils.scala:196) with 2 output partitions
17/12/20 18:18:20 INFO DAGScheduler: Final stage: ResultStage 25 (collect at utils.scala:196)
17/12/20 18:18:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)
17/12/20 18:18:20 INFO DAGScheduler: Missing parents: List()
17/12/20 18:18:20 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[81] at collect at utils.scala:196), which has no missing parents
17/12/20 18:18:20 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 132.3 KB, free 2002.9 MB)
17/12/20 18:18:20 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 50.4 KB, free 2002.9 MB)
17/12/20 18:18:20 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 127.0.0.1:64668 (size: 50.4 KB, free: 2004.2 MB)
17/12/20 18:18:20 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:996
17/12/20 18:18:20 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 25 (MapPartitionsRDD[81] at collect at utils.scala:196)
17/12/20 18:18:20 INFO TaskSchedulerImpl: Adding task set 25.0 with 2 tasks
17/12/20 18:18:20 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 5947 bytes)
17/12/20 18:18:20 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 127.0.0.1:64668 in memory (size: 7.5 KB, free: 2004.2 MB)
17/12/20 18:18:20 INFO TaskSetManager: Starting task 1.0 in stage 25.0 (TID 23, localhost, executor driver, partition 1, PROCESS_LOCAL, 5947 bytes)
17/12/20 18:18:20 INFO Executor: Running task 0.0 in stage 25.0 (TID 22)
17/12/20 18:18:20 INFO BlockManager: Found block rdd_74_0 locally
17/12/20 18:18:20 INFO Executor: Running task 1.0 in stage 25.0 (TID 23)
17/12/20 18:18:20 INFO Executor: Finished task 0.0 in stage 25.0 (TID 22). 1618 bytes result sent to driver
17/12/20 18:18:20 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 127.0.0.1:64668 in memory (size: 6.9 KB, free: 2004.2 MB)
17/12/20 18:18:20 INFO BlockManager: Found block rdd_74_1 locally
17/12/20 18:18:20 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 22) in 28 ms on localhost (executor driver) (1/2)
17/12/20 18:18:20 INFO Executor: Finished task 1.0 in stage 25.0 (TID 23). 1617 bytes result sent to driver
17/12/20 18:18:20 INFO DAGScheduler: ResultStage 25 (collect at utils.scala:196) finished in 0.035 s
17/12/20 18:18:20 INFO DAGScheduler: Job 11 finished: collect at utils.scala:196, took 0.043280 s
17/12/20 18:18:20 INFO TaskSetManager: Finished task 1.0 in stage 25.0 (TID 23) in 29 ms on localhost (executor driver) (2/2)
17/12/20 18:18:20 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
17/12/20 18:18:20 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 127.0.0.1:64668 in memory (size: 3.7 KB, free: 2004.2 MB)
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 781
17/12/20 18:18:20 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 127.0.0.1:64668 in memory (size: 6.9 KB, free: 2004.3 MB)
17/12/20 18:18:20 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 127.0.0.1:64668 in memory (size: 3.7 KB, free: 2004.3 MB)
17/12/20 18:18:20 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 127.0.0.1:64668 in memory (size: 45.7 KB, free: 2004.3 MB)
17/12/20 18:18:20 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 127.0.0.1:64668 in memory (size: 45.7 KB, free: 2004.3 MB)
17/12/20 18:18:20 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 127.0.0.1:64668 in memory (size: 45.6 KB, free: 2004.4 MB)
17/12/20 18:18:20 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 127.0.0.1:64668 in memory (size: 45.6 KB, free: 2004.4 MB)
17/12/20 18:18:20 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 127.0.0.1:64668 in memory (size: 50.4 KB, free: 2004.5 MB)
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 543
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 544
17/12/20 18:18:20 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 127.0.0.1:64668 in memory (size: 4.6 KB, free: 2004.5 MB)
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 593
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 594
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 600
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 601
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 602
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 603
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 604
17/12/20 18:18:20 INFO ContextCleaner: Cleaned accumulator 605
17/12/20 18:18:21 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:21 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:18:21 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:18:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:18:21 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:18:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:18:21 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:18:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:18:21 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:21 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:18:21 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:18:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:18:21 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:18:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:18:21 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:18:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:18:21 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:21 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:18:21 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:18:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:18:21 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:18:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:18:21 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:18:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:18:21 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:18:21 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:18:21 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:18:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:18:21 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:18:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:18:21 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:18:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:25:13 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:13 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:25:13 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:25:13 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:25:13 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:25:13 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:25:13 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:25:13 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:25:13 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 18:25:13 INFO DAGScheduler: Got job 12 (collect at utils.scala:58) with 1 output partitions
17/12/20 18:25:13 INFO DAGScheduler: Final stage: ResultStage 26 (collect at utils.scala:58)
17/12/20 18:25:13 INFO DAGScheduler: Parents of final stage: List()
17/12/20 18:25:13 INFO DAGScheduler: Missing parents: List()
17/12/20 18:25:13 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[91] at map at utils.scala:55), which has no missing parents
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 8.7 KB, free 2003.8 MB)
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2003.8 MB)
17/12/20 18:25:13 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 127.0.0.1:64668 (size: 4.6 KB, free: 2004.5 MB)
17/12/20 18:25:13 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:996
17/12/20 18:25:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[91] at map at utils.scala:55)
17/12/20 18:25:13 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
17/12/20 18:25:13 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 6505 bytes)
17/12/20 18:25:13 INFO Executor: Running task 0.0 in stage 26.0 (TID 24)
17/12/20 18:25:13 INFO Executor: Finished task 0.0 in stage 26.0 (TID 24). 966 bytes result sent to driver
17/12/20 18:25:13 INFO DAGScheduler: ResultStage 26 (collect at utils.scala:58) finished in 0.002 s
17/12/20 18:25:13 INFO DAGScheduler: Job 12 finished: collect at utils.scala:58, took 0.011482 s
17/12/20 18:25:13 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 24) in 2 ms on localhost (executor driver) (1/1)
17/12/20 18:25:13 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
17/12/20 18:25:13 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:13 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:13 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:25:13 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:13 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 18:25:13 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 18:25:13 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 18:25:13 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 18:25:13 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 18:25:13 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 293.7 KB, free 2003.5 MB)
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2003.5 MB)
17/12/20 18:25:13 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 127.0.0.1:64668 (size: 24.0 KB, free: 2004.5 MB)
17/12/20 18:25:13 INFO SparkContext: Created broadcast 21 from sql at <unknown>:0
17/12/20 18:25:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 18:25:13 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 18:25:13 INFO DAGScheduler: Registering RDD 95 (sql at <unknown>:0)
17/12/20 18:25:13 INFO DAGScheduler: Registering RDD 100 (sql at <unknown>:0)
17/12/20 18:25:13 INFO DAGScheduler: Got job 13 (sql at <unknown>:0) with 1 output partitions
17/12/20 18:25:13 INFO DAGScheduler: Final stage: ResultStage 29 (sql at <unknown>:0)
17/12/20 18:25:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
17/12/20 18:25:13 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 28)
17/12/20 18:25:13 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[95] at sql at <unknown>:0), which has no missing parents
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 13.1 KB, free 2003.5 MB)
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2003.4 MB)
17/12/20 18:25:13 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 127.0.0.1:64668 (size: 7.5 KB, free: 2004.5 MB)
17/12/20 18:25:13 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:996
17/12/20 18:25:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[95] at sql at <unknown>:0)
17/12/20 18:25:13 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
17/12/20 18:25:13 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 6678 bytes)
17/12/20 18:25:13 INFO Executor: Running task 0.0 in stage 27.0 (TID 25)
17/12/20 18:25:13 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/RtmpusRKRl/spark_serialize_23464416f26121f9db12d9336ac0a5fb9aef785c9b6ee8b752338180a3721979.csv, range: 0-1926, partition values: [empty row]
17/12/20 18:25:13 INFO Executor: Finished task 0.0 in stage 27.0 (TID 25). 1719 bytes result sent to driver
17/12/20 18:25:13 INFO DAGScheduler: ShuffleMapStage 27 (sql at <unknown>:0) finished in 0.035 s
17/12/20 18:25:13 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:25:13 INFO DAGScheduler: running: Set()
17/12/20 18:25:13 INFO DAGScheduler: waiting: Set(ShuffleMapStage 28, ResultStage 29)
17/12/20 18:25:13 INFO DAGScheduler: failed: Set()
17/12/20 18:25:13 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 25) in 35 ms on localhost (executor driver) (1/1)
17/12/20 18:25:13 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
17/12/20 18:25:13 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[100] at sql at <unknown>:0), which has no missing parents
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 15.3 KB, free 2003.4 MB)
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2003.4 MB)
17/12/20 18:25:13 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 127.0.0.1:64668 (size: 6.9 KB, free: 2004.5 MB)
17/12/20 18:25:13 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:996
17/12/20 18:25:13 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[100] at sql at <unknown>:0)
17/12/20 18:25:13 INFO TaskSchedulerImpl: Adding task set 28.0 with 2 tasks
17/12/20 18:25:13 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 26, localhost, executor driver, partition 0, ANY, 5944 bytes)
17/12/20 18:25:13 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 27, localhost, executor driver, partition 1, ANY, 5944 bytes)
17/12/20 18:25:13 INFO Executor: Running task 0.0 in stage 28.0 (TID 26)
17/12/20 18:25:13 INFO Executor: Running task 1.0 in stage 28.0 (TID 27)
17/12/20 18:25:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 18:25:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 18:25:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:25:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:25:13 INFO MemoryStore: Block rdd_97_0 stored as values in memory (estimated size 1512.0 B, free 2003.4 MB)
17/12/20 18:25:13 INFO MemoryStore: Block rdd_97_1 stored as values in memory (estimated size 1512.0 B, free 2003.4 MB)
17/12/20 18:25:13 INFO BlockManagerInfo: Added rdd_97_0 in memory on 127.0.0.1:64668 (size: 1512.0 B, free: 2004.4 MB)
17/12/20 18:25:13 INFO BlockManagerInfo: Added rdd_97_1 in memory on 127.0.0.1:64668 (size: 1512.0 B, free: 2004.4 MB)
17/12/20 18:25:13 INFO Executor: Finished task 1.0 in stage 28.0 (TID 27). 3072 bytes result sent to driver
17/12/20 18:25:13 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 27) in 18 ms on localhost (executor driver) (1/2)
17/12/20 18:25:13 INFO Executor: Finished task 0.0 in stage 28.0 (TID 26). 3072 bytes result sent to driver
17/12/20 18:25:13 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 26) in 21 ms on localhost (executor driver) (2/2)
17/12/20 18:25:13 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
17/12/20 18:25:13 INFO DAGScheduler: ShuffleMapStage 28 (sql at <unknown>:0) finished in 0.021 s
17/12/20 18:25:13 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:25:13 INFO DAGScheduler: running: Set()
17/12/20 18:25:13 INFO DAGScheduler: waiting: Set(ResultStage 29)
17/12/20 18:25:13 INFO DAGScheduler: failed: Set()
17/12/20 18:25:13 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[103] at sql at <unknown>:0), which has no missing parents
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 7.0 KB, free 2003.4 MB)
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.4 MB)
17/12/20 18:25:13 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 127.0.0.1:64668 (size: 3.7 KB, free: 2004.4 MB)
17/12/20 18:25:13 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:996
17/12/20 18:25:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[103] at sql at <unknown>:0)
17/12/20 18:25:13 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
17/12/20 18:25:13 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 28, localhost, executor driver, partition 0, ANY, 5955 bytes)
17/12/20 18:25:13 INFO Executor: Running task 0.0 in stage 29.0 (TID 28)
17/12/20 18:25:13 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 18:25:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:25:13 INFO Executor: Finished task 0.0 in stage 29.0 (TID 28). 1873 bytes result sent to driver
17/12/20 18:25:13 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 28) in 4 ms on localhost (executor driver) (1/1)
17/12/20 18:25:13 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
17/12/20 18:25:13 INFO DAGScheduler: ResultStage 29 (sql at <unknown>:0) finished in 0.004 s
17/12/20 18:25:13 INFO DAGScheduler: Job 13 finished: sql at <unknown>:0, took 0.075916 s
17/12/20 18:25:13 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:13 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 18:25:13 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 18:25:13 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 145 bytes
17/12/20 18:25:13 INFO DAGScheduler: Registering RDD 107 (collect at utils.scala:196)
17/12/20 18:25:13 INFO DAGScheduler: Got job 14 (collect at utils.scala:196) with 1 output partitions
17/12/20 18:25:13 INFO DAGScheduler: Final stage: ResultStage 32 (collect at utils.scala:196)
17/12/20 18:25:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
17/12/20 18:25:13 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 31)
17/12/20 18:25:13 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[107] at collect at utils.scala:196), which has no missing parents
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.3 KB, free 2003.4 MB)
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2003.4 MB)
17/12/20 18:25:13 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 127.0.0.1:64668 (size: 6.9 KB, free: 2004.4 MB)
17/12/20 18:25:13 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:996
17/12/20 18:25:13 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[107] at collect at utils.scala:196)
17/12/20 18:25:13 INFO TaskSchedulerImpl: Adding task set 31.0 with 2 tasks
17/12/20 18:25:13 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 29, localhost, executor driver, partition 0, PROCESS_LOCAL, 5936 bytes)
17/12/20 18:25:13 INFO TaskSetManager: Starting task 1.0 in stage 31.0 (TID 30, localhost, executor driver, partition 1, PROCESS_LOCAL, 5936 bytes)
17/12/20 18:25:13 INFO Executor: Running task 0.0 in stage 31.0 (TID 29)
17/12/20 18:25:13 INFO BlockManager: Found block rdd_97_0 locally
17/12/20 18:25:13 INFO Executor: Running task 1.0 in stage 31.0 (TID 30)
17/12/20 18:25:13 INFO BlockManager: Found block rdd_97_1 locally
17/12/20 18:25:13 INFO Executor: Finished task 0.0 in stage 31.0 (TID 29). 1950 bytes result sent to driver
17/12/20 18:25:13 INFO Executor: Finished task 1.0 in stage 31.0 (TID 30). 1871 bytes result sent to driver
17/12/20 18:25:13 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 29) in 19 ms on localhost (executor driver) (1/2)
17/12/20 18:25:13 INFO DAGScheduler: ShuffleMapStage 31 (collect at utils.scala:196) finished in 0.020 s
17/12/20 18:25:13 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:25:13 INFO DAGScheduler: running: Set()
17/12/20 18:25:13 INFO DAGScheduler: waiting: Set(ResultStage 32)
17/12/20 18:25:13 INFO DAGScheduler: failed: Set()
17/12/20 18:25:13 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[110] at collect at utils.scala:196), which has no missing parents
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 7.0 KB, free 2003.4 MB)
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.4 MB)
17/12/20 18:25:13 INFO TaskSetManager: Finished task 1.0 in stage 31.0 (TID 30) in 17 ms on localhost (executor driver) (2/2)
17/12/20 18:25:13 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
17/12/20 18:25:13 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 127.0.0.1:64668 (size: 3.7 KB, free: 2004.4 MB)
17/12/20 18:25:13 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:996
17/12/20 18:25:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[110] at collect at utils.scala:196)
17/12/20 18:25:13 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
17/12/20 18:25:13 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 31, localhost, executor driver, partition 0, ANY, 5947 bytes)
17/12/20 18:25:13 INFO Executor: Running task 0.0 in stage 32.0 (TID 31)
17/12/20 18:25:13 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 18:25:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:25:13 INFO Executor: Finished task 0.0 in stage 32.0 (TID 31). 1873 bytes result sent to driver
17/12/20 18:25:13 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 31) in 5 ms on localhost (executor driver) (1/1)
17/12/20 18:25:13 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
17/12/20 18:25:13 INFO DAGScheduler: ResultStage 32 (collect at utils.scala:196) finished in 0.006 s
17/12/20 18:25:13 INFO DAGScheduler: Job 14 finished: collect at utils.scala:196, took 0.033948 s
17/12/20 18:25:13 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:13 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz11`
WHERE (0 = 1)
17/12/20 18:25:13 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:13 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S3` + 0.91651514 * RANDN() AS `V1`, `S10` + 0.9486833 * RANDN() AS `V2`, `S10` + 0.9486833 * RANDN() AS `V3`, `S10` + 0.9486833 * RANDN() AS `V4`, `S5` + 0.9 * RANDN() AS `V5`, `S2` + 0.9486833 * RANDN() AS `V6`, `S8` + 0.9 * RANDN() AS `V7`, `S7` + 0.91104336 * RANDN() AS `V8`, `S4` + 0.93808315 * RANDN() AS `V9`, `S8` + 0.9 * RANDN() AS `V10`
FROM `analyis_tbl`) `gizdrpejvs`
17/12/20 18:25:13 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:25:13 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:13 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz12`
WHERE (0 = 1)
17/12/20 18:25:13 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:13 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 18:25:13 INFO CodeGenerator: Code generated in 15.16667 ms
17/12/20 18:25:13 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 18:25:13 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 145 bytes
17/12/20 18:25:13 INFO DAGScheduler: Got job 15 (take at <unknown>:0) with 1 output partitions
17/12/20 18:25:13 INFO DAGScheduler: Final stage: ResultStage 34 (take at <unknown>:0)
17/12/20 18:25:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)
17/12/20 18:25:13 INFO DAGScheduler: Missing parents: List()
17/12/20 18:25:13 INFO DAGScheduler: Submitting ResultStage 34 (WorkerRDD[116] at RDD at rdd.scala:18), which has no missing parents
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 122.6 KB, free 2003.3 MB)
17/12/20 18:25:13 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 46.6 KB, free 2003.2 MB)
17/12/20 18:25:13 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 127.0.0.1:64668 (size: 46.6 KB, free: 2004.4 MB)
17/12/20 18:25:13 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:996
17/12/20 18:25:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (WorkerRDD[116] at RDD at rdd.scala:18)
17/12/20 18:25:13 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks
17/12/20 18:25:13 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 32, localhost, executor driver, partition 0, PROCESS_LOCAL, 5914 bytes)
17/12/20 18:25:13 INFO Executor: Running task 0.0 in stage 34.0 (TID 32)
17/12/20 18:25:13 INFO BlockManager: Found block rdd_97_0 locally
17/12/20 18:25:14 INFO MemoryStore: Block rdd_116_0 stored as values in memory (estimated size 1600.0 B, free 2003.2 MB)
17/12/20 18:25:14 INFO BlockManagerInfo: Added rdd_116_0 in memory on 127.0.0.1:64668 (size: 1600.0 B, free: 2004.4 MB)
17/12/20 18:25:14 INFO Executor: Finished task 0.0 in stage 34.0 (TID 32). 3090 bytes result sent to driver
17/12/20 18:25:14 INFO DAGScheduler: ResultStage 34 (take at <unknown>:0) finished in 0.664 s
17/12/20 18:25:14 INFO DAGScheduler: Job 15 finished: take at <unknown>:0, took 0.669137 s
17/12/20 18:25:14 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 32) in 664 ms on localhost (executor driver) (1/1)
17/12/20 18:25:14 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
17/12/20 18:25:14 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 18:25:14 INFO DAGScheduler: Got job 16 (take at <unknown>:0) with 1 output partitions
17/12/20 18:25:14 INFO DAGScheduler: Final stage: ResultStage 36 (take at <unknown>:0)
17/12/20 18:25:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)
17/12/20 18:25:14 INFO DAGScheduler: Missing parents: List()
17/12/20 18:25:14 INFO DAGScheduler: Submitting ResultStage 36 (WorkerRDD[116] at RDD at rdd.scala:18), which has no missing parents
17/12/20 18:25:14 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 122.6 KB, free 2003.1 MB)
17/12/20 18:25:14 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 46.6 KB, free 2003.0 MB)
17/12/20 18:25:14 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 127.0.0.1:64668 (size: 46.6 KB, free: 2004.3 MB)
17/12/20 18:25:14 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:996
17/12/20 18:25:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (WorkerRDD[116] at RDD at rdd.scala:18)
17/12/20 18:25:14 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks
17/12/20 18:25:14 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 33, localhost, executor driver, partition 1, PROCESS_LOCAL, 5914 bytes)
17/12/20 18:25:14 INFO Executor: Running task 0.0 in stage 36.0 (TID 33)
17/12/20 18:25:14 INFO BlockManager: Found block rdd_97_1 locally
17/12/20 18:25:15 INFO MemoryStore: Block rdd_116_1 stored as values in memory (estimated size 1600.0 B, free 2003.0 MB)
17/12/20 18:25:15 INFO BlockManagerInfo: Added rdd_116_1 in memory on 127.0.0.1:64668 (size: 1600.0 B, free: 2004.3 MB)
17/12/20 18:25:15 WARN Executor: 1 block locks were not released by TID = 33:
[rdd_116_1]
17/12/20 18:25:15 INFO Executor: Finished task 0.0 in stage 36.0 (TID 33). 3090 bytes result sent to driver
17/12/20 18:25:15 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 33) in 703 ms on localhost (executor driver) (1/1)
17/12/20 18:25:15 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
17/12/20 18:25:15 INFO DAGScheduler: ResultStage 36 (take at <unknown>:0) finished in 0.704 s
17/12/20 18:25:15 INFO DAGScheduler: Job 16 finished: take at <unknown>:0, took 0.710080 s
17/12/20 18:25:15 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:15 INFO SparkSqlParser: Parsing command: sparklyr_tmp_1c0ccbe6ca7
17/12/20 18:25:15 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:15 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0ccbe6ca7` AS `zzz13`
WHERE (0 = 1)
17/12/20 18:25:15 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:15 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0ccbe6ca7`
17/12/20 18:25:15 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:25:15 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:15 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz14`
WHERE (0 = 1)
17/12/20 18:25:15 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:15 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.0023) AS `V1`, (`V2` < 0.0138) AS `V2`, (`V3` < 0.0185) AS `V3`, (`V4` < 0.009) AS `V4`, (`V5` < 0.0375) AS `V5`, (`V6` < 0.054) AS `V6`, (`V7` < 0.054) AS `V7`, (`V8` < 0.0185) AS `V8`, (`V9` < 0.026) AS `V9`, (`V10` < 0.026) AS `V10`
FROM `analyis_tbl`
17/12/20 18:25:15 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:25:15 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:15 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz15`
WHERE (0 = 1)
17/12/20 18:25:15 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:15 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 18:25:15 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 18:25:15 INFO DAGScheduler: Got job 17 (collect at utils.scala:196) with 2 output partitions
17/12/20 18:25:15 INFO DAGScheduler: Final stage: ResultStage 38 (collect at utils.scala:196)
17/12/20 18:25:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
17/12/20 18:25:15 INFO DAGScheduler: Missing parents: List()
17/12/20 18:25:15 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[123] at collect at utils.scala:196), which has no missing parents
17/12/20 18:25:15 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 135.4 KB, free 2002.9 MB)
17/12/20 18:25:15 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 51.2 KB, free 2002.9 MB)
17/12/20 18:25:15 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 127.0.0.1:64668 (size: 51.2 KB, free: 2004.3 MB)
17/12/20 18:25:15 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:996
17/12/20 18:25:15 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 38 (MapPartitionsRDD[123] at collect at utils.scala:196)
17/12/20 18:25:15 INFO TaskSchedulerImpl: Adding task set 38.0 with 2 tasks
17/12/20 18:25:15 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 5947 bytes)
17/12/20 18:25:15 INFO TaskSetManager: Starting task 1.0 in stage 38.0 (TID 35, localhost, executor driver, partition 1, PROCESS_LOCAL, 5947 bytes)
17/12/20 18:25:15 INFO Executor: Running task 1.0 in stage 38.0 (TID 35)
17/12/20 18:25:15 INFO Executor: Running task 0.0 in stage 38.0 (TID 34)
17/12/20 18:25:15 INFO BlockManager: Found block rdd_116_0 locally
17/12/20 18:25:15 INFO BlockManager: Found block rdd_116_1 locally
17/12/20 18:25:15 INFO Executor: Finished task 0.0 in stage 38.0 (TID 34). 1630 bytes result sent to driver
17/12/20 18:25:15 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 34) in 7 ms on localhost (executor driver) (1/2)
17/12/20 18:25:15 INFO Executor: Finished task 1.0 in stage 38.0 (TID 35). 1620 bytes result sent to driver
17/12/20 18:25:15 INFO DAGScheduler: ResultStage 38 (collect at utils.scala:196) finished in 0.013 s
17/12/20 18:25:15 INFO DAGScheduler: Job 17 finished: collect at utils.scala:196, took 0.024501 s
17/12/20 18:25:15 INFO TaskSetManager: Finished task 1.0 in stage 38.0 (TID 35) in 11 ms on localhost (executor driver) (2/2)
17/12/20 18:25:15 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
17/12/20 18:25:15 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:15 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:25:15 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:25:15 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:25:15 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:25:15 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:25:15 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:25:15 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:25:15 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:15 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:25:15 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:25:15 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:25:15 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:25:15 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:25:15 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:25:15 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:25:15 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:15 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:25:15 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:25:15 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:25:15 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:25:15 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:25:15 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:25:15 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:25:15 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:25:15 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:25:15 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:25:15 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:25:15 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:25:15 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:25:15 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:25:15 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:27:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:29 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:27:29 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:27:29 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:27:29 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:27:29 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:27:29 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:27:29 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:27:29 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 18:27:29 INFO DAGScheduler: Got job 18 (collect at utils.scala:58) with 1 output partitions
17/12/20 18:27:29 INFO DAGScheduler: Final stage: ResultStage 39 (collect at utils.scala:58)
17/12/20 18:27:29 INFO DAGScheduler: Parents of final stage: List()
17/12/20 18:27:29 INFO DAGScheduler: Missing parents: List()
17/12/20 18:27:29 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[133] at map at utils.scala:55), which has no missing parents
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 8.7 KB, free 2002.9 MB)
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2002.8 MB)
17/12/20 18:27:29 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 127.0.0.1:64668 (size: 4.6 KB, free: 2004.3 MB)
17/12/20 18:27:29 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[133] at map at utils.scala:55)
17/12/20 18:27:29 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks
17/12/20 18:27:29 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 6576 bytes)
17/12/20 18:27:29 INFO Executor: Running task 0.0 in stage 39.0 (TID 36)
17/12/20 18:27:29 INFO Executor: Finished task 0.0 in stage 39.0 (TID 36). 1080 bytes result sent to driver
17/12/20 18:27:29 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 36) in 4 ms on localhost (executor driver) (1/1)
17/12/20 18:27:29 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
17/12/20 18:27:29 INFO DAGScheduler: ResultStage 39 (collect at utils.scala:58) finished in 0.004 s
17/12/20 18:27:29 INFO DAGScheduler: Job 18 finished: collect at utils.scala:58, took 0.009062 s
17/12/20 18:27:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:29 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:27:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:29 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 18:27:29 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 18:27:29 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 18:27:29 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 18:27:29 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 18:27:29 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 293.7 KB, free 2002.6 MB)
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2002.5 MB)
17/12/20 18:27:29 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 127.0.0.1:64668 (size: 24.0 KB, free: 2004.3 MB)
17/12/20 18:27:29 INFO SparkContext: Created broadcast 31 from sql at <unknown>:0
17/12/20 18:27:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 18:27:29 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 127.0.0.1:64668 in memory (size: 50.4 KB, free: 2004.3 MB)
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1149
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1150
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1151
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1152
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1153
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1154
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1155
17/12/20 18:27:29 INFO ContextCleaner: Cleaned shuffle 7
17/12/20 18:27:29 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 127.0.0.1:64668 in memory (size: 7.5 KB, free: 2004.3 MB)
17/12/20 18:27:29 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 127.0.0.1:64668 in memory (size: 6.9 KB, free: 2004.3 MB)
17/12/20 18:27:29 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 127.0.0.1:64668 in memory (size: 3.7 KB, free: 2004.3 MB)
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1324
17/12/20 18:27:29 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 127.0.0.1:64668 in memory (size: 6.9 KB, free: 2004.3 MB)
17/12/20 18:27:29 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 127.0.0.1:64668 in memory (size: 3.7 KB, free: 2004.3 MB)
17/12/20 18:27:29 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 127.0.0.1:64668 in memory (size: 46.6 KB, free: 2004.4 MB)
17/12/20 18:27:29 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 127.0.0.1:64668 in memory (size: 46.6 KB, free: 2004.4 MB)
17/12/20 18:27:29 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 18:27:29 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 127.0.0.1:64668 in memory (size: 51.2 KB, free: 2004.5 MB)
17/12/20 18:27:29 INFO DAGScheduler: Registering RDD 137 (sql at <unknown>:0)
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1629
17/12/20 18:27:29 INFO DAGScheduler: Registering RDD 142 (sql at <unknown>:0)
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1630
17/12/20 18:27:29 INFO DAGScheduler: Got job 19 (sql at <unknown>:0) with 1 output partitions
17/12/20 18:27:29 INFO DAGScheduler: Final stage: ResultStage 42 (sql at <unknown>:0)
17/12/20 18:27:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)
17/12/20 18:27:29 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 41)
17/12/20 18:27:29 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 127.0.0.1:64668 in memory (size: 4.6 KB, free: 2004.5 MB)
17/12/20 18:27:29 INFO DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[137] at sql at <unknown>:0), which has no missing parents
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1679
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1680
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1686
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1086
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1087
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 13.1 KB, free 2003.3 MB)
17/12/20 18:27:29 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 127.0.0.1:64668 in memory (size: 4.6 KB, free: 2004.5 MB)
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1136
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1137
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1143
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1144
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1145
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1146
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1147
17/12/20 18:27:29 INFO ContextCleaner: Cleaned accumulator 1148
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2003.3 MB)
17/12/20 18:27:29 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 127.0.0.1:64668 (size: 7.5 KB, free: 2004.5 MB)
17/12/20 18:27:29 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[137] at sql at <unknown>:0)
17/12/20 18:27:29 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks
17/12/20 18:27:29 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 37, localhost, executor driver, partition 0, PROCESS_LOCAL, 6678 bytes)
17/12/20 18:27:29 INFO Executor: Running task 0.0 in stage 40.0 (TID 37)
17/12/20 18:27:29 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/RtmpusRKRl/spark_serialize_c5ffe24879de9abd88915152ea497421db89631c7b9d3dde2411520b4b3b60ad.csv, range: 0-1910, partition values: [empty row]
17/12/20 18:27:29 INFO Executor: Finished task 0.0 in stage 40.0 (TID 37). 1806 bytes result sent to driver
17/12/20 18:27:29 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 37) in 33 ms on localhost (executor driver) (1/1)
17/12/20 18:27:29 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
17/12/20 18:27:29 INFO DAGScheduler: ShuffleMapStage 40 (sql at <unknown>:0) finished in 0.034 s
17/12/20 18:27:29 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:27:29 INFO DAGScheduler: running: Set()
17/12/20 18:27:29 INFO DAGScheduler: waiting: Set(ResultStage 42, ShuffleMapStage 41)
17/12/20 18:27:29 INFO DAGScheduler: failed: Set()
17/12/20 18:27:29 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[142] at sql at <unknown>:0), which has no missing parents
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 15.3 KB, free 2003.3 MB)
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2003.3 MB)
17/12/20 18:27:29 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 127.0.0.1:64668 (size: 6.9 KB, free: 2004.5 MB)
17/12/20 18:27:29 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:29 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[142] at sql at <unknown>:0)
17/12/20 18:27:29 INFO TaskSchedulerImpl: Adding task set 41.0 with 2 tasks
17/12/20 18:27:29 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 38, localhost, executor driver, partition 0, ANY, 5944 bytes)
17/12/20 18:27:29 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 39, localhost, executor driver, partition 1, ANY, 5944 bytes)
17/12/20 18:27:29 INFO Executor: Running task 0.0 in stage 41.0 (TID 38)
17/12/20 18:27:29 INFO Executor: Running task 1.0 in stage 41.0 (TID 39)
17/12/20 18:27:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 18:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:27:29 INFO MemoryStore: Block rdd_139_0 stored as values in memory (estimated size 1512.0 B, free 2003.3 MB)
17/12/20 18:27:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 18:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:27:29 INFO BlockManagerInfo: Added rdd_139_0 in memory on 127.0.0.1:64668 (size: 1512.0 B, free: 2004.5 MB)
17/12/20 18:27:29 INFO MemoryStore: Block rdd_139_1 stored as values in memory (estimated size 1512.0 B, free 2003.3 MB)
17/12/20 18:27:29 INFO BlockManagerInfo: Added rdd_139_1 in memory on 127.0.0.1:64668 (size: 1512.0 B, free: 2004.5 MB)
17/12/20 18:27:29 INFO Executor: Finished task 0.0 in stage 41.0 (TID 38). 3072 bytes result sent to driver
17/12/20 18:27:29 INFO Executor: Finished task 1.0 in stage 41.0 (TID 39). 3151 bytes result sent to driver
17/12/20 18:27:29 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 38) in 21 ms on localhost (executor driver) (1/2)
17/12/20 18:27:29 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 39) in 20 ms on localhost (executor driver) (2/2)
17/12/20 18:27:29 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
17/12/20 18:27:29 INFO DAGScheduler: ShuffleMapStage 41 (sql at <unknown>:0) finished in 0.025 s
17/12/20 18:27:29 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:27:29 INFO DAGScheduler: running: Set()
17/12/20 18:27:29 INFO DAGScheduler: waiting: Set(ResultStage 42)
17/12/20 18:27:29 INFO DAGScheduler: failed: Set()
17/12/20 18:27:29 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[145] at sql at <unknown>:0), which has no missing parents
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 7.0 KB, free 2003.3 MB)
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.3 MB)
17/12/20 18:27:29 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 127.0.0.1:64668 (size: 3.7 KB, free: 2004.5 MB)
17/12/20 18:27:29 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[145] at sql at <unknown>:0)
17/12/20 18:27:29 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
17/12/20 18:27:29 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 40, localhost, executor driver, partition 0, ANY, 5955 bytes)
17/12/20 18:27:29 INFO Executor: Running task 0.0 in stage 42.0 (TID 40)
17/12/20 18:27:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 18:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:27:29 INFO Executor: Finished task 0.0 in stage 42.0 (TID 40). 1873 bytes result sent to driver
17/12/20 18:27:29 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 40) in 4 ms on localhost (executor driver) (1/1)
17/12/20 18:27:29 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
17/12/20 18:27:29 INFO DAGScheduler: ResultStage 42 (sql at <unknown>:0) finished in 0.004 s
17/12/20 18:27:29 INFO DAGScheduler: Job 19 finished: sql at <unknown>:0, took 0.074611 s
17/12/20 18:27:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:29 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 18:27:29 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 18:27:29 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 145 bytes
17/12/20 18:27:29 INFO DAGScheduler: Registering RDD 149 (collect at utils.scala:196)
17/12/20 18:27:29 INFO DAGScheduler: Got job 20 (collect at utils.scala:196) with 1 output partitions
17/12/20 18:27:29 INFO DAGScheduler: Final stage: ResultStage 45 (collect at utils.scala:196)
17/12/20 18:27:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)
17/12/20 18:27:29 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 44)
17/12/20 18:27:29 INFO DAGScheduler: Submitting ShuffleMapStage 44 (MapPartitionsRDD[149] at collect at utils.scala:196), which has no missing parents
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 15.3 KB, free 2003.3 MB)
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 7.0 KB, free 2003.3 MB)
17/12/20 18:27:29 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 127.0.0.1:64668 (size: 7.0 KB, free: 2004.5 MB)
17/12/20 18:27:29 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:29 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 44 (MapPartitionsRDD[149] at collect at utils.scala:196)
17/12/20 18:27:29 INFO TaskSchedulerImpl: Adding task set 44.0 with 2 tasks
17/12/20 18:27:29 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 41, localhost, executor driver, partition 0, PROCESS_LOCAL, 5936 bytes)
17/12/20 18:27:29 INFO TaskSetManager: Starting task 1.0 in stage 44.0 (TID 42, localhost, executor driver, partition 1, PROCESS_LOCAL, 5936 bytes)
17/12/20 18:27:29 INFO Executor: Running task 0.0 in stage 44.0 (TID 41)
17/12/20 18:27:29 INFO BlockManager: Found block rdd_139_0 locally
17/12/20 18:27:29 INFO Executor: Running task 1.0 in stage 44.0 (TID 42)
17/12/20 18:27:29 INFO BlockManager: Found block rdd_139_1 locally
17/12/20 18:27:29 INFO Executor: Finished task 0.0 in stage 44.0 (TID 41). 1871 bytes result sent to driver
17/12/20 18:27:29 INFO Executor: Finished task 1.0 in stage 44.0 (TID 42). 1871 bytes result sent to driver
17/12/20 18:27:29 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 41) in 20 ms on localhost (executor driver) (1/2)
17/12/20 18:27:29 INFO DAGScheduler: ShuffleMapStage 44 (collect at utils.scala:196) finished in 0.020 s
17/12/20 18:27:29 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:27:29 INFO DAGScheduler: running: Set()
17/12/20 18:27:29 INFO DAGScheduler: waiting: Set(ResultStage 45)
17/12/20 18:27:29 INFO DAGScheduler: failed: Set()
17/12/20 18:27:29 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[152] at collect at utils.scala:196), which has no missing parents
17/12/20 18:27:29 INFO TaskSetManager: Finished task 1.0 in stage 44.0 (TID 42) in 17 ms on localhost (executor driver) (2/2)
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 7.0 KB, free 2003.3 MB)
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.3 MB)
17/12/20 18:27:29 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
17/12/20 18:27:29 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 127.0.0.1:64668 (size: 3.7 KB, free: 2004.5 MB)
17/12/20 18:27:29 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[152] at collect at utils.scala:196)
17/12/20 18:27:29 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks
17/12/20 18:27:29 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 43, localhost, executor driver, partition 0, ANY, 5947 bytes)
17/12/20 18:27:29 INFO Executor: Running task 0.0 in stage 45.0 (TID 43)
17/12/20 18:27:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 18:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:27:29 INFO Executor: Finished task 0.0 in stage 45.0 (TID 43). 1794 bytes result sent to driver
17/12/20 18:27:29 INFO DAGScheduler: ResultStage 45 (collect at utils.scala:196) finished in 0.008 s
17/12/20 18:27:29 INFO DAGScheduler: Job 20 finished: collect at utils.scala:196, took 0.035544 s
17/12/20 18:27:29 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 43) in 8 ms on localhost (executor driver) (1/1)
17/12/20 18:27:29 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
17/12/20 18:27:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:29 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz16`
WHERE (0 = 1)
17/12/20 18:27:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:29 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S3` + 0.91651514 * RANDN() AS `V1`, `S10` + 0.9486833 * RANDN() AS `V2`, `S10` + 0.9486833 * RANDN() AS `V3`, `S10` + 0.9486833 * RANDN() AS `V4`, `S5` + 0.9 * RANDN() AS `V5`, `S2` + 0.9486833 * RANDN() AS `V6`, `S8` + 0.9 * RANDN() AS `V7`, `S7` + 0.91104336 * RANDN() AS `V8`, `S4` + 0.93808315 * RANDN() AS `V9`, `S8` + 0.9 * RANDN() AS `V10`
FROM `analyis_tbl`) `jypkjghena`
17/12/20 18:27:29 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:27:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:29 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz17`
WHERE (0 = 1)
17/12/20 18:27:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:29 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 18:27:29 INFO CodeGenerator: Code generated in 18.110291 ms
17/12/20 18:27:29 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 18:27:29 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 145 bytes
17/12/20 18:27:29 INFO DAGScheduler: Got job 21 (take at <unknown>:0) with 1 output partitions
17/12/20 18:27:29 INFO DAGScheduler: Final stage: ResultStage 47 (take at <unknown>:0)
17/12/20 18:27:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 46)
17/12/20 18:27:29 INFO DAGScheduler: Missing parents: List()
17/12/20 18:27:29 INFO DAGScheduler: Submitting ResultStage 47 (WorkerRDD[158] at RDD at rdd.scala:18), which has no missing parents
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 122.6 KB, free 2003.1 MB)
17/12/20 18:27:29 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 46.6 KB, free 2003.1 MB)
17/12/20 18:27:29 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 127.0.0.1:64668 (size: 46.6 KB, free: 2004.4 MB)
17/12/20 18:27:29 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (WorkerRDD[158] at RDD at rdd.scala:18)
17/12/20 18:27:29 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks
17/12/20 18:27:29 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 44, localhost, executor driver, partition 0, PROCESS_LOCAL, 5914 bytes)
17/12/20 18:27:29 INFO Executor: Running task 0.0 in stage 47.0 (TID 44)
17/12/20 18:27:29 INFO BlockManager: Found block rdd_139_0 locally
17/12/20 18:27:30 INFO MemoryStore: Block rdd_158_0 stored as values in memory (estimated size 1600.0 B, free 2003.1 MB)
17/12/20 18:27:30 INFO BlockManagerInfo: Added rdd_158_0 in memory on 127.0.0.1:64668 (size: 1600.0 B, free: 2004.4 MB)
17/12/20 18:27:30 INFO Executor: Finished task 0.0 in stage 47.0 (TID 44). 3090 bytes result sent to driver
17/12/20 18:27:30 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 44) in 677 ms on localhost (executor driver) (1/1)
17/12/20 18:27:30 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
17/12/20 18:27:30 INFO DAGScheduler: ResultStage 47 (take at <unknown>:0) finished in 0.677 s
17/12/20 18:27:30 INFO DAGScheduler: Job 21 finished: take at <unknown>:0, took 0.682575 s
17/12/20 18:27:30 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 18:27:30 INFO DAGScheduler: Got job 22 (take at <unknown>:0) with 1 output partitions
17/12/20 18:27:30 INFO DAGScheduler: Final stage: ResultStage 49 (take at <unknown>:0)
17/12/20 18:27:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)
17/12/20 18:27:30 INFO DAGScheduler: Missing parents: List()
17/12/20 18:27:30 INFO DAGScheduler: Submitting ResultStage 49 (WorkerRDD[158] at RDD at rdd.scala:18), which has no missing parents
17/12/20 18:27:30 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 122.6 KB, free 2003.0 MB)
17/12/20 18:27:30 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 46.6 KB, free 2002.9 MB)
17/12/20 18:27:30 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 127.0.0.1:64668 (size: 46.6 KB, free: 2004.4 MB)
17/12/20 18:27:30 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (WorkerRDD[158] at RDD at rdd.scala:18)
17/12/20 18:27:30 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks
17/12/20 18:27:30 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 45, localhost, executor driver, partition 1, PROCESS_LOCAL, 5914 bytes)
17/12/20 18:27:30 INFO Executor: Running task 0.0 in stage 49.0 (TID 45)
17/12/20 18:27:30 INFO BlockManager: Found block rdd_139_1 locally
17/12/20 18:27:31 INFO MemoryStore: Block rdd_158_1 stored as values in memory (estimated size 1600.0 B, free 2002.9 MB)
17/12/20 18:27:31 INFO BlockManagerInfo: Added rdd_158_1 in memory on 127.0.0.1:64668 (size: 1600.0 B, free: 2004.4 MB)
17/12/20 18:27:31 WARN Executor: 1 block locks were not released by TID = 45:
[rdd_158_1]
17/12/20 18:27:31 INFO Executor: Finished task 0.0 in stage 49.0 (TID 45). 3090 bytes result sent to driver
17/12/20 18:27:31 INFO DAGScheduler: ResultStage 49 (take at <unknown>:0) finished in 0.706 s
17/12/20 18:27:31 INFO DAGScheduler: Job 22 finished: take at <unknown>:0, took 0.710304 s
17/12/20 18:27:31 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 45) in 706 ms on localhost (executor driver) (1/1)
17/12/20 18:27:31 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool 
17/12/20 18:27:31 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:31 INFO SparkSqlParser: Parsing command: sparklyr_tmp_1c0ca70668c
17/12/20 18:27:31 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:31 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0ca70668c` AS `zzz18`
WHERE (0 = 1)
17/12/20 18:27:31 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:31 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0ca70668c`
17/12/20 18:27:31 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:27:31 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:31 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz19`
WHERE (0 = 1)
17/12/20 18:27:31 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:31 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.0023) AS `V1`, (`V2` < 0.0138) AS `V2`, (`V3` < 0.0185) AS `V3`, (`V4` < 0.009) AS `V4`, (`V5` < 0.0375) AS `V5`, (`V6` < 0.054) AS `V6`, (`V7` < 0.054) AS `V7`, (`V8` < 0.0185) AS `V8`, (`V9` < 0.026) AS `V9`, (`V10` < 0.026) AS `V10`
FROM `analyis_tbl`
17/12/20 18:27:31 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:27:31 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:31 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz20`
WHERE (0 = 1)
17/12/20 18:27:31 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:31 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 18:27:31 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 18:27:31 INFO DAGScheduler: Got job 23 (collect at utils.scala:196) with 2 output partitions
17/12/20 18:27:31 INFO DAGScheduler: Final stage: ResultStage 51 (collect at utils.scala:196)
17/12/20 18:27:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 50)
17/12/20 18:27:31 INFO DAGScheduler: Missing parents: List()
17/12/20 18:27:31 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[165] at collect at utils.scala:196), which has no missing parents
17/12/20 18:27:31 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 135.4 KB, free 2002.8 MB)
17/12/20 18:27:31 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 51.2 KB, free 2002.7 MB)
17/12/20 18:27:31 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 127.0.0.1:64668 (size: 51.2 KB, free: 2004.3 MB)
17/12/20 18:27:31 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:31 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 51 (MapPartitionsRDD[165] at collect at utils.scala:196)
17/12/20 18:27:31 INFO TaskSchedulerImpl: Adding task set 51.0 with 2 tasks
17/12/20 18:27:31 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 46, localhost, executor driver, partition 0, PROCESS_LOCAL, 5947 bytes)
17/12/20 18:27:31 INFO TaskSetManager: Starting task 1.0 in stage 51.0 (TID 47, localhost, executor driver, partition 1, PROCESS_LOCAL, 5947 bytes)
17/12/20 18:27:31 INFO Executor: Running task 0.0 in stage 51.0 (TID 46)
17/12/20 18:27:31 INFO Executor: Running task 1.0 in stage 51.0 (TID 47)
17/12/20 18:27:31 INFO BlockManager: Found block rdd_158_1 locally
17/12/20 18:27:31 INFO Executor: Finished task 1.0 in stage 51.0 (TID 47). 1644 bytes result sent to driver
17/12/20 18:27:31 INFO BlockManager: Found block rdd_158_0 locally
17/12/20 18:27:31 INFO Executor: Finished task 0.0 in stage 51.0 (TID 46). 1626 bytes result sent to driver
17/12/20 18:27:31 INFO TaskSetManager: Finished task 1.0 in stage 51.0 (TID 47) in 9 ms on localhost (executor driver) (1/2)
17/12/20 18:27:31 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 46) in 16 ms on localhost (executor driver) (2/2)
17/12/20 18:27:31 INFO DAGScheduler: ResultStage 51 (collect at utils.scala:196) finished in 0.017 s
17/12/20 18:27:31 INFO DAGScheduler: Job 23 finished: collect at utils.scala:196, took 0.025706 s
17/12/20 18:27:31 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
17/12/20 18:27:31 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:31 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:27:31 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:27:31 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:27:31 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:27:31 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:27:31 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:27:31 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:27:31 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:31 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:27:31 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:27:31 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:27:31 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:27:31 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:27:31 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:27:31 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:27:31 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:31 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:27:31 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:27:31 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:27:31 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:27:31 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:27:31 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:27:31 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:27:31 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:31 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:27:31 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:27:31 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:27:31 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:27:31 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:27:31 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:27:31 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:27:58 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:58 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:27:58 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:27:58 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:27:58 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:27:58 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:27:58 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:27:58 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:27:58 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 18:27:58 INFO DAGScheduler: Got job 24 (collect at utils.scala:58) with 1 output partitions
17/12/20 18:27:58 INFO DAGScheduler: Final stage: ResultStage 52 (collect at utils.scala:58)
17/12/20 18:27:58 INFO DAGScheduler: Parents of final stage: List()
17/12/20 18:27:58 INFO DAGScheduler: Missing parents: List()
17/12/20 18:27:58 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[175] at map at utils.scala:55), which has no missing parents
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 8.7 KB, free 2002.7 MB)
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2002.7 MB)
17/12/20 18:27:58 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 127.0.0.1:64668 (size: 4.6 KB, free: 2004.3 MB)
17/12/20 18:27:58 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[175] at map at utils.scala:55)
17/12/20 18:27:58 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks
17/12/20 18:27:58 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 48, localhost, executor driver, partition 0, PROCESS_LOCAL, 6647 bytes)
17/12/20 18:27:58 INFO Executor: Running task 0.0 in stage 52.0 (TID 48)
17/12/20 18:27:58 INFO Executor: Finished task 0.0 in stage 52.0 (TID 48). 1107 bytes result sent to driver
17/12/20 18:27:58 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 48) in 3 ms on localhost (executor driver) (1/1)
17/12/20 18:27:58 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool 
17/12/20 18:27:58 INFO DAGScheduler: ResultStage 52 (collect at utils.scala:58) finished in 0.004 s
17/12/20 18:27:58 INFO DAGScheduler: Job 24 finished: collect at utils.scala:58, took 0.006622 s
17/12/20 18:27:58 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:58 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:58 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:27:58 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:58 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 18:27:58 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 18:27:58 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 18:27:58 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 18:27:58 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 18:27:58 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 293.7 KB, free 2002.4 MB)
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2002.4 MB)
17/12/20 18:27:58 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 127.0.0.1:64668 (size: 24.0 KB, free: 2004.3 MB)
17/12/20 18:27:58 INFO SparkContext: Created broadcast 41 from sql at <unknown>:0
17/12/20 18:27:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 18:27:58 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 18:27:58 INFO DAGScheduler: Registering RDD 179 (sql at <unknown>:0)
17/12/20 18:27:58 INFO DAGScheduler: Registering RDD 184 (sql at <unknown>:0)
17/12/20 18:27:58 INFO DAGScheduler: Got job 25 (sql at <unknown>:0) with 1 output partitions
17/12/20 18:27:58 INFO DAGScheduler: Final stage: ResultStage 55 (sql at <unknown>:0)
17/12/20 18:27:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 54)
17/12/20 18:27:58 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 54)
17/12/20 18:27:58 INFO DAGScheduler: Submitting ShuffleMapStage 53 (MapPartitionsRDD[179] at sql at <unknown>:0), which has no missing parents
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 13.1 KB, free 2002.4 MB)
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2002.4 MB)
17/12/20 18:27:58 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 127.0.0.1:64668 (size: 7.5 KB, free: 2004.3 MB)
17/12/20 18:27:58 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 53 (MapPartitionsRDD[179] at sql at <unknown>:0)
17/12/20 18:27:58 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks
17/12/20 18:27:58 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 49, localhost, executor driver, partition 0, PROCESS_LOCAL, 6678 bytes)
17/12/20 18:27:58 INFO Executor: Running task 0.0 in stage 53.0 (TID 49)
17/12/20 18:27:58 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/RtmpusRKRl/spark_serialize_96f38e202125ee72a36637f8fec8b859ebb73c199aa2398f45294e97f33a50e2.csv, range: 0-1920, partition values: [empty row]
17/12/20 18:27:58 INFO Executor: Finished task 0.0 in stage 53.0 (TID 49). 1640 bytes result sent to driver
17/12/20 18:27:58 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 49) in 27 ms on localhost (executor driver) (1/1)
17/12/20 18:27:58 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool 
17/12/20 18:27:58 INFO DAGScheduler: ShuffleMapStage 53 (sql at <unknown>:0) finished in 0.028 s
17/12/20 18:27:58 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:27:58 INFO DAGScheduler: running: Set()
17/12/20 18:27:58 INFO DAGScheduler: waiting: Set(ShuffleMapStage 54, ResultStage 55)
17/12/20 18:27:58 INFO DAGScheduler: failed: Set()
17/12/20 18:27:58 INFO DAGScheduler: Submitting ShuffleMapStage 54 (MapPartitionsRDD[184] at sql at <unknown>:0), which has no missing parents
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 15.3 KB, free 2002.4 MB)
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2002.4 MB)
17/12/20 18:27:58 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 127.0.0.1:64668 (size: 6.9 KB, free: 2004.3 MB)
17/12/20 18:27:58 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:58 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[184] at sql at <unknown>:0)
17/12/20 18:27:58 INFO TaskSchedulerImpl: Adding task set 54.0 with 2 tasks
17/12/20 18:27:58 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 50, localhost, executor driver, partition 0, ANY, 5944 bytes)
17/12/20 18:27:58 INFO TaskSetManager: Starting task 1.0 in stage 54.0 (TID 51, localhost, executor driver, partition 1, ANY, 5944 bytes)
17/12/20 18:27:58 INFO Executor: Running task 0.0 in stage 54.0 (TID 50)
17/12/20 18:27:58 INFO Executor: Running task 1.0 in stage 54.0 (TID 51)
17/12/20 18:27:58 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 18:27:58 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 18:27:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:27:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:27:58 INFO MemoryStore: Block rdd_181_0 stored as values in memory (estimated size 1512.0 B, free 2002.4 MB)
17/12/20 18:27:58 INFO BlockManagerInfo: Added rdd_181_0 in memory on 127.0.0.1:64668 (size: 1512.0 B, free: 2004.3 MB)
17/12/20 18:27:58 INFO MemoryStore: Block rdd_181_1 stored as values in memory (estimated size 1512.0 B, free 2002.4 MB)
17/12/20 18:27:58 INFO BlockManagerInfo: Added rdd_181_1 in memory on 127.0.0.1:64668 (size: 1512.0 B, free: 2004.3 MB)
17/12/20 18:27:58 INFO Executor: Finished task 0.0 in stage 54.0 (TID 50). 3162 bytes result sent to driver
17/12/20 18:27:58 INFO Executor: Finished task 1.0 in stage 54.0 (TID 51). 3072 bytes result sent to driver
17/12/20 18:27:58 INFO TaskSetManager: Finished task 1.0 in stage 54.0 (TID 51) in 15 ms on localhost (executor driver) (1/2)
17/12/20 18:27:58 INFO DAGScheduler: ShuffleMapStage 54 (sql at <unknown>:0) finished in 0.016 s
17/12/20 18:27:58 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:27:58 INFO DAGScheduler: running: Set()
17/12/20 18:27:58 INFO DAGScheduler: waiting: Set(ResultStage 55)
17/12/20 18:27:58 INFO DAGScheduler: failed: Set()
17/12/20 18:27:58 INFO DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[187] at sql at <unknown>:0), which has no missing parents
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 7.0 KB, free 2002.4 MB)
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2002.4 MB)
17/12/20 18:27:58 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 50) in 16 ms on localhost (executor driver) (2/2)
17/12/20 18:27:58 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
17/12/20 18:27:58 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 127.0.0.1:64668 (size: 3.7 KB, free: 2004.3 MB)
17/12/20 18:27:58 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[187] at sql at <unknown>:0)
17/12/20 18:27:58 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks
17/12/20 18:27:58 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 52, localhost, executor driver, partition 0, ANY, 5955 bytes)
17/12/20 18:27:58 INFO Executor: Running task 0.0 in stage 55.0 (TID 52)
17/12/20 18:27:58 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 18:27:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:27:58 INFO Executor: Finished task 0.0 in stage 55.0 (TID 52). 1873 bytes result sent to driver
17/12/20 18:27:58 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 52) in 4 ms on localhost (executor driver) (1/1)
17/12/20 18:27:58 INFO DAGScheduler: ResultStage 55 (sql at <unknown>:0) finished in 0.012 s
17/12/20 18:27:58 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool 
17/12/20 18:27:58 INFO DAGScheduler: Job 25 finished: sql at <unknown>:0, took 0.063406 s
17/12/20 18:27:58 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:58 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 18:27:58 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 18:27:58 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 12 is 145 bytes
17/12/20 18:27:58 INFO DAGScheduler: Registering RDD 191 (collect at utils.scala:196)
17/12/20 18:27:58 INFO DAGScheduler: Got job 26 (collect at utils.scala:196) with 1 output partitions
17/12/20 18:27:58 INFO DAGScheduler: Final stage: ResultStage 58 (collect at utils.scala:196)
17/12/20 18:27:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 57)
17/12/20 18:27:58 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 57)
17/12/20 18:27:58 INFO DAGScheduler: Submitting ShuffleMapStage 57 (MapPartitionsRDD[191] at collect at utils.scala:196), which has no missing parents
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 15.3 KB, free 2002.3 MB)
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 7.0 KB, free 2002.3 MB)
17/12/20 18:27:58 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 127.0.0.1:64668 (size: 7.0 KB, free: 2004.3 MB)
17/12/20 18:27:58 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:58 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 57 (MapPartitionsRDD[191] at collect at utils.scala:196)
17/12/20 18:27:58 INFO TaskSchedulerImpl: Adding task set 57.0 with 2 tasks
17/12/20 18:27:58 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 53, localhost, executor driver, partition 0, PROCESS_LOCAL, 5936 bytes)
17/12/20 18:27:58 INFO TaskSetManager: Starting task 1.0 in stage 57.0 (TID 54, localhost, executor driver, partition 1, PROCESS_LOCAL, 5936 bytes)
17/12/20 18:27:58 INFO Executor: Running task 1.0 in stage 57.0 (TID 54)
17/12/20 18:27:58 INFO Executor: Running task 0.0 in stage 57.0 (TID 53)
17/12/20 18:27:58 INFO BlockManager: Found block rdd_181_0 locally
17/12/20 18:27:58 INFO BlockManager: Found block rdd_181_1 locally
17/12/20 18:27:58 INFO Executor: Finished task 1.0 in stage 57.0 (TID 54). 2048 bytes result sent to driver
17/12/20 18:27:58 INFO TaskSetManager: Finished task 1.0 in stage 57.0 (TID 54) in 9 ms on localhost (executor driver) (1/2)
17/12/20 18:27:58 INFO Executor: Finished task 0.0 in stage 57.0 (TID 53). 2048 bytes result sent to driver
17/12/20 18:27:58 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 53) in 10 ms on localhost (executor driver) (2/2)
17/12/20 18:27:58 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool 
17/12/20 18:27:58 INFO DAGScheduler: ShuffleMapStage 57 (collect at utils.scala:196) finished in 0.011 s
17/12/20 18:27:58 INFO DAGScheduler: looking for newly runnable stages
17/12/20 18:27:58 INFO DAGScheduler: running: Set()
17/12/20 18:27:58 INFO DAGScheduler: waiting: Set(ResultStage 58)
17/12/20 18:27:58 INFO DAGScheduler: failed: Set()
17/12/20 18:27:58 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[194] at collect at utils.scala:196), which has no missing parents
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 7.0 KB, free 2002.3 MB)
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2002.3 MB)
17/12/20 18:27:58 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 127.0.0.1:64668 (size: 3.7 KB, free: 2004.3 MB)
17/12/20 18:27:58 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[194] at collect at utils.scala:196)
17/12/20 18:27:58 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks
17/12/20 18:27:58 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 55, localhost, executor driver, partition 0, ANY, 5947 bytes)
17/12/20 18:27:58 INFO Executor: Running task 0.0 in stage 58.0 (TID 55)
17/12/20 18:27:58 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 18:27:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 18:27:58 INFO Executor: Finished task 0.0 in stage 58.0 (TID 55). 1873 bytes result sent to driver
17/12/20 18:27:58 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 55) in 4 ms on localhost (executor driver) (1/1)
17/12/20 18:27:58 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool 
17/12/20 18:27:58 INFO DAGScheduler: ResultStage 58 (collect at utils.scala:196) finished in 0.004 s
17/12/20 18:27:58 INFO DAGScheduler: Job 26 finished: collect at utils.scala:196, took 0.021931 s
17/12/20 18:27:58 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:58 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz21`
WHERE (0 = 1)
17/12/20 18:27:58 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:58 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S3` + 0.91651514 * RANDN() AS `V1`, `S10` + 0.9486833 * RANDN() AS `V2`, `S10` + 0.9486833 * RANDN() AS `V3`, `S10` + 0.9486833 * RANDN() AS `V4`, `S5` + 0.9 * RANDN() AS `V5`, `S2` + 0.9486833 * RANDN() AS `V6`, `S8` + 0.9 * RANDN() AS `V7`, `S7` + 0.91104336 * RANDN() AS `V8`, `S4` + 0.93808315 * RANDN() AS `V9`, `S8` + 0.9 * RANDN() AS `V10`
FROM `analyis_tbl`) `vakwafcnsb`
17/12/20 18:27:58 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:27:58 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:58 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz22`
WHERE (0 = 1)
17/12/20 18:27:58 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:27:58 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 18:27:58 INFO CodeGenerator: Code generated in 13.369384 ms
17/12/20 18:27:58 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 18:27:58 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 12 is 145 bytes
17/12/20 18:27:58 INFO DAGScheduler: Got job 27 (take at <unknown>:0) with 1 output partitions
17/12/20 18:27:58 INFO DAGScheduler: Final stage: ResultStage 60 (take at <unknown>:0)
17/12/20 18:27:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 59)
17/12/20 18:27:58 INFO DAGScheduler: Missing parents: List()
17/12/20 18:27:58 INFO DAGScheduler: Submitting ResultStage 60 (WorkerRDD[200] at RDD at rdd.scala:18), which has no missing parents
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 121.5 KB, free 2002.2 MB)
17/12/20 18:27:58 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 46.2 KB, free 2002.2 MB)
17/12/20 18:27:58 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 127.0.0.1:64668 (size: 46.2 KB, free: 2004.2 MB)
17/12/20 18:27:58 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (WorkerRDD[200] at RDD at rdd.scala:18)
17/12/20 18:27:58 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks
17/12/20 18:27:58 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 56, localhost, executor driver, partition 0, PROCESS_LOCAL, 5914 bytes)
17/12/20 18:27:58 INFO Executor: Running task 0.0 in stage 60.0 (TID 56)
17/12/20 18:27:58 INFO BlockManager: Found block rdd_181_0 locally
17/12/20 18:27:59 INFO MemoryStore: Block rdd_200_0 stored as values in memory (estimated size 1600.0 B, free 2002.2 MB)
17/12/20 18:27:59 INFO BlockManagerInfo: Added rdd_200_0 in memory on 127.0.0.1:64668 (size: 1600.0 B, free: 2004.2 MB)
17/12/20 18:27:59 INFO Executor: Finished task 0.0 in stage 60.0 (TID 56). 3090 bytes result sent to driver
17/12/20 18:27:59 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 56) in 735 ms on localhost (executor driver) (1/1)
17/12/20 18:27:59 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool 
17/12/20 18:27:59 INFO DAGScheduler: ResultStage 60 (take at <unknown>:0) finished in 0.735 s
17/12/20 18:27:59 INFO DAGScheduler: Job 27 finished: take at <unknown>:0, took 0.737643 s
17/12/20 18:27:59 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 18:27:59 INFO DAGScheduler: Got job 28 (take at <unknown>:0) with 1 output partitions
17/12/20 18:27:59 INFO DAGScheduler: Final stage: ResultStage 62 (take at <unknown>:0)
17/12/20 18:27:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 61)
17/12/20 18:27:59 INFO DAGScheduler: Missing parents: List()
17/12/20 18:27:59 INFO DAGScheduler: Submitting ResultStage 62 (WorkerRDD[200] at RDD at rdd.scala:18), which has no missing parents
17/12/20 18:27:59 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 121.5 KB, free 2002.0 MB)
17/12/20 18:27:59 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 46.2 KB, free 2002.0 MB)
17/12/20 18:27:59 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 127.0.0.1:64668 (size: 46.2 KB, free: 2004.2 MB)
17/12/20 18:27:59 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:996
17/12/20 18:27:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 62 (WorkerRDD[200] at RDD at rdd.scala:18)
17/12/20 18:27:59 INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks
17/12/20 18:27:59 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 57, localhost, executor driver, partition 1, PROCESS_LOCAL, 5914 bytes)
17/12/20 18:27:59 INFO Executor: Running task 0.0 in stage 62.0 (TID 57)
17/12/20 18:27:59 INFO BlockManager: Found block rdd_181_1 locally
17/12/20 18:28:00 INFO MemoryStore: Block rdd_200_1 stored as values in memory (estimated size 1600.0 B, free 2002.0 MB)
17/12/20 18:28:00 INFO BlockManagerInfo: Added rdd_200_1 in memory on 127.0.0.1:64668 (size: 1600.0 B, free: 2004.2 MB)
17/12/20 18:28:00 WARN Executor: 1 block locks were not released by TID = 57:
[rdd_200_1]
17/12/20 18:28:00 INFO Executor: Finished task 0.0 in stage 62.0 (TID 57). 3090 bytes result sent to driver
17/12/20 18:28:00 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 57) in 684 ms on localhost (executor driver) (1/1)
17/12/20 18:28:00 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool 
17/12/20 18:28:00 INFO DAGScheduler: ResultStage 62 (take at <unknown>:0) finished in 0.684 s
17/12/20 18:28:00 INFO DAGScheduler: Job 28 finished: take at <unknown>:0, took 0.688403 s
17/12/20 18:28:00 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:28:00 INFO SparkSqlParser: Parsing command: sparklyr_tmp_1c0c3c0d31bf
17/12/20 18:28:00 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:28:00 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0c3c0d31bf` AS `zzz23`
WHERE (0 = 1)
17/12/20 18:28:00 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:28:00 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0c3c0d31bf`
17/12/20 18:28:00 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:28:00 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:28:00 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz24`
WHERE (0 = 1)
17/12/20 18:28:00 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:28:00 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.0023) AS `V1`, (`V2` < 0.0138) AS `V2`, (`V3` < 0.0185) AS `V3`, (`V4` < 0.009) AS `V4`, (`V5` < 0.0375) AS `V5`, (`V6` < 0.054) AS `V6`, (`V7` < 0.054) AS `V7`, (`V8` < 0.0185) AS `V8`, (`V9` < 0.026) AS `V9`, (`V10` < 0.026) AS `V10`
FROM `analyis_tbl`
17/12/20 18:28:00 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 18:28:00 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:28:00 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz25`
WHERE (0 = 1)
17/12/20 18:28:00 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:28:00 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 1687
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 1688
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 1689
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 1690
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 1691
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 1692
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 1693
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 1694
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 1695
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 1696
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 1697
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 1698
17/12/20 18:28:00 INFO ContextCleaner: Cleaned shuffle 10
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 127.0.0.1:64668 in memory (size: 7.5 KB, free: 2004.2 MB)
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 127.0.0.1:64668 in memory (size: 6.9 KB, free: 2004.2 MB)
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 127.0.0.1:64668 in memory (size: 3.7 KB, free: 2004.2 MB)
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 1867
17/12/20 18:28:00 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 127.0.0.1:64668 in memory (size: 7.0 KB, free: 2004.2 MB)
17/12/20 18:28:00 INFO DAGScheduler: Got job 29 (collect at utils.scala:196) with 2 output partitions
17/12/20 18:28:00 INFO DAGScheduler: Final stage: ResultStage 64 (collect at utils.scala:196)
17/12/20 18:28:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 63)
17/12/20 18:28:00 INFO DAGScheduler: Missing parents: List()
17/12/20 18:28:00 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[207] at collect at utils.scala:196), which has no missing parents
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 127.0.0.1:64668 in memory (size: 3.7 KB, free: 2004.2 MB)
17/12/20 18:28:00 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 134.3 KB, free 2002.0 MB)
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 127.0.0.1:64668 in memory (size: 46.6 KB, free: 2004.2 MB)
17/12/20 18:28:00 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 50.9 KB, free 2002.1 MB)
17/12/20 18:28:00 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 127.0.0.1:64668 (size: 50.9 KB, free: 2004.2 MB)
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 127.0.0.1:64668 in memory (size: 46.6 KB, free: 2004.2 MB)
17/12/20 18:28:00 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:996
17/12/20 18:28:00 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 64 (MapPartitionsRDD[207] at collect at utils.scala:196)
17/12/20 18:28:00 INFO TaskSchedulerImpl: Adding task set 64.0 with 2 tasks
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 127.0.0.1:64668 in memory (size: 51.2 KB, free: 2004.3 MB)
17/12/20 18:28:00 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 58, localhost, executor driver, partition 0, PROCESS_LOCAL, 5947 bytes)
17/12/20 18:28:00 INFO TaskSetManager: Starting task 1.0 in stage 64.0 (TID 59, localhost, executor driver, partition 1, PROCESS_LOCAL, 5947 bytes)
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2172
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2173
17/12/20 18:28:00 INFO Executor: Running task 1.0 in stage 64.0 (TID 59)
17/12/20 18:28:00 INFO Executor: Running task 0.0 in stage 64.0 (TID 58)
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 127.0.0.1:64668 in memory (size: 4.6 KB, free: 2004.3 MB)
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2222
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2223
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2229
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2230
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2231
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2232
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2233
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2234
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2235
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2236
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2237
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2238
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2239
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2240
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2241
17/12/20 18:28:00 INFO BlockManager: Found block rdd_200_0 locally
17/12/20 18:28:00 INFO ContextCleaner: Cleaned shuffle 13
17/12/20 18:28:00 INFO BlockManager: Found block rdd_200_1 locally
17/12/20 18:28:00 INFO Executor: Finished task 1.0 in stage 64.0 (TID 59). 1616 bytes result sent to driver
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 127.0.0.1:64668 in memory (size: 7.5 KB, free: 2004.3 MB)
17/12/20 18:28:00 INFO TaskSetManager: Finished task 1.0 in stage 64.0 (TID 59) in 12 ms on localhost (executor driver) (1/2)
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 127.0.0.1:64668 in memory (size: 6.9 KB, free: 2004.3 MB)
17/12/20 18:28:00 INFO Executor: Finished task 0.0 in stage 64.0 (TID 58). 1772 bytes result sent to driver
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 127.0.0.1:64668 in memory (size: 3.7 KB, free: 2004.3 MB)
17/12/20 18:28:00 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 58) in 16 ms on localhost (executor driver) (2/2)
17/12/20 18:28:00 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool 
17/12/20 18:28:00 INFO DAGScheduler: ResultStage 64 (collect at utils.scala:196) finished in 0.016 s
17/12/20 18:28:00 INFO DAGScheduler: Job 29 finished: collect at utils.scala:196, took 0.020317 s
17/12/20 18:28:00 INFO ContextCleaner: Cleaned accumulator 2410
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 127.0.0.1:64668 in memory (size: 7.0 KB, free: 2004.3 MB)
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 127.0.0.1:64668 in memory (size: 3.7 KB, free: 2004.3 MB)
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 127.0.0.1:64668 in memory (size: 46.2 KB, free: 2004.4 MB)
17/12/20 18:28:00 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 127.0.0.1:64668 in memory (size: 46.2 KB, free: 2004.4 MB)
17/12/20 18:28:00 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:28:00 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:28:00 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:28:00 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:28:00 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:28:00 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:28:00 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:28:00 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:28:00 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:28:00 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:28:00 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:28:00 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:28:00 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:28:00 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:28:00 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:28:00 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:28:00 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:28:00 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:28:00 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:28:00 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:28:00 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:28:00 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:28:00 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:28:00 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 18:28:00 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 18:28:00 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 18:28:00 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:28:00 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:28:00 INFO HiveMetaStore: 0: get_database: default
17/12/20 18:28:00 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 18:28:00 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 18:28:00 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 19:22:40 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@3051f0c4,BlockManagerId(driver, 127.0.0.1, 64668, None))] in 1 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:538)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:567)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:567)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:567)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:567)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.runAndReset(Unknown Source)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(Unknown Source)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:81)
	... 14 more
17/12/20 19:22:53 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@3051f0c4,BlockManagerId(driver, 127.0.0.1, 64668, None))] in 2 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:538)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:567)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:567)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:567)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:567)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.runAndReset(Unknown Source)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(Unknown Source)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:81)
	... 14 more
17/12/20 19:23:03 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
17/12/20 19:23:03 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
17/12/20 19:26:04 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 127.0.0.1:64668 in memory (size: 50.9 KB, free: 2004.5 MB)
17/12/20 20:11:21 INFO SparkContext: Invoking stop() from shutdown hook
17/12/20 20:11:21 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/12/20 20:11:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/12/20 20:11:21 INFO MemoryStore: MemoryStore cleared
17/12/20 20:11:21 INFO BlockManager: BlockManager stopped
17/12/20 20:11:21 INFO BlockManagerMaster: BlockManagerMaster stopped
17/12/20 20:11:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/12/20 20:11:21 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-6996707d-1ff2-4999-a3cb-15fb5dfb9c61\userFiles-f1cd7b40-4ded-4bf1-89c7-4d02f67e2f29
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-6996707d-1ff2-4999-a3cb-15fb5dfb9c61\userFiles-f1cd7b40-4ded-4bf1-89c7-4d02f67e2f29
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:102)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1842)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1841)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 20:11:21 INFO SparkContext: Successfully stopped SparkContext
17/12/20 20:11:21 INFO ShutdownHookManager: Shutdown hook called
17/12/20 20:11:21 INFO ShutdownHookManager: Deleting directory C:\Users\conan\AppData\Local\Temp\spark-6996707d-1ff2-4999-a3cb-15fb5dfb9c61
17/12/20 20:11:21 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-6996707d-1ff2-4999-a3cb-15fb5dfb9c61
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-6996707d-1ff2-4999-a3cb-15fb5dfb9c61
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 20:11:21 INFO ShutdownHookManager: Deleting directory C:\Users\conan\AppData\Local\Temp\spark-6996707d-1ff2-4999-a3cb-15fb5dfb9c61\userFiles-f1cd7b40-4ded-4bf1-89c7-4d02f67e2f29
17/12/20 20:11:21 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-6996707d-1ff2-4999-a3cb-15fb5dfb9c61\userFiles-f1cd7b40-4ded-4bf1-89c7-4d02f67e2f29
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-6996707d-1ff2-4999-a3cb-15fb5dfb9c61\userFiles-f1cd7b40-4ded-4bf1-89c7-4d02f67e2f29
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 20:11:34 INFO SparkContext: Running Spark version 2.1.0
17/12/20 20:11:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/12/20 20:11:35 INFO SecurityManager: Changing view acls to: conan
17/12/20 20:11:35 INFO SecurityManager: Changing modify acls to: conan
17/12/20 20:11:35 INFO SecurityManager: Changing view acls groups to: 
17/12/20 20:11:35 INFO SecurityManager: Changing modify acls groups to: 
17/12/20 20:11:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(conan); groups with view permissions: Set(); users  with modify permissions: Set(conan); groups with modify permissions: Set()
17/12/20 20:11:35 INFO Utils: Successfully started service 'sparkDriver' on port 61010.
17/12/20 20:11:35 INFO SparkEnv: Registering MapOutputTracker
17/12/20 20:11:35 INFO SparkEnv: Registering BlockManagerMaster
17/12/20 20:11:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/12/20 20:11:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/12/20 20:11:35 INFO DiskBlockManager: Created local directory at C:\Users\conan\AppData\Local\Temp\blockmgr-ab116036-ed3e-47d3-8355-c794549c218e
17/12/20 20:11:35 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
17/12/20 20:11:35 INFO SparkEnv: Registering OutputCommitCoordinator
17/12/20 20:11:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/12/20 20:11:35 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
17/12/20 20:11:35 INFO SparkContext: Added JAR file:/C:/Users/conan/Documents/R/win-library/3.4/sparklyr/java/sparklyr-2.1-2.11.jar at spark://127.0.0.1:61010/jars/sparklyr-2.1-2.11.jar with timestamp 1513800695982
17/12/20 20:11:36 INFO Executor: Starting executor ID driver on host localhost
17/12/20 20:11:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61031.
17/12/20 20:11:36 INFO NettyBlockTransferService: Server created on 127.0.0.1:61031
17/12/20 20:11:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/12/20 20:11:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 61031, None)
17/12/20 20:11:36 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:61031 with 2004.6 MB RAM, BlockManagerId(driver, 127.0.0.1, 61031, None)
17/12/20 20:11:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 61031, None)
17/12/20 20:11:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 61031, None)
17/12/20 20:11:37 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
17/12/20 20:11:37 INFO SharedState: Warehouse path is 'C:UsersconanAppDataLocalsparkspark-2.1.0-bin-hadoop2.7	mphive'.
17/12/20 20:11:37 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/12/20 20:11:37 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/12/20 20:11:37 INFO ObjectStore: ObjectStore, initialize called
17/12/20 20:11:38 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/12/20 20:11:38 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/12/20 20:11:39 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/12/20 20:11:40 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 20:11:40 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 20:11:41 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 20:11:41 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 20:11:41 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/12/20 20:11:41 INFO ObjectStore: Initialized ObjectStore
17/12/20 20:11:41 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/12/20 20:11:41 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/12/20 20:11:42 INFO HiveMetaStore: Added admin role in metastore
17/12/20 20:11:42 INFO HiveMetaStore: Added public role in metastore
17/12/20 20:11:42 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/12/20 20:11:42 INFO HiveMetaStore: 0: get_all_databases
17/12/20 20:11:42 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_all_databases	
17/12/20 20:11:42 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/12/20 20:11:42 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/12/20 20:11:42 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 20:11:42 INFO SessionState: Created local directory: C:/Users/conan/AppData/Local/Temp/a9f20e73-a638-48ba-a5fd-aeeec01142ff_resources
17/12/20 20:11:42 INFO SessionState: Created HDFS directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/conan/a9f20e73-a638-48ba-a5fd-aeeec01142ff
17/12/20 20:11:42 INFO SessionState: Created local directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/a9f20e73-a638-48ba-a5fd-aeeec01142ff
17/12/20 20:11:42 INFO SessionState: Created HDFS directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/conan/a9f20e73-a638-48ba-a5fd-aeeec01142ff/_tmp_space.db
17/12/20 20:11:42 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:UsersconanAppDataLocalsparkspark-2.1.0-bin-hadoop2.7	mphive
17/12/20 20:11:42 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:11:42 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:11:42 INFO HiveMetaStore: 0: get_database: global_temp
17/12/20 20:11:42 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: global_temp	
17/12/20 20:11:42 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
17/12/20 20:11:43 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:11:45 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:11:45 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:11:45 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:11:45 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:11:45 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:11:45 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:11:46 INFO CodeGenerator: Code generated in 240.094259 ms
17/12/20 20:11:46 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 20:11:46 INFO DAGScheduler: Got job 0 (collect at utils.scala:58) with 1 output partitions
17/12/20 20:11:46 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:58)
17/12/20 20:11:46 INFO DAGScheduler: Parents of final stage: List()
17/12/20 20:11:46 INFO DAGScheduler: Missing parents: List()
17/12/20 20:11:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at map at utils.scala:55), which has no missing parents
17/12/20 20:11:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 8.7 KB, free 2004.6 MB)
17/12/20 20:11:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2004.6 MB)
17/12/20 20:11:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:61031 (size: 4.6 KB, free: 2004.6 MB)
17/12/20 20:11:46 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996
17/12/20 20:11:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at map at utils.scala:55)
17/12/20 20:11:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/12/20 20:11:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6041 bytes)
17/12/20 20:11:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/12/20 20:11:46 INFO Executor: Fetching spark://127.0.0.1:61010/jars/sparklyr-2.1-2.11.jar with timestamp 1513800695982
17/12/20 20:11:46 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:61010 after 14 ms (0 ms spent in bootstraps)
17/12/20 20:11:46 INFO Utils: Fetching spark://127.0.0.1:61010/jars/sparklyr-2.1-2.11.jar to C:\Users\conan\AppData\Local\Temp\spark-ee5ae4cd-d8f2-47c6-b8c6-fa18c20899f7\userFiles-b6e5d874-5200-45d5-abeb-b3095777b78d\fetchFileTemp3458081845285279481.tmp
17/12/20 20:11:46 INFO Executor: Adding file:/C:/Users/conan/AppData/Local/Temp/spark-ee5ae4cd-d8f2-47c6-b8c6-fa18c20899f7/userFiles-b6e5d874-5200-45d5-abeb-b3095777b78d/sparklyr-2.1-2.11.jar to class loader
17/12/20 20:11:46 INFO CodeGenerator: Code generated in 13.223639 ms
17/12/20 20:11:46 INFO CodeGenerator: Code generated in 12.877019 ms
17/12/20 20:11:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1231 bytes result sent to driver
17/12/20 20:11:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 322 ms on localhost (executor driver) (1/1)
17/12/20 20:11:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/12/20 20:11:46 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:58) finished in 0.333 s
17/12/20 20:11:46 INFO DAGScheduler: Job 0 finished: collect at utils.scala:58, took 0.499443 s
17/12/20 20:11:46 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:61031 in memory (size: 4.6 KB, free: 2004.6 MB)
17/12/20 20:11:47 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:11:47 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:47 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 20:11:47 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 20:11:47 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 20:11:47 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 20:11:47 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 20:11:47 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 20:11:47 INFO CodeGenerator: Code generated in 12.390695 ms
17/12/20 20:11:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 293.7 KB, free 2004.3 MB)
17/12/20 20:11:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2004.3 MB)
17/12/20 20:11:47 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:61031 (size: 24.0 KB, free: 2004.6 MB)
17/12/20 20:11:47 INFO SparkContext: Created broadcast 1 from sql at <unknown>:0
17/12/20 20:11:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 20:11:47 INFO CodeGenerator: Code generated in 13.824748 ms
17/12/20 20:11:47 INFO CodeGenerator: Code generated in 12.00896 ms
17/12/20 20:11:47 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 20:11:47 INFO DAGScheduler: Registering RDD 9 (sql at <unknown>:0)
17/12/20 20:11:47 INFO DAGScheduler: Registering RDD 14 (sql at <unknown>:0)
17/12/20 20:11:47 INFO DAGScheduler: Got job 1 (sql at <unknown>:0) with 1 output partitions
17/12/20 20:11:47 INFO DAGScheduler: Final stage: ResultStage 3 (sql at <unknown>:0)
17/12/20 20:11:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
17/12/20 20:11:47 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
17/12/20 20:11:47 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[9] at sql at <unknown>:0), which has no missing parents
17/12/20 20:11:47 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.1 KB, free 2004.3 MB)
17/12/20 20:11:47 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2004.3 MB)
17/12/20 20:11:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:61031 (size: 7.5 KB, free: 2004.6 MB)
17/12/20 20:11:47 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/12/20 20:11:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[9] at sql at <unknown>:0)
17/12/20 20:11:47 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/12/20 20:11:47 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 6676 bytes)
17/12/20 20:11:47 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/12/20 20:11:47 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/RtmpusRKRl/spark_serialize_4dc4fe64e205e600184c046c9b0d1035d59e1875c28a0b89d9f0e4366046658d.csv, range: 0-1928, partition values: [empty row]
17/12/20 20:11:47 INFO CodeGenerator: Code generated in 19.206408 ms
17/12/20 20:11:47 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1896 bytes result sent to driver
17/12/20 20:11:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 281 ms on localhost (executor driver) (1/1)
17/12/20 20:11:47 INFO DAGScheduler: ShuffleMapStage 1 (sql at <unknown>:0) finished in 0.281 s
17/12/20 20:11:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/12/20 20:11:47 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:11:47 INFO DAGScheduler: running: Set()
17/12/20 20:11:47 INFO DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)
17/12/20 20:11:47 INFO DAGScheduler: failed: Set()
17/12/20 20:11:48 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[14] at sql at <unknown>:0), which has no missing parents
17/12/20 20:11:48 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.3 KB, free 2004.3 MB)
17/12/20 20:11:48 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2004.2 MB)
17/12/20 20:11:48 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:61031 (size: 6.9 KB, free: 2004.6 MB)
17/12/20 20:11:48 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/12/20 20:11:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[14] at sql at <unknown>:0)
17/12/20 20:11:48 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
17/12/20 20:11:48 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5942 bytes)
17/12/20 20:11:48 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3, localhost, executor driver, partition 1, ANY, 5942 bytes)
17/12/20 20:11:48 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/12/20 20:11:48 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)
17/12/20 20:11:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 20:11:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 20:11:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
17/12/20 20:11:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
17/12/20 20:11:48 INFO MemoryStore: Block rdd_11_1 stored as values in memory (estimated size 1512.0 B, free 2004.2 MB)
17/12/20 20:11:48 INFO MemoryStore: Block rdd_11_0 stored as values in memory (estimated size 1512.0 B, free 2004.2 MB)
17/12/20 20:11:48 INFO BlockManagerInfo: Added rdd_11_1 in memory on 127.0.0.1:61031 (size: 1512.0 B, free: 2004.6 MB)
17/12/20 20:11:48 INFO BlockManagerInfo: Added rdd_11_0 in memory on 127.0.0.1:61031 (size: 1512.0 B, free: 2004.6 MB)
17/12/20 20:11:48 INFO CodeGenerator: Code generated in 5.118486 ms
17/12/20 20:11:48 INFO CodeGenerator: Code generated in 69.335937 ms
17/12/20 20:11:48 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3064 bytes result sent to driver
17/12/20 20:11:48 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 3151 bytes result sent to driver
17/12/20 20:11:48 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 215 ms on localhost (executor driver) (1/2)
17/12/20 20:11:48 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 215 ms on localhost (executor driver) (2/2)
17/12/20 20:11:48 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/12/20 20:11:48 INFO DAGScheduler: ShuffleMapStage 2 (sql at <unknown>:0) finished in 0.215 s
17/12/20 20:11:48 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:11:48 INFO DAGScheduler: running: Set()
17/12/20 20:11:48 INFO DAGScheduler: waiting: Set(ResultStage 3)
17/12/20 20:11:48 INFO DAGScheduler: failed: Set()
17/12/20 20:11:48 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at sql at <unknown>:0), which has no missing parents
17/12/20 20:11:48 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 2004.2 MB)
17/12/20 20:11:48 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2004.2 MB)
17/12/20 20:11:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:61031 (size: 3.7 KB, free: 2004.6 MB)
17/12/20 20:11:48 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:996
17/12/20 20:11:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at sql at <unknown>:0)
17/12/20 20:11:48 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/12/20 20:11:48 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4, localhost, executor driver, partition 0, ANY, 5953 bytes)
17/12/20 20:11:48 INFO Executor: Running task 0.0 in stage 3.0 (TID 4)
17/12/20 20:11:48 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 20:11:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:11:48 INFO Executor: Finished task 0.0 in stage 3.0 (TID 4). 1865 bytes result sent to driver
17/12/20 20:11:48 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 16 ms on localhost (executor driver) (1/1)
17/12/20 20:11:48 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/12/20 20:11:48 INFO DAGScheduler: ResultStage 3 (sql at <unknown>:0) finished in 0.016 s
17/12/20 20:11:48 INFO DAGScheduler: Job 1 finished: sql at <unknown>:0, took 0.572053 s
17/12/20 20:11:48 INFO CodeGenerator: Code generated in 7.784208 ms
17/12/20 20:11:48 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:48 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 20:11:48 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 20:11:48 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:61031 in memory (size: 7.5 KB, free: 2004.6 MB)
17/12/20 20:11:48 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 145 bytes
17/12/20 20:11:48 INFO DAGScheduler: Registering RDD 21 (collect at utils.scala:196)
17/12/20 20:11:48 INFO DAGScheduler: Got job 2 (collect at utils.scala:196) with 1 output partitions
17/12/20 20:11:48 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:196)
17/12/20 20:11:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/12/20 20:11:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/12/20 20:11:48 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:61031 in memory (size: 6.9 KB, free: 2004.6 MB)
17/12/20 20:11:48 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[21] at collect at utils.scala:196), which has no missing parents
17/12/20 20:11:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.3 KB, free 2004.3 MB)
17/12/20 20:11:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2004.3 MB)
17/12/20 20:11:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:61031 (size: 6.9 KB, free: 2004.6 MB)
17/12/20 20:11:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:996
17/12/20 20:11:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[21] at collect at utils.scala:196)
17/12/20 20:11:48 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/12/20 20:11:48 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:61031 in memory (size: 3.7 KB, free: 2004.6 MB)
17/12/20 20:11:48 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 5934 bytes)
17/12/20 20:11:48 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6, localhost, executor driver, partition 1, PROCESS_LOCAL, 5934 bytes)
17/12/20 20:11:48 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
17/12/20 20:11:48 INFO Executor: Running task 1.0 in stage 5.0 (TID 6)
17/12/20 20:11:48 INFO BlockManager: Found block rdd_11_0 locally
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 238
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 50
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 51
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 57
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 58
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 59
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 60
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 61
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 62
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 63
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 64
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 65
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 66
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 67
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 68
17/12/20 20:11:48 INFO ContextCleaner: Cleaned accumulator 69
17/12/20 20:11:48 INFO ContextCleaner: Cleaned shuffle 1
17/12/20 20:11:48 INFO BlockManager: Found block rdd_11_1 locally
17/12/20 20:11:48 INFO Executor: Finished task 1.0 in stage 5.0 (TID 6). 2037 bytes result sent to driver
17/12/20 20:11:48 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 32 ms on localhost (executor driver) (1/2)
17/12/20 20:11:48 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1958 bytes result sent to driver
17/12/20 20:11:48 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:196) finished in 0.047 s
17/12/20 20:11:48 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:11:48 INFO DAGScheduler: running: Set()
17/12/20 20:11:48 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/12/20 20:11:48 INFO DAGScheduler: failed: Set()
17/12/20 20:11:48 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[24] at collect at utils.scala:196), which has no missing parents
17/12/20 20:11:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.0 KB, free 2004.3 MB)
17/12/20 20:11:48 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 47 ms on localhost (executor driver) (2/2)
17/12/20 20:11:48 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/12/20 20:11:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2004.3 MB)
17/12/20 20:11:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:61031 (size: 3.7 KB, free: 2004.6 MB)
17/12/20 20:11:48 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:996
17/12/20 20:11:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[24] at collect at utils.scala:196)
17/12/20 20:11:48 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/12/20 20:11:48 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 7, localhost, executor driver, partition 0, ANY, 5945 bytes)
17/12/20 20:11:48 INFO Executor: Running task 0.0 in stage 6.0 (TID 7)
17/12/20 20:11:48 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 20:11:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:11:48 INFO Executor: Finished task 0.0 in stage 6.0 (TID 7). 1707 bytes result sent to driver
17/12/20 20:11:48 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 7) in 0 ms on localhost (executor driver) (1/1)
17/12/20 20:11:48 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:196) finished in 0.000 s
17/12/20 20:11:48 INFO DAGScheduler: Job 2 finished: collect at utils.scala:196, took 0.075686 s
17/12/20 20:11:48 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/12/20 20:11:48 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:48 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz26`
WHERE (0 = 1)
17/12/20 20:11:48 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:48 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S2` + 0.92195445 * RANDN() AS `V1`, `S9` + 0.9 * RANDN() AS `V2`, `S6` + 0.9486833 * RANDN() AS `V3`, `S10` + 0.92195445 * RANDN() AS `V4`, `S5` + 0.93273791 * RANDN() AS `V5`, `S1` + 0.92736185 * RANDN() AS `V6`, `S8` + 0.91651514 * RANDN() AS `V7`, `S3` + 0.90553851 * RANDN() AS `V8`, `S10` + 0.92195445 * RANDN() AS `V9`, `S7` + 0.93273791 * RANDN() AS `V10`
FROM `analyis_tbl`) `ygmntzlrla`
17/12/20 20:11:48 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:11:48 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:48 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz27`
WHERE (0 = 1)
17/12/20 20:11:48 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:48 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 20:11:49 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:61031 in memory (size: 6.9 KB, free: 2004.6 MB)
17/12/20 20:11:49 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 127.0.0.1:61031 in memory (size: 3.7 KB, free: 2004.6 MB)
17/12/20 20:11:49 INFO ContextCleaner: Cleaned accumulator 1
17/12/20 20:11:49 INFO ContextCleaner: Cleaned accumulator 0
17/12/20 20:11:49 INFO CodeGenerator: Code generated in 25.227691 ms
17/12/20 20:11:49 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 20:11:49 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 145 bytes
17/12/20 20:11:49 INFO DAGScheduler: Got job 3 (take at <unknown>:0) with 1 output partitions
17/12/20 20:11:49 INFO DAGScheduler: Final stage: ResultStage 8 (take at <unknown>:0)
17/12/20 20:11:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
17/12/20 20:11:49 INFO DAGScheduler: Missing parents: List()
17/12/20 20:11:49 INFO DAGScheduler: Submitting ResultStage 8 (WorkerRDD[30] at RDD at rdd.scala:18), which has no missing parents
17/12/20 20:11:49 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 121.9 KB, free 2004.2 MB)
17/12/20 20:11:49 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 46.5 KB, free 2004.1 MB)
17/12/20 20:11:49 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:61031 (size: 46.5 KB, free: 2004.5 MB)
17/12/20 20:11:49 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:996
17/12/20 20:11:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (WorkerRDD[30] at RDD at rdd.scala:18)
17/12/20 20:11:49 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/12/20 20:11:49 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5913 bytes)
17/12/20 20:11:49 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
17/12/20 20:11:49 INFO BlockManager: Found block rdd_11_0 locally
17/12/20 20:11:49 INFO CodeGenerator: Code generated in 24.046995 ms
17/12/20 20:11:49 INFO CodeGenerator: Code generated in 12.053516 ms
17/12/20 20:11:50 INFO MemoryStore: Block rdd_30_0 stored as values in memory (estimated size 1600.0 B, free 2004.1 MB)
17/12/20 20:11:50 INFO BlockManagerInfo: Added rdd_30_0 in memory on 127.0.0.1:61031 (size: 1600.0 B, free: 2004.5 MB)
17/12/20 20:11:50 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 3169 bytes result sent to driver
17/12/20 20:11:50 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 1160 ms on localhost (executor driver) (1/1)
17/12/20 20:11:50 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/12/20 20:11:50 INFO DAGScheduler: ResultStage 8 (take at <unknown>:0) finished in 1.160 s
17/12/20 20:11:50 INFO DAGScheduler: Job 3 finished: take at <unknown>:0, took 1.166432 s
17/12/20 20:11:50 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 20:11:50 INFO DAGScheduler: Got job 4 (take at <unknown>:0) with 1 output partitions
17/12/20 20:11:50 INFO DAGScheduler: Final stage: ResultStage 10 (take at <unknown>:0)
17/12/20 20:11:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
17/12/20 20:11:50 INFO DAGScheduler: Missing parents: List()
17/12/20 20:11:50 INFO DAGScheduler: Submitting ResultStage 10 (WorkerRDD[30] at RDD at rdd.scala:18), which has no missing parents
17/12/20 20:11:50 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 121.9 KB, free 2004.0 MB)
17/12/20 20:11:50 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 46.5 KB, free 2004.0 MB)
17/12/20 20:11:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:61031 (size: 46.5 KB, free: 2004.5 MB)
17/12/20 20:11:50 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:996
17/12/20 20:11:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (WorkerRDD[30] at RDD at rdd.scala:18)
17/12/20 20:11:50 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
17/12/20 20:11:50 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 5913 bytes)
17/12/20 20:11:50 INFO Executor: Running task 0.0 in stage 10.0 (TID 9)
17/12/20 20:11:50 INFO BlockManager: Found block rdd_11_1 locally
17/12/20 20:11:51 INFO MemoryStore: Block rdd_30_1 stored as values in memory (estimated size 1600.0 B, free 2004.0 MB)
17/12/20 20:11:51 INFO BlockManagerInfo: Added rdd_30_1 in memory on 127.0.0.1:61031 (size: 1600.0 B, free: 2004.5 MB)
17/12/20 20:11:51 WARN Executor: 1 block locks were not released by TID = 9:
[rdd_30_1]
17/12/20 20:11:51 INFO Executor: Finished task 0.0 in stage 10.0 (TID 9). 3169 bytes result sent to driver
17/12/20 20:11:51 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 802 ms on localhost (executor driver) (1/1)
17/12/20 20:11:51 INFO DAGScheduler: ResultStage 10 (take at <unknown>:0) finished in 0.802 s
17/12/20 20:11:51 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
17/12/20 20:11:51 INFO DAGScheduler: Job 4 finished: take at <unknown>:0, took 0.805283 s
17/12/20 20:11:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:51 INFO SparkSqlParser: Parsing command: sparklyr_tmp_1c0c7aef6181
17/12/20 20:11:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:51 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0c7aef6181` AS `zzz28`
WHERE (0 = 1)
17/12/20 20:11:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:51 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0c7aef6181`
17/12/20 20:11:51 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:11:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:51 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz29`
WHERE (0 = 1)
17/12/20 20:11:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:51 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.0018) AS `V1`, (`V2` < 0.0185) AS `V2`, (`V3` < 0.0185) AS `V3`, (`V4` < 0.0018) AS `V4`, (`V5` < 0.3) AS `V5`, (`V6` < 0.054) AS `V6`, (`V7` < 0.026) AS `V7`, (`V8` < 0.0035) AS `V8`, (`V9` < 0.0055) AS `V9`, (`V10` < 0.0138) AS `V10`
FROM `analyis_tbl`
17/12/20 20:11:51 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:11:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:51 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz30`
WHERE (0 = 1)
17/12/20 20:11:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:51 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 20:11:51 INFO CodeGenerator: Code generated in 19.257004 ms
17/12/20 20:11:51 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 20:11:51 INFO DAGScheduler: Got job 5 (collect at utils.scala:196) with 2 output partitions
17/12/20 20:11:51 INFO DAGScheduler: Final stage: ResultStage 12 (collect at utils.scala:196)
17/12/20 20:11:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
17/12/20 20:11:51 INFO DAGScheduler: Missing parents: List()
17/12/20 20:11:51 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[37] at collect at utils.scala:196), which has no missing parents
17/12/20 20:11:51 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 134.7 KB, free 2003.8 MB)
17/12/20 20:11:51 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 51.2 KB, free 2003.8 MB)
17/12/20 20:11:51 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 127.0.0.1:61031 (size: 51.2 KB, free: 2004.4 MB)
17/12/20 20:11:51 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:996
17/12/20 20:11:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[37] at collect at utils.scala:196)
17/12/20 20:11:51 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks
17/12/20 20:11:51 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 5945 bytes)
17/12/20 20:11:51 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 11, localhost, executor driver, partition 1, PROCESS_LOCAL, 5945 bytes)
17/12/20 20:11:51 INFO Executor: Running task 0.0 in stage 12.0 (TID 10)
17/12/20 20:11:51 INFO Executor: Running task 1.0 in stage 12.0 (TID 11)
17/12/20 20:11:51 INFO BlockManager: Found block rdd_30_0 locally
17/12/20 20:11:51 INFO BlockManager: Found block rdd_30_1 locally
17/12/20 20:11:51 INFO CodeGenerator: Code generated in 100.727636 ms
17/12/20 20:11:51 INFO Executor: Finished task 0.0 in stage 12.0 (TID 10). 1556 bytes result sent to driver
17/12/20 20:11:51 INFO Executor: Finished task 1.0 in stage 12.0 (TID 11). 1612 bytes result sent to driver
17/12/20 20:11:51 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 128 ms on localhost (executor driver) (1/2)
17/12/20 20:11:51 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 11) in 128 ms on localhost (executor driver) (2/2)
17/12/20 20:11:51 INFO DAGScheduler: ResultStage 12 (collect at utils.scala:196) finished in 0.144 s
17/12/20 20:11:51 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
17/12/20 20:11:51 INFO DAGScheduler: Job 5 finished: collect at utils.scala:196, took 0.139931 s
17/12/20 20:11:51 INFO CodeGenerator: Code generated in 8.804809 ms
17/12/20 20:11:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:51 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:11:51 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:11:51 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:11:51 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:11:51 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:11:51 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:11:51 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:11:51 INFO CodeGenerator: Code generated in 6.95202 ms
17/12/20 20:11:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:51 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:11:51 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:11:51 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:11:51 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:11:51 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:11:51 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:11:51 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:11:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:51 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:11:51 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:11:51 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:11:51 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:11:51 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:11:51 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:11:51 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:11:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:51 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:11:51 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:11:51 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:11:51 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:11:51 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:11:51 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:11:51 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:11:52 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:11:52 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:11:52 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:11:52 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:11:52 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:11:52 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:11:52 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:11:52 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:25:07 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:07 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:25:07 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:07 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:07 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:07 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:07 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:25:07 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:25:07 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 20:25:07 INFO DAGScheduler: Got job 6 (collect at utils.scala:58) with 1 output partitions
17/12/20 20:25:07 INFO DAGScheduler: Final stage: ResultStage 13 (collect at utils.scala:58)
17/12/20 20:25:07 INFO DAGScheduler: Parents of final stage: List()
17/12/20 20:25:07 INFO DAGScheduler: Missing parents: List()
17/12/20 20:25:07 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[48] at map at utils.scala:55), which has no missing parents
17/12/20 20:25:07 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 8.7 KB, free 2003.8 MB)
17/12/20 20:25:07 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2003.8 MB)
17/12/20 20:25:07 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 127.0.0.1:61031 (size: 4.6 KB, free: 2004.4 MB)
17/12/20 20:25:07 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[48] at map at utils.scala:55)
17/12/20 20:25:07 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
17/12/20 20:25:07 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 6432 bytes)
17/12/20 20:25:07 INFO Executor: Running task 0.0 in stage 13.0 (TID 12)
17/12/20 20:25:07 INFO Executor: Finished task 0.0 in stage 13.0 (TID 12). 938 bytes result sent to driver
17/12/20 20:25:07 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 12) in 0 ms on localhost (executor driver) (1/1)
17/12/20 20:25:07 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
17/12/20 20:25:07 INFO DAGScheduler: ResultStage 13 (collect at utils.scala:58) finished in 0.000 s
17/12/20 20:25:07 INFO DAGScheduler: Job 6 finished: collect at utils.scala:58, took 0.014241 s
17/12/20 20:25:07 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:07 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:07 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:25:07 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:07 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 20:25:07 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 20:25:07 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 20:25:07 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 20:25:07 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 20:25:07 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 20:25:08 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 293.7 KB, free 2003.5 MB)
17/12/20 20:25:08 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2003.5 MB)
17/12/20 20:25:08 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 127.0.0.1:61031 (size: 24.0 KB, free: 2004.4 MB)
17/12/20 20:25:08 INFO SparkContext: Created broadcast 11 from sql at <unknown>:0
17/12/20 20:25:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 20:25:08 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 20:25:08 INFO DAGScheduler: Registering RDD 52 (sql at <unknown>:0)
17/12/20 20:25:08 INFO DAGScheduler: Registering RDD 57 (sql at <unknown>:0)
17/12/20 20:25:08 INFO DAGScheduler: Got job 7 (sql at <unknown>:0) with 1 output partitions
17/12/20 20:25:08 INFO DAGScheduler: Final stage: ResultStage 16 (sql at <unknown>:0)
17/12/20 20:25:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)
17/12/20 20:25:08 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 15)
17/12/20 20:25:08 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[52] at sql at <unknown>:0), which has no missing parents
17/12/20 20:25:08 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 13.1 KB, free 2003.4 MB)
17/12/20 20:25:08 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2003.4 MB)
17/12/20 20:25:08 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 127.0.0.1:61031 (size: 7.5 KB, free: 2004.4 MB)
17/12/20 20:25:08 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[52] at sql at <unknown>:0)
17/12/20 20:25:08 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
17/12/20 20:25:08 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 6677 bytes)
17/12/20 20:25:08 INFO Executor: Running task 0.0 in stage 14.0 (TID 13)
17/12/20 20:25:08 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/RtmpusRKRl/spark_serialize_869f28f10e762cb294b4a4d547e81ac29cade98eae16016839994178bf91e0ea.csv, range: 0-1936, partition values: [empty row]
17/12/20 20:25:08 INFO Executor: Finished task 0.0 in stage 14.0 (TID 13). 1730 bytes result sent to driver
17/12/20 20:25:08 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 13) in 63 ms on localhost (executor driver) (1/1)
17/12/20 20:25:08 INFO DAGScheduler: ShuffleMapStage 14 (sql at <unknown>:0) finished in 0.063 s
17/12/20 20:25:08 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:25:08 INFO DAGScheduler: running: Set()
17/12/20 20:25:08 INFO DAGScheduler: waiting: Set(ShuffleMapStage 15, ResultStage 16)
17/12/20 20:25:08 INFO DAGScheduler: failed: Set()
17/12/20 20:25:08 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
17/12/20 20:25:08 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[57] at sql at <unknown>:0), which has no missing parents
17/12/20 20:25:08 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 15.3 KB, free 2003.4 MB)
17/12/20 20:25:08 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2003.4 MB)
17/12/20 20:25:08 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 127.0.0.1:61031 (size: 6.9 KB, free: 2004.4 MB)
17/12/20 20:25:08 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:08 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[57] at sql at <unknown>:0)
17/12/20 20:25:08 INFO TaskSchedulerImpl: Adding task set 15.0 with 2 tasks
17/12/20 20:25:08 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 14, localhost, executor driver, partition 0, ANY, 5943 bytes)
17/12/20 20:25:08 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 15, localhost, executor driver, partition 1, ANY, 5943 bytes)
17/12/20 20:25:08 INFO Executor: Running task 0.0 in stage 15.0 (TID 14)
17/12/20 20:25:08 INFO Executor: Running task 1.0 in stage 15.0 (TID 15)
17/12/20 20:25:08 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 20:25:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:25:08 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 20:25:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:25:08 INFO MemoryStore: Block rdd_54_1 stored as values in memory (estimated size 1512.0 B, free 2003.4 MB)
17/12/20 20:25:08 INFO BlockManagerInfo: Added rdd_54_1 in memory on 127.0.0.1:61031 (size: 1512.0 B, free: 2004.4 MB)
17/12/20 20:25:08 INFO MemoryStore: Block rdd_54_0 stored as values in memory (estimated size 1512.0 B, free 2003.4 MB)
17/12/20 20:25:08 INFO BlockManagerInfo: Added rdd_54_0 in memory on 127.0.0.1:61031 (size: 1512.0 B, free: 2004.4 MB)
17/12/20 20:25:08 INFO Executor: Finished task 1.0 in stage 15.0 (TID 15). 3083 bytes result sent to driver
17/12/20 20:25:08 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 15) in 31 ms on localhost (executor driver) (1/2)
17/12/20 20:25:08 INFO Executor: Finished task 0.0 in stage 15.0 (TID 14). 3241 bytes result sent to driver
17/12/20 20:25:08 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 14) in 31 ms on localhost (executor driver) (2/2)
17/12/20 20:25:08 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
17/12/20 20:25:08 INFO DAGScheduler: ShuffleMapStage 15 (sql at <unknown>:0) finished in 0.031 s
17/12/20 20:25:08 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:25:08 INFO DAGScheduler: running: Set()
17/12/20 20:25:08 INFO DAGScheduler: waiting: Set(ResultStage 16)
17/12/20 20:25:08 INFO DAGScheduler: failed: Set()
17/12/20 20:25:08 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[60] at sql at <unknown>:0), which has no missing parents
17/12/20 20:25:08 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 7.0 KB, free 2003.4 MB)
17/12/20 20:25:08 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.4 MB)
17/12/20 20:25:08 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 127.0.0.1:61031 (size: 3.7 KB, free: 2004.4 MB)
17/12/20 20:25:08 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[60] at sql at <unknown>:0)
17/12/20 20:25:08 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
17/12/20 20:25:08 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, ANY, 5954 bytes)
17/12/20 20:25:08 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
17/12/20 20:25:08 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 20:25:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:25:08 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 1707 bytes result sent to driver
17/12/20 20:25:08 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 0 ms on localhost (executor driver) (1/1)
17/12/20 20:25:08 INFO DAGScheduler: ResultStage 16 (sql at <unknown>:0) finished in 0.000 s
17/12/20 20:25:08 INFO DAGScheduler: Job 7 finished: sql at <unknown>:0, took 0.102855 s
17/12/20 20:25:08 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
17/12/20 20:25:08 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:08 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 20:25:08 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 20:25:08 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 145 bytes
17/12/20 20:25:08 INFO DAGScheduler: Registering RDD 64 (collect at utils.scala:196)
17/12/20 20:25:08 INFO DAGScheduler: Got job 8 (collect at utils.scala:196) with 1 output partitions
17/12/20 20:25:08 INFO DAGScheduler: Final stage: ResultStage 19 (collect at utils.scala:196)
17/12/20 20:25:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
17/12/20 20:25:08 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 18)
17/12/20 20:25:08 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[64] at collect at utils.scala:196), which has no missing parents
17/12/20 20:25:08 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 15.3 KB, free 2003.4 MB)
17/12/20 20:25:08 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2003.4 MB)
17/12/20 20:25:08 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 127.0.0.1:61031 (size: 6.9 KB, free: 2004.4 MB)
17/12/20 20:25:08 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:08 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[64] at collect at utils.scala:196)
17/12/20 20:25:08 INFO TaskSchedulerImpl: Adding task set 18.0 with 2 tasks
17/12/20 20:25:08 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 5935 bytes)
17/12/20 20:25:08 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 18, localhost, executor driver, partition 1, PROCESS_LOCAL, 5935 bytes)
17/12/20 20:25:08 INFO Executor: Running task 0.0 in stage 18.0 (TID 17)
17/12/20 20:25:08 INFO Executor: Running task 1.0 in stage 18.0 (TID 18)
17/12/20 20:25:08 INFO BlockManager: Found block rdd_54_1 locally
17/12/20 20:25:08 INFO BlockManager: Found block rdd_54_0 locally
17/12/20 20:25:08 INFO Executor: Finished task 1.0 in stage 18.0 (TID 18). 1792 bytes result sent to driver
17/12/20 20:25:08 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 18) in 16 ms on localhost (executor driver) (1/2)
17/12/20 20:25:08 INFO Executor: Finished task 0.0 in stage 18.0 (TID 17). 1950 bytes result sent to driver
17/12/20 20:25:08 INFO DAGScheduler: ShuffleMapStage 18 (collect at utils.scala:196) finished in 0.016 s
17/12/20 20:25:08 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:25:08 INFO DAGScheduler: running: Set()
17/12/20 20:25:08 INFO DAGScheduler: waiting: Set(ResultStage 19)
17/12/20 20:25:08 INFO DAGScheduler: failed: Set()
17/12/20 20:25:08 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[67] at collect at utils.scala:196), which has no missing parents
17/12/20 20:25:08 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 7.0 KB, free 2003.4 MB)
17/12/20 20:25:08 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.4 MB)
17/12/20 20:25:08 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 17) in 16 ms on localhost (executor driver) (2/2)
17/12/20 20:25:08 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
17/12/20 20:25:08 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 127.0.0.1:61031 (size: 3.7 KB, free: 2004.4 MB)
17/12/20 20:25:08 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[67] at collect at utils.scala:196)
17/12/20 20:25:08 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
17/12/20 20:25:08 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, ANY, 5946 bytes)
17/12/20 20:25:08 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
17/12/20 20:25:08 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 20:25:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:25:08 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 1707 bytes result sent to driver
17/12/20 20:25:08 INFO DAGScheduler: ResultStage 19 (collect at utils.scala:196) finished in 0.000 s
17/12/20 20:25:08 INFO DAGScheduler: Job 8 finished: collect at utils.scala:196, took 0.034090 s
17/12/20 20:25:08 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 0 ms on localhost (executor driver) (1/1)
17/12/20 20:25:08 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
17/12/20 20:25:08 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:08 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz31`
WHERE (0 = 1)
17/12/20 20:25:08 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:08 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S6` + 0.9 * RANDN() AS `V1`, `S4` + 0.93273791 * RANDN() AS `V2`, `S3` + 0.92736185 * RANDN() AS `V3`, `S8` + 0.92195445 * RANDN() AS `V4`, `S7` + 0.89442719 * RANDN() AS `V5`, `S6` + 0.9 * RANDN() AS `V6`, `S7` + 0.89442719 * RANDN() AS `V7`, `S7` + 0.89442719 * RANDN() AS `V8`, `S5` + 0.9 * RANDN() AS `V9`, `S8` + 0.92195445 * RANDN() AS `V10`
FROM `analyis_tbl`) `yjzqbnyvpv`
17/12/20 20:25:08 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:25:08 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:08 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz32`
WHERE (0 = 1)
17/12/20 20:25:08 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:08 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 20:25:08 INFO CodeGenerator: Code generated in 13.742057 ms
17/12/20 20:25:08 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 20:25:08 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 145 bytes
17/12/20 20:25:08 INFO DAGScheduler: Got job 9 (take at <unknown>:0) with 1 output partitions
17/12/20 20:25:08 INFO DAGScheduler: Final stage: ResultStage 21 (take at <unknown>:0)
17/12/20 20:25:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
17/12/20 20:25:08 INFO DAGScheduler: Missing parents: List()
17/12/20 20:25:08 INFO DAGScheduler: Submitting ResultStage 21 (WorkerRDD[73] at RDD at rdd.scala:18), which has no missing parents
17/12/20 20:25:08 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 114.3 KB, free 2003.3 MB)
17/12/20 20:25:08 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 43.1 KB, free 2003.2 MB)
17/12/20 20:25:08 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 127.0.0.1:61031 (size: 43.1 KB, free: 2004.3 MB)
17/12/20 20:25:08 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (WorkerRDD[73] at RDD at rdd.scala:18)
17/12/20 20:25:08 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
17/12/20 20:25:08 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 5914 bytes)
17/12/20 20:25:08 INFO Executor: Running task 0.0 in stage 21.0 (TID 20)
17/12/20 20:25:08 INFO BlockManager: Found block rdd_54_0 locally
17/12/20 20:25:09 INFO MemoryStore: Block rdd_73_0 stored as values in memory (estimated size 1600.0 B, free 2003.2 MB)
17/12/20 20:25:09 INFO BlockManagerInfo: Added rdd_73_0 in memory on 127.0.0.1:61031 (size: 1600.0 B, free: 2004.3 MB)
17/12/20 20:25:09 INFO Executor: Finished task 0.0 in stage 21.0 (TID 20). 3169 bytes result sent to driver
17/12/20 20:25:09 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 20) in 666 ms on localhost (executor driver) (1/1)
17/12/20 20:25:09 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
17/12/20 20:25:09 INFO DAGScheduler: ResultStage 21 (take at <unknown>:0) finished in 0.666 s
17/12/20 20:25:09 INFO DAGScheduler: Job 9 finished: take at <unknown>:0, took 0.690430 s
17/12/20 20:25:09 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 20:25:09 INFO DAGScheduler: Got job 10 (take at <unknown>:0) with 1 output partitions
17/12/20 20:25:09 INFO DAGScheduler: Final stage: ResultStage 23 (take at <unknown>:0)
17/12/20 20:25:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
17/12/20 20:25:09 INFO DAGScheduler: Missing parents: List()
17/12/20 20:25:09 INFO DAGScheduler: Submitting ResultStage 23 (WorkerRDD[73] at RDD at rdd.scala:18), which has no missing parents
17/12/20 20:25:09 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 114.3 KB, free 2003.1 MB)
17/12/20 20:25:09 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 43.1 KB, free 2003.1 MB)
17/12/20 20:25:09 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 127.0.0.1:61031 (size: 43.1 KB, free: 2004.3 MB)
17/12/20 20:25:09 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (WorkerRDD[73] at RDD at rdd.scala:18)
17/12/20 20:25:09 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
17/12/20 20:25:09 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 21, localhost, executor driver, partition 1, PROCESS_LOCAL, 5914 bytes)
17/12/20 20:25:09 INFO Executor: Running task 0.0 in stage 23.0 (TID 21)
17/12/20 20:25:09 INFO BlockManager: Found block rdd_54_1 locally
17/12/20 20:25:09 INFO MemoryStore: Block rdd_73_1 stored as values in memory (estimated size 1600.0 B, free 2003.1 MB)
17/12/20 20:25:09 INFO BlockManagerInfo: Added rdd_73_1 in memory on 127.0.0.1:61031 (size: 1600.0 B, free: 2004.3 MB)
17/12/20 20:25:09 WARN Executor: 1 block locks were not released by TID = 21:
[rdd_73_1]
17/12/20 20:25:09 INFO Executor: Finished task 0.0 in stage 23.0 (TID 21). 3155 bytes result sent to driver
17/12/20 20:25:09 INFO DAGScheduler: ResultStage 23 (take at <unknown>:0) finished in 0.740 s
17/12/20 20:25:09 INFO DAGScheduler: Job 10 finished: take at <unknown>:0, took 0.747403 s
17/12/20 20:25:09 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 21) in 740 ms on localhost (executor driver) (1/1)
17/12/20 20:25:09 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
17/12/20 20:25:09 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 127.0.0.1:61031 in memory (size: 3.7 KB, free: 2004.3 MB)
17/12/20 20:25:09 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 127.0.0.1:61031 in memory (size: 46.5 KB, free: 2004.3 MB)
17/12/20 20:25:09 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 127.0.0.1:61031 in memory (size: 46.5 KB, free: 2004.4 MB)
17/12/20 20:25:09 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 127.0.0.1:61031 in memory (size: 51.2 KB, free: 2004.4 MB)
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 543
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 544
17/12/20 20:25:09 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 127.0.0.1:61031 in memory (size: 4.6 KB, free: 2004.4 MB)
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 593
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 594
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 600
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 601
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 602
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 603
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 604
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 605
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 606
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 607
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 608
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 609
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 610
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 611
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 612
17/12/20 20:25:09 INFO ContextCleaner: Cleaned shuffle 4
17/12/20 20:25:09 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 127.0.0.1:61031 in memory (size: 7.5 KB, free: 2004.4 MB)
17/12/20 20:25:09 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 127.0.0.1:61031 in memory (size: 6.9 KB, free: 2004.4 MB)
17/12/20 20:25:09 INFO ContextCleaner: Cleaned accumulator 781
17/12/20 20:25:09 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 127.0.0.1:61031 in memory (size: 6.9 KB, free: 2004.5 MB)
17/12/20 20:25:09 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 127.0.0.1:61031 in memory (size: 3.7 KB, free: 2004.5 MB)
17/12/20 20:25:09 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 127.0.0.1:61031 in memory (size: 43.1 KB, free: 2004.5 MB)
17/12/20 20:25:09 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:10 INFO SparkSqlParser: Parsing command: sparklyr_tmp_1c0c40b060a6
17/12/20 20:25:10 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:10 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0c40b060a6` AS `zzz33`
WHERE (0 = 1)
17/12/20 20:25:10 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:10 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0c40b060a6`
17/12/20 20:25:10 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:25:10 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:10 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz34`
WHERE (0 = 1)
17/12/20 20:25:10 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:10 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.0028) AS `V1`, (`V2` < 0.0055) AS `V2`, (`V3` < 0.0138) AS `V3`, (`V4` < 0.0138) AS `V4`, (`V5` < 0.3) AS `V5`, (`V6` < 0.0045) AS `V6`, (`V7` < 0.1) AS `V7`, (`V8` < 0.0185) AS `V8`, (`V9` < 0.0028) AS `V9`, (`V10` < 0.026) AS `V10`
FROM `analyis_tbl`
17/12/20 20:25:10 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:25:10 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:10 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz35`
WHERE (0 = 1)
17/12/20 20:25:10 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:10 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 20:25:10 INFO CodeGenerator: Code generated in 14.447001 ms
17/12/20 20:25:10 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 20:25:10 INFO DAGScheduler: Got job 11 (collect at utils.scala:196) with 2 output partitions
17/12/20 20:25:10 INFO DAGScheduler: Final stage: ResultStage 25 (collect at utils.scala:196)
17/12/20 20:25:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)
17/12/20 20:25:10 INFO DAGScheduler: Missing parents: List()
17/12/20 20:25:10 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[80] at collect at utils.scala:196), which has no missing parents
17/12/20 20:25:10 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 127.1 KB, free 2003.7 MB)
17/12/20 20:25:10 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 47.4 KB, free 2003.6 MB)
17/12/20 20:25:10 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 127.0.0.1:61031 (size: 47.4 KB, free: 2004.5 MB)
17/12/20 20:25:10 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 25 (MapPartitionsRDD[80] at collect at utils.scala:196)
17/12/20 20:25:10 INFO TaskSchedulerImpl: Adding task set 25.0 with 2 tasks
17/12/20 20:25:10 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 5947 bytes)
17/12/20 20:25:10 INFO TaskSetManager: Starting task 1.0 in stage 25.0 (TID 23, localhost, executor driver, partition 1, PROCESS_LOCAL, 5947 bytes)
17/12/20 20:25:10 INFO Executor: Running task 0.0 in stage 25.0 (TID 22)
17/12/20 20:25:10 INFO Executor: Running task 1.0 in stage 25.0 (TID 23)
17/12/20 20:25:10 INFO BlockManager: Found block rdd_73_1 locally
17/12/20 20:25:10 INFO BlockManager: Found block rdd_73_0 locally
17/12/20 20:25:10 INFO Executor: Finished task 1.0 in stage 25.0 (TID 23). 1603 bytes result sent to driver
17/12/20 20:25:10 INFO Executor: Finished task 0.0 in stage 25.0 (TID 22). 1531 bytes result sent to driver
17/12/20 20:25:10 INFO TaskSetManager: Finished task 1.0 in stage 25.0 (TID 23) in 16 ms on localhost (executor driver) (1/2)
17/12/20 20:25:10 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 22) in 31 ms on localhost (executor driver) (2/2)
17/12/20 20:25:10 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
17/12/20 20:25:10 INFO DAGScheduler: ResultStage 25 (collect at utils.scala:196) finished in 0.031 s
17/12/20 20:25:10 INFO DAGScheduler: Job 11 finished: collect at utils.scala:196, took 0.045039 s
17/12/20 20:25:10 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:10 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:25:10 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:10 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:10 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:10 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:10 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:25:10 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:25:10 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:10 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:25:10 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:10 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:10 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:10 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:10 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:25:10 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:25:10 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:10 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:25:10 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:10 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:10 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:10 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:10 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:25:10 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:25:10 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:10 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:25:10 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:10 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:10 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:10 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:10 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:25:10 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:25:25 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:25 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:25:25 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:25 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:25 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:25 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:25 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:25:25 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:25:25 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 20:25:25 INFO DAGScheduler: Got job 12 (collect at utils.scala:58) with 1 output partitions
17/12/20 20:25:25 INFO DAGScheduler: Final stage: ResultStage 26 (collect at utils.scala:58)
17/12/20 20:25:25 INFO DAGScheduler: Parents of final stage: List()
17/12/20 20:25:25 INFO DAGScheduler: Missing parents: List()
17/12/20 20:25:25 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[90] at map at utils.scala:55), which has no missing parents
17/12/20 20:25:25 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 8.7 KB, free 2003.6 MB)
17/12/20 20:25:25 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2003.6 MB)
17/12/20 20:25:25 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 127.0.0.1:61031 (size: 4.6 KB, free: 2004.4 MB)
17/12/20 20:25:25 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[90] at map at utils.scala:55)
17/12/20 20:25:25 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
17/12/20 20:25:25 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 6505 bytes)
17/12/20 20:25:25 INFO Executor: Running task 0.0 in stage 26.0 (TID 24)
17/12/20 20:25:25 INFO Executor: Finished task 0.0 in stage 26.0 (TID 24). 1132 bytes result sent to driver
17/12/20 20:25:25 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 24) in 6 ms on localhost (executor driver) (1/1)
17/12/20 20:25:25 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
17/12/20 20:25:25 INFO DAGScheduler: ResultStage 26 (collect at utils.scala:58) finished in 0.006 s
17/12/20 20:25:25 INFO DAGScheduler: Job 12 finished: collect at utils.scala:58, took 0.013304 s
17/12/20 20:25:25 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:25 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:26 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:25:26 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:26 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 20:25:26 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 20:25:26 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 20:25:26 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 20:25:26 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 20:25:26 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 20:25:26 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 293.7 KB, free 2003.3 MB)
17/12/20 20:25:26 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2003.3 MB)
17/12/20 20:25:26 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 127.0.0.1:61031 (size: 24.0 KB, free: 2004.4 MB)
17/12/20 20:25:26 INFO SparkContext: Created broadcast 21 from sql at <unknown>:0
17/12/20 20:25:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 20:25:26 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 20:25:26 INFO DAGScheduler: Registering RDD 94 (sql at <unknown>:0)
17/12/20 20:25:26 INFO DAGScheduler: Registering RDD 99 (sql at <unknown>:0)
17/12/20 20:25:26 INFO DAGScheduler: Got job 13 (sql at <unknown>:0) with 1 output partitions
17/12/20 20:25:26 INFO DAGScheduler: Final stage: ResultStage 29 (sql at <unknown>:0)
17/12/20 20:25:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
17/12/20 20:25:26 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 28)
17/12/20 20:25:26 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[94] at sql at <unknown>:0), which has no missing parents
17/12/20 20:25:26 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 13.1 KB, free 2003.3 MB)
17/12/20 20:25:26 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2003.3 MB)
17/12/20 20:25:26 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 127.0.0.1:61031 (size: 7.5 KB, free: 2004.4 MB)
17/12/20 20:25:26 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[94] at sql at <unknown>:0)
17/12/20 20:25:26 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
17/12/20 20:25:26 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 6678 bytes)
17/12/20 20:25:26 INFO Executor: Running task 0.0 in stage 27.0 (TID 25)
17/12/20 20:25:26 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/RtmpusRKRl/spark_serialize_7cea19b31a903ddb1c25c7103da0066c49edc93ec6e05cfbc84d9424d5b7035f.csv, range: 0-1920, partition values: [empty row]
17/12/20 20:25:26 INFO Executor: Finished task 0.0 in stage 27.0 (TID 25). 1632 bytes result sent to driver
17/12/20 20:25:26 INFO DAGScheduler: ShuffleMapStage 27 (sql at <unknown>:0) finished in 0.031 s
17/12/20 20:25:26 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:25:26 INFO DAGScheduler: running: Set()
17/12/20 20:25:26 INFO DAGScheduler: waiting: Set(ShuffleMapStage 28, ResultStage 29)
17/12/20 20:25:26 INFO DAGScheduler: failed: Set()
17/12/20 20:25:26 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 25) in 31 ms on localhost (executor driver) (1/1)
17/12/20 20:25:26 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
17/12/20 20:25:26 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[99] at sql at <unknown>:0), which has no missing parents
17/12/20 20:25:26 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 15.3 KB, free 2003.3 MB)
17/12/20 20:25:26 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2003.3 MB)
17/12/20 20:25:26 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 127.0.0.1:61031 (size: 6.9 KB, free: 2004.4 MB)
17/12/20 20:25:26 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:26 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[99] at sql at <unknown>:0)
17/12/20 20:25:26 INFO TaskSchedulerImpl: Adding task set 28.0 with 2 tasks
17/12/20 20:25:26 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 26, localhost, executor driver, partition 0, ANY, 5944 bytes)
17/12/20 20:25:26 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 27, localhost, executor driver, partition 1, ANY, 5944 bytes)
17/12/20 20:25:26 INFO Executor: Running task 0.0 in stage 28.0 (TID 26)
17/12/20 20:25:26 INFO Executor: Running task 1.0 in stage 28.0 (TID 27)
17/12/20 20:25:26 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 20:25:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/12/20 20:25:26 INFO MemoryStore: Block rdd_96_0 stored as values in memory (estimated size 1512.0 B, free 2003.3 MB)
17/12/20 20:25:26 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 20:25:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:25:26 INFO BlockManagerInfo: Added rdd_96_0 in memory on 127.0.0.1:61031 (size: 1512.0 B, free: 2004.4 MB)
17/12/20 20:25:26 INFO MemoryStore: Block rdd_96_1 stored as values in memory (estimated size 1512.0 B, free 2003.3 MB)
17/12/20 20:25:26 INFO BlockManagerInfo: Added rdd_96_1 in memory on 127.0.0.1:61031 (size: 1512.0 B, free: 2004.4 MB)
17/12/20 20:25:26 INFO Executor: Finished task 0.0 in stage 28.0 (TID 26). 3072 bytes result sent to driver
17/12/20 20:25:26 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 26) in 19 ms on localhost (executor driver) (1/2)
17/12/20 20:25:26 INFO Executor: Finished task 1.0 in stage 28.0 (TID 27). 3151 bytes result sent to driver
17/12/20 20:25:26 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 27) in 18 ms on localhost (executor driver) (2/2)
17/12/20 20:25:26 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
17/12/20 20:25:26 INFO DAGScheduler: ShuffleMapStage 28 (sql at <unknown>:0) finished in 0.019 s
17/12/20 20:25:26 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:25:26 INFO DAGScheduler: running: Set()
17/12/20 20:25:26 INFO DAGScheduler: waiting: Set(ResultStage 29)
17/12/20 20:25:26 INFO DAGScheduler: failed: Set()
17/12/20 20:25:26 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[102] at sql at <unknown>:0), which has no missing parents
17/12/20 20:25:26 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 7.0 KB, free 2003.3 MB)
17/12/20 20:25:26 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.3 MB)
17/12/20 20:25:26 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 127.0.0.1:61031 (size: 3.7 KB, free: 2004.4 MB)
17/12/20 20:25:26 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[102] at sql at <unknown>:0)
17/12/20 20:25:26 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
17/12/20 20:25:26 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 28, localhost, executor driver, partition 0, ANY, 5955 bytes)
17/12/20 20:25:26 INFO Executor: Running task 0.0 in stage 29.0 (TID 28)
17/12/20 20:25:26 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 20:25:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:25:26 INFO Executor: Finished task 0.0 in stage 29.0 (TID 28). 1707 bytes result sent to driver
17/12/20 20:25:26 INFO DAGScheduler: ResultStage 29 (sql at <unknown>:0) finished in 0.000 s
17/12/20 20:25:26 INFO DAGScheduler: Job 13 finished: sql at <unknown>:0, took 0.080531 s
17/12/20 20:25:26 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 28) in 0 ms on localhost (executor driver) (1/1)
17/12/20 20:25:26 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
17/12/20 20:25:26 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:26 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 20:25:26 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 20:25:26 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 145 bytes
17/12/20 20:25:26 INFO DAGScheduler: Registering RDD 106 (collect at utils.scala:196)
17/12/20 20:25:26 INFO DAGScheduler: Got job 14 (collect at utils.scala:196) with 1 output partitions
17/12/20 20:25:26 INFO DAGScheduler: Final stage: ResultStage 32 (collect at utils.scala:196)
17/12/20 20:25:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
17/12/20 20:25:26 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 31)
17/12/20 20:25:26 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[106] at collect at utils.scala:196), which has no missing parents
17/12/20 20:25:26 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.3 KB, free 2003.3 MB)
17/12/20 20:25:26 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2003.2 MB)
17/12/20 20:25:26 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 127.0.0.1:61031 (size: 6.9 KB, free: 2004.4 MB)
17/12/20 20:25:26 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:26 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[106] at collect at utils.scala:196)
17/12/20 20:25:26 INFO TaskSchedulerImpl: Adding task set 31.0 with 2 tasks
17/12/20 20:25:26 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 29, localhost, executor driver, partition 0, PROCESS_LOCAL, 5936 bytes)
17/12/20 20:25:26 INFO TaskSetManager: Starting task 1.0 in stage 31.0 (TID 30, localhost, executor driver, partition 1, PROCESS_LOCAL, 5936 bytes)
17/12/20 20:25:26 INFO Executor: Running task 0.0 in stage 31.0 (TID 29)
17/12/20 20:25:26 INFO Executor: Running task 1.0 in stage 31.0 (TID 30)
17/12/20 20:25:26 INFO BlockManager: Found block rdd_96_0 locally
17/12/20 20:25:26 INFO BlockManager: Found block rdd_96_1 locally
17/12/20 20:25:26 INFO Executor: Finished task 0.0 in stage 31.0 (TID 29). 1950 bytes result sent to driver
17/12/20 20:25:26 INFO Executor: Finished task 1.0 in stage 31.0 (TID 30). 1950 bytes result sent to driver
17/12/20 20:25:26 INFO TaskSetManager: Finished task 1.0 in stage 31.0 (TID 30) in 26 ms on localhost (executor driver) (1/2)
17/12/20 20:25:26 INFO DAGScheduler: ShuffleMapStage 31 (collect at utils.scala:196) finished in 0.027 s
17/12/20 20:25:26 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:25:26 INFO DAGScheduler: running: Set()
17/12/20 20:25:26 INFO DAGScheduler: waiting: Set(ResultStage 32)
17/12/20 20:25:26 INFO DAGScheduler: failed: Set()
17/12/20 20:25:26 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[109] at collect at utils.scala:196), which has no missing parents
17/12/20 20:25:26 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 7.0 KB, free 2003.2 MB)
17/12/20 20:25:26 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.2 MB)
17/12/20 20:25:26 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 29) in 27 ms on localhost (executor driver) (2/2)
17/12/20 20:25:26 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
17/12/20 20:25:26 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 127.0.0.1:61031 (size: 3.7 KB, free: 2004.4 MB)
17/12/20 20:25:26 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[109] at collect at utils.scala:196)
17/12/20 20:25:26 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
17/12/20 20:25:26 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 31, localhost, executor driver, partition 0, ANY, 5947 bytes)
17/12/20 20:25:26 INFO Executor: Running task 0.0 in stage 32.0 (TID 31)
17/12/20 20:25:26 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 20:25:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:25:26 INFO Executor: Finished task 0.0 in stage 32.0 (TID 31). 1952 bytes result sent to driver
17/12/20 20:25:26 INFO DAGScheduler: ResultStage 32 (collect at utils.scala:196) finished in 0.006 s
17/12/20 20:25:26 INFO DAGScheduler: Job 14 finished: collect at utils.scala:196, took 0.033915 s
17/12/20 20:25:26 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 31) in 6 ms on localhost (executor driver) (1/1)
17/12/20 20:25:26 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
17/12/20 20:25:26 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:26 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz36`
WHERE (0 = 1)
17/12/20 20:25:26 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:26 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S6` + 0.89442719 * RANDN() AS `V1`, `S1` + 0.9486833 * RANDN() AS `V2`, `S6` + 0.89442719 * RANDN() AS `V3`, `S6` + 0.89442719 * RANDN() AS `V4`, `S9` + 0.94339811 * RANDN() AS `V5`, `S2` + 0.93273791 * RANDN() AS `V6`, `S10` + 0.91651514 * RANDN() AS `V7`, `S5` + 0.93273791 * RANDN() AS `V8`, `S5` + 0.93273791 * RANDN() AS `V9`, `S9` + 0.94339811 * RANDN() AS `V10`
FROM `analyis_tbl`) `joybehbaqu`
17/12/20 20:25:26 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:25:26 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:26 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz37`
WHERE (0 = 1)
17/12/20 20:25:26 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:26 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 20:25:26 INFO CodeGenerator: Code generated in 13.120559 ms
17/12/20 20:25:26 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 20:25:26 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 145 bytes
17/12/20 20:25:26 INFO DAGScheduler: Got job 15 (take at <unknown>:0) with 1 output partitions
17/12/20 20:25:26 INFO DAGScheduler: Final stage: ResultStage 34 (take at <unknown>:0)
17/12/20 20:25:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)
17/12/20 20:25:26 INFO DAGScheduler: Missing parents: List()
17/12/20 20:25:26 INFO DAGScheduler: Submitting ResultStage 34 (WorkerRDD[115] at RDD at rdd.scala:18), which has no missing parents
17/12/20 20:25:26 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 114.3 KB, free 2003.1 MB)
17/12/20 20:25:26 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 43.1 KB, free 2003.1 MB)
17/12/20 20:25:26 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 127.0.0.1:61031 (size: 43.1 KB, free: 2004.4 MB)
17/12/20 20:25:26 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (WorkerRDD[115] at RDD at rdd.scala:18)
17/12/20 20:25:26 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks
17/12/20 20:25:26 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 32, localhost, executor driver, partition 0, PROCESS_LOCAL, 5914 bytes)
17/12/20 20:25:26 INFO Executor: Running task 0.0 in stage 34.0 (TID 32)
17/12/20 20:25:26 INFO BlockManager: Found block rdd_96_0 locally
17/12/20 20:25:27 INFO MemoryStore: Block rdd_115_0 stored as values in memory (estimated size 1600.0 B, free 2003.1 MB)
17/12/20 20:25:27 INFO BlockManagerInfo: Added rdd_115_0 in memory on 127.0.0.1:61031 (size: 1600.0 B, free: 2004.4 MB)
17/12/20 20:25:27 INFO Executor: Finished task 0.0 in stage 34.0 (TID 32). 3082 bytes result sent to driver
17/12/20 20:25:27 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 32) in 696 ms on localhost (executor driver) (1/1)
17/12/20 20:25:27 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
17/12/20 20:25:27 INFO DAGScheduler: ResultStage 34 (take at <unknown>:0) finished in 0.696 s
17/12/20 20:25:27 INFO DAGScheduler: Job 15 finished: take at <unknown>:0, took 0.716780 s
17/12/20 20:25:27 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 20:25:27 INFO DAGScheduler: Got job 16 (take at <unknown>:0) with 1 output partitions
17/12/20 20:25:27 INFO DAGScheduler: Final stage: ResultStage 36 (take at <unknown>:0)
17/12/20 20:25:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)
17/12/20 20:25:27 INFO DAGScheduler: Missing parents: List()
17/12/20 20:25:27 INFO DAGScheduler: Submitting ResultStage 36 (WorkerRDD[115] at RDD at rdd.scala:18), which has no missing parents
17/12/20 20:25:27 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 114.3 KB, free 2003.0 MB)
17/12/20 20:25:27 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 43.1 KB, free 2002.9 MB)
17/12/20 20:25:27 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 127.0.0.1:61031 (size: 43.1 KB, free: 2004.3 MB)
17/12/20 20:25:27 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (WorkerRDD[115] at RDD at rdd.scala:18)
17/12/20 20:25:27 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks
17/12/20 20:25:27 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 33, localhost, executor driver, partition 1, PROCESS_LOCAL, 5914 bytes)
17/12/20 20:25:27 INFO Executor: Running task 0.0 in stage 36.0 (TID 33)
17/12/20 20:25:27 INFO BlockManager: Found block rdd_96_1 locally
17/12/20 20:25:27 INFO MemoryStore: Block rdd_115_1 stored as values in memory (estimated size 1600.0 B, free 2002.9 MB)
17/12/20 20:25:27 INFO BlockManagerInfo: Added rdd_115_1 in memory on 127.0.0.1:61031 (size: 1600.0 B, free: 2004.3 MB)
17/12/20 20:25:27 WARN Executor: 1 block locks were not released by TID = 33:
[rdd_115_1]
17/12/20 20:25:27 INFO Executor: Finished task 0.0 in stage 36.0 (TID 33). 3003 bytes result sent to driver
17/12/20 20:25:27 INFO DAGScheduler: ResultStage 36 (take at <unknown>:0) finished in 0.752 s
17/12/20 20:25:27 INFO DAGScheduler: Job 16 finished: take at <unknown>:0, took 0.750578 s
17/12/20 20:25:27 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 33) in 752 ms on localhost (executor driver) (1/1)
17/12/20 20:25:27 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
17/12/20 20:25:27 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:27 INFO SparkSqlParser: Parsing command: sparklyr_tmp_1c0c6f373773
17/12/20 20:25:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:28 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0c6f373773` AS `zzz38`
WHERE (0 = 1)
17/12/20 20:25:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:28 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0c6f373773`
17/12/20 20:25:28 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:25:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:28 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz39`
WHERE (0 = 1)
17/12/20 20:25:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:28 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.054) AS `V1`, (`V2` < 0.1) AS `V2`, (`V3` < 0.0055) AS `V3`, (`V4` < 0.0028) AS `V4`, (`V5` < 0.009) AS `V5`, (`V6` < 0.15) AS `V6`, (`V7` < 0.009) AS `V7`, (`V8` < 0.0028) AS `V8`, (`V9` < 0.0185) AS `V9`, (`V10` < 0.0138) AS `V10`
FROM `analyis_tbl`
17/12/20 20:25:28 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:25:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:28 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz40`
WHERE (0 = 1)
17/12/20 20:25:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:28 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 20:25:28 INFO CodeGenerator: Code generated in 14.19742 ms
17/12/20 20:25:28 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 20:25:28 INFO DAGScheduler: Got job 17 (collect at utils.scala:196) with 2 output partitions
17/12/20 20:25:28 INFO DAGScheduler: Final stage: ResultStage 38 (collect at utils.scala:196)
17/12/20 20:25:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
17/12/20 20:25:28 INFO DAGScheduler: Missing parents: List()
17/12/20 20:25:28 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[122] at collect at utils.scala:196), which has no missing parents
17/12/20 20:25:28 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 127.1 KB, free 2002.8 MB)
17/12/20 20:25:28 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 47.4 KB, free 2002.8 MB)
17/12/20 20:25:28 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 127.0.0.1:61031 (size: 47.4 KB, free: 2004.3 MB)
17/12/20 20:25:28 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:996
17/12/20 20:25:28 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 38 (MapPartitionsRDD[122] at collect at utils.scala:196)
17/12/20 20:25:28 INFO TaskSchedulerImpl: Adding task set 38.0 with 2 tasks
17/12/20 20:25:28 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 5947 bytes)
17/12/20 20:25:28 INFO TaskSetManager: Starting task 1.0 in stage 38.0 (TID 35, localhost, executor driver, partition 1, PROCESS_LOCAL, 5947 bytes)
17/12/20 20:25:28 INFO Executor: Running task 0.0 in stage 38.0 (TID 34)
17/12/20 20:25:28 INFO Executor: Running task 1.0 in stage 38.0 (TID 35)
17/12/20 20:25:28 INFO BlockManager: Found block rdd_115_0 locally
17/12/20 20:25:28 INFO Executor: Finished task 0.0 in stage 38.0 (TID 34). 1633 bytes result sent to driver
17/12/20 20:25:28 INFO BlockManager: Found block rdd_115_1 locally
17/12/20 20:25:28 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 34) in 16 ms on localhost (executor driver) (1/2)
17/12/20 20:25:28 INFO Executor: Finished task 1.0 in stage 38.0 (TID 35). 1699 bytes result sent to driver
17/12/20 20:25:28 INFO TaskSetManager: Finished task 1.0 in stage 38.0 (TID 35) in 35 ms on localhost (executor driver) (2/2)
17/12/20 20:25:28 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
17/12/20 20:25:28 INFO DAGScheduler: ResultStage 38 (collect at utils.scala:196) finished in 0.035 s
17/12/20 20:25:28 INFO DAGScheduler: Job 17 finished: collect at utils.scala:196, took 0.029542 s
17/12/20 20:25:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:28 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:25:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:28 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:25:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:25:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:28 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:25:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:28 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:25:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:25:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:28 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:25:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:28 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:25:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:25:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:25:28 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:25:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:25:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:25:28 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:25:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:26:47 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:47 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:26:47 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:26:47 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:26:47 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:26:47 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:26:47 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:26:47 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:26:47 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 20:26:47 INFO DAGScheduler: Got job 18 (collect at utils.scala:58) with 1 output partitions
17/12/20 20:26:47 INFO DAGScheduler: Final stage: ResultStage 39 (collect at utils.scala:58)
17/12/20 20:26:47 INFO DAGScheduler: Parents of final stage: List()
17/12/20 20:26:47 INFO DAGScheduler: Missing parents: List()
17/12/20 20:26:47 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[132] at map at utils.scala:55), which has no missing parents
17/12/20 20:26:47 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 8.7 KB, free 2002.7 MB)
17/12/20 20:26:47 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2002.7 MB)
17/12/20 20:26:47 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 127.0.0.1:61031 (size: 4.6 KB, free: 2004.3 MB)
17/12/20 20:26:47 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:996
17/12/20 20:26:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[132] at map at utils.scala:55)
17/12/20 20:26:47 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks
17/12/20 20:26:47 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 6577 bytes)
17/12/20 20:26:47 INFO Executor: Running task 0.0 in stage 39.0 (TID 36)
17/12/20 20:26:47 INFO Executor: Finished task 0.0 in stage 39.0 (TID 36). 1081 bytes result sent to driver
17/12/20 20:26:47 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 36) in 4 ms on localhost (executor driver) (1/1)
17/12/20 20:26:47 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
17/12/20 20:26:47 INFO DAGScheduler: ResultStage 39 (collect at utils.scala:58) finished in 0.004 s
17/12/20 20:26:47 INFO DAGScheduler: Job 18 finished: collect at utils.scala:58, took 0.008497 s
17/12/20 20:26:47 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:48 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:48 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:26:48 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:48 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 20:26:48 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 20:26:48 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 20:26:48 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 20:26:48 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 20:26:48 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 20:26:48 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 293.7 KB, free 2002.5 MB)
17/12/20 20:26:48 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2002.4 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 127.0.0.1:61031 (size: 24.0 KB, free: 2004.2 MB)
17/12/20 20:26:48 INFO SparkContext: Created broadcast 31 from sql at <unknown>:0
17/12/20 20:26:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 20:26:48 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 20:26:48 INFO DAGScheduler: Registering RDD 136 (sql at <unknown>:0)
17/12/20 20:26:48 INFO DAGScheduler: Registering RDD 141 (sql at <unknown>:0)
17/12/20 20:26:48 INFO DAGScheduler: Got job 19 (sql at <unknown>:0) with 1 output partitions
17/12/20 20:26:48 INFO DAGScheduler: Final stage: ResultStage 42 (sql at <unknown>:0)
17/12/20 20:26:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)
17/12/20 20:26:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 41)
17/12/20 20:26:48 INFO DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[136] at sql at <unknown>:0), which has no missing parents
17/12/20 20:26:48 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 13.1 KB, free 2002.4 MB)
17/12/20 20:26:48 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2002.4 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 127.0.0.1:61031 (size: 7.5 KB, free: 2004.2 MB)
17/12/20 20:26:48 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:996
17/12/20 20:26:48 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[136] at sql at <unknown>:0)
17/12/20 20:26:48 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks
17/12/20 20:26:48 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 37, localhost, executor driver, partition 0, PROCESS_LOCAL, 6678 bytes)
17/12/20 20:26:48 INFO Executor: Running task 0.0 in stage 40.0 (TID 37)
17/12/20 20:26:48 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/RtmpusRKRl/spark_serialize_55867f301bc95b7ad9fa97ae4542d314c226926bc22031778975f06e8dccb10c.csv, range: 0-1873132, partition values: [empty row]
17/12/20 20:26:48 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 127.0.0.1:61031 in memory (size: 47.4 KB, free: 2004.3 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 127.0.0.1:61031 in memory (size: 43.1 KB, free: 2004.3 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 127.0.0.1:61031 in memory (size: 47.4 KB, free: 2004.4 MB)
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1086
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1087
17/12/20 20:26:48 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 127.0.0.1:61031 in memory (size: 4.6 KB, free: 2004.4 MB)
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1136
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1137
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1143
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1144
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1145
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1146
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1147
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1148
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1149
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1150
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1151
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1152
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1153
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1154
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1155
17/12/20 20:26:48 INFO ContextCleaner: Cleaned shuffle 7
17/12/20 20:26:48 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 127.0.0.1:61031 in memory (size: 7.5 KB, free: 2004.4 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 127.0.0.1:61031 in memory (size: 6.9 KB, free: 2004.4 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 127.0.0.1:61031 in memory (size: 3.7 KB, free: 2004.4 MB)
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1324
17/12/20 20:26:48 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 127.0.0.1:61031 in memory (size: 6.9 KB, free: 2004.4 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 127.0.0.1:61031 in memory (size: 3.7 KB, free: 2004.4 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 127.0.0.1:61031 in memory (size: 43.1 KB, free: 2004.4 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 127.0.0.1:61031 in memory (size: 43.1 KB, free: 2004.5 MB)
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1629
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1630
17/12/20 20:26:48 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 127.0.0.1:61031 in memory (size: 4.6 KB, free: 2004.5 MB)
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1679
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1680
17/12/20 20:26:48 INFO ContextCleaner: Cleaned accumulator 1686
17/12/20 20:26:48 INFO Executor: Finished task 0.0 in stage 40.0 (TID 37). 1882 bytes result sent to driver
17/12/20 20:26:48 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 37) in 187 ms on localhost (executor driver) (1/1)
17/12/20 20:26:48 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
17/12/20 20:26:48 INFO DAGScheduler: ShuffleMapStage 40 (sql at <unknown>:0) finished in 0.187 s
17/12/20 20:26:48 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:26:48 INFO DAGScheduler: running: Set()
17/12/20 20:26:48 INFO DAGScheduler: waiting: Set(ResultStage 42, ShuffleMapStage 41)
17/12/20 20:26:48 INFO DAGScheduler: failed: Set()
17/12/20 20:26:48 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[141] at sql at <unknown>:0), which has no missing parents
17/12/20 20:26:48 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 15.3 KB, free 2003.3 MB)
17/12/20 20:26:48 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 7.0 KB, free 2003.3 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 127.0.0.1:61031 (size: 7.0 KB, free: 2004.5 MB)
17/12/20 20:26:48 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:996
17/12/20 20:26:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[141] at sql at <unknown>:0)
17/12/20 20:26:48 INFO TaskSchedulerImpl: Adding task set 41.0 with 2 tasks
17/12/20 20:26:48 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 38, localhost, executor driver, partition 0, ANY, 5944 bytes)
17/12/20 20:26:48 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 39, localhost, executor driver, partition 1, ANY, 5944 bytes)
17/12/20 20:26:48 INFO Executor: Running task 0.0 in stage 41.0 (TID 38)
17/12/20 20:26:48 INFO Executor: Running task 1.0 in stage 41.0 (TID 39)
17/12/20 20:26:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 20:26:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:26:48 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 20:26:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
17/12/20 20:26:48 INFO MemoryStore: Block rdd_138_1 stored as values in memory (estimated size 392.1 KB, free 2002.9 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Added rdd_138_1 in memory on 127.0.0.1:61031 (size: 392.1 KB, free: 2004.1 MB)
17/12/20 20:26:48 INFO MemoryStore: Block rdd_138_0 stored as values in memory (estimated size 392.1 KB, free 2002.5 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Added rdd_138_0 in memory on 127.0.0.1:61031 (size: 392.1 KB, free: 2003.7 MB)
17/12/20 20:26:48 INFO Executor: Finished task 0.0 in stage 41.0 (TID 38). 3064 bytes result sent to driver
17/12/20 20:26:48 INFO Executor: Finished task 1.0 in stage 41.0 (TID 39). 3064 bytes result sent to driver
17/12/20 20:26:48 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 38) in 77 ms on localhost (executor driver) (1/2)
17/12/20 20:26:48 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 39) in 78 ms on localhost (executor driver) (2/2)
17/12/20 20:26:48 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
17/12/20 20:26:48 INFO DAGScheduler: ShuffleMapStage 41 (sql at <unknown>:0) finished in 0.078 s
17/12/20 20:26:48 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:26:48 INFO DAGScheduler: running: Set()
17/12/20 20:26:48 INFO DAGScheduler: waiting: Set(ResultStage 42)
17/12/20 20:26:48 INFO DAGScheduler: failed: Set()
17/12/20 20:26:48 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[144] at sql at <unknown>:0), which has no missing parents
17/12/20 20:26:48 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 7.0 KB, free 2002.5 MB)
17/12/20 20:26:48 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2002.5 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 127.0.0.1:61031 (size: 3.7 KB, free: 2003.7 MB)
17/12/20 20:26:48 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:996
17/12/20 20:26:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[144] at sql at <unknown>:0)
17/12/20 20:26:48 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
17/12/20 20:26:48 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 40, localhost, executor driver, partition 0, ANY, 5955 bytes)
17/12/20 20:26:48 INFO Executor: Running task 0.0 in stage 42.0 (TID 40)
17/12/20 20:26:48 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 20:26:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:26:48 INFO Executor: Finished task 0.0 in stage 42.0 (TID 40). 1952 bytes result sent to driver
17/12/20 20:26:48 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 40) in 3 ms on localhost (executor driver) (1/1)
17/12/20 20:26:48 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
17/12/20 20:26:48 INFO DAGScheduler: ResultStage 42 (sql at <unknown>:0) finished in 0.004 s
17/12/20 20:26:48 INFO DAGScheduler: Job 19 finished: sql at <unknown>:0, took 0.263272 s
17/12/20 20:26:48 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:48 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 20:26:48 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 20:26:48 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 145 bytes
17/12/20 20:26:48 INFO DAGScheduler: Registering RDD 148 (collect at utils.scala:196)
17/12/20 20:26:48 INFO DAGScheduler: Got job 20 (collect at utils.scala:196) with 1 output partitions
17/12/20 20:26:48 INFO DAGScheduler: Final stage: ResultStage 45 (collect at utils.scala:196)
17/12/20 20:26:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)
17/12/20 20:26:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 44)
17/12/20 20:26:48 INFO DAGScheduler: Submitting ShuffleMapStage 44 (MapPartitionsRDD[148] at collect at utils.scala:196), which has no missing parents
17/12/20 20:26:48 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 15.3 KB, free 2002.5 MB)
17/12/20 20:26:48 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 7.0 KB, free 2002.5 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 127.0.0.1:61031 (size: 7.0 KB, free: 2003.7 MB)
17/12/20 20:26:48 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:996
17/12/20 20:26:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 44 (MapPartitionsRDD[148] at collect at utils.scala:196)
17/12/20 20:26:48 INFO TaskSchedulerImpl: Adding task set 44.0 with 2 tasks
17/12/20 20:26:48 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 41, localhost, executor driver, partition 0, PROCESS_LOCAL, 5936 bytes)
17/12/20 20:26:48 INFO TaskSetManager: Starting task 1.0 in stage 44.0 (TID 42, localhost, executor driver, partition 1, PROCESS_LOCAL, 5936 bytes)
17/12/20 20:26:48 INFO Executor: Running task 1.0 in stage 44.0 (TID 42)
17/12/20 20:26:48 INFO Executor: Running task 0.0 in stage 44.0 (TID 41)
17/12/20 20:26:48 INFO BlockManager: Found block rdd_138_1 locally
17/12/20 20:26:48 INFO BlockManager: Found block rdd_138_0 locally
17/12/20 20:26:48 INFO Executor: Finished task 0.0 in stage 44.0 (TID 41). 2037 bytes result sent to driver
17/12/20 20:26:48 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 41) in 15 ms on localhost (executor driver) (1/2)
17/12/20 20:26:48 INFO Executor: Finished task 1.0 in stage 44.0 (TID 42). 1958 bytes result sent to driver
17/12/20 20:26:48 INFO TaskSetManager: Finished task 1.0 in stage 44.0 (TID 42) in 16 ms on localhost (executor driver) (2/2)
17/12/20 20:26:48 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
17/12/20 20:26:48 INFO DAGScheduler: ShuffleMapStage 44 (collect at utils.scala:196) finished in 0.017 s
17/12/20 20:26:48 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:26:48 INFO DAGScheduler: running: Set()
17/12/20 20:26:48 INFO DAGScheduler: waiting: Set(ResultStage 45)
17/12/20 20:26:48 INFO DAGScheduler: failed: Set()
17/12/20 20:26:48 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[151] at collect at utils.scala:196), which has no missing parents
17/12/20 20:26:48 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 7.0 KB, free 2002.5 MB)
17/12/20 20:26:48 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2002.5 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 127.0.0.1:61031 (size: 3.7 KB, free: 2003.7 MB)
17/12/20 20:26:48 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:996
17/12/20 20:26:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[151] at collect at utils.scala:196)
17/12/20 20:26:48 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks
17/12/20 20:26:48 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 43, localhost, executor driver, partition 0, ANY, 5947 bytes)
17/12/20 20:26:48 INFO Executor: Running task 0.0 in stage 45.0 (TID 43)
17/12/20 20:26:48 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 20:26:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:26:48 INFO Executor: Finished task 0.0 in stage 45.0 (TID 43). 1963 bytes result sent to driver
17/12/20 20:26:48 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 43) in 5 ms on localhost (executor driver) (1/1)
17/12/20 20:26:48 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
17/12/20 20:26:48 INFO DAGScheduler: ResultStage 45 (collect at utils.scala:196) finished in 0.006 s
17/12/20 20:26:48 INFO DAGScheduler: Job 20 finished: collect at utils.scala:196, took 0.032787 s
17/12/20 20:26:48 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:48 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz41`
WHERE (0 = 1)
17/12/20 20:26:48 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:48 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S6` + 0.89442719 * RANDN() AS `V1`, `S1` + 0.9486833 * RANDN() AS `V2`, `S6` + 0.89442719 * RANDN() AS `V3`, `S6` + 0.89442719 * RANDN() AS `V4`, `S9` + 0.94339811 * RANDN() AS `V5`, `S2` + 0.93273791 * RANDN() AS `V6`, `S10` + 0.91651514 * RANDN() AS `V7`, `S5` + 0.93273791 * RANDN() AS `V8`, `S5` + 0.93273791 * RANDN() AS `V9`, `S9` + 0.94339811 * RANDN() AS `V10`
FROM `analyis_tbl`) `lohaogbolg`
17/12/20 20:26:48 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:26:48 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:48 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz42`
WHERE (0 = 1)
17/12/20 20:26:48 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:48 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 20:26:48 INFO CodeGenerator: Code generated in 13.770376 ms
17/12/20 20:26:48 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 20:26:48 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 145 bytes
17/12/20 20:26:48 INFO DAGScheduler: Got job 21 (take at <unknown>:0) with 1 output partitions
17/12/20 20:26:48 INFO DAGScheduler: Final stage: ResultStage 47 (take at <unknown>:0)
17/12/20 20:26:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 46)
17/12/20 20:26:48 INFO DAGScheduler: Missing parents: List()
17/12/20 20:26:48 INFO DAGScheduler: Submitting ResultStage 47 (WorkerRDD[157] at RDD at rdd.scala:18), which has no missing parents
17/12/20 20:26:48 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 894.8 KB, free 2001.6 MB)
17/12/20 20:26:48 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 825.5 KB, free 2000.8 MB)
17/12/20 20:26:48 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 127.0.0.1:61031 (size: 825.5 KB, free: 2002.9 MB)
17/12/20 20:26:48 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:996
17/12/20 20:26:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (WorkerRDD[157] at RDD at rdd.scala:18)
17/12/20 20:26:48 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks
17/12/20 20:26:48 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 44, localhost, executor driver, partition 0, PROCESS_LOCAL, 5914 bytes)
17/12/20 20:26:48 INFO Executor: Running task 0.0 in stage 47.0 (TID 44)
17/12/20 20:26:48 INFO BlockManager: Found block rdd_138_0 locally
17/12/20 20:26:51 INFO MemoryStore: Block rdd_157_0 stored as values in memory (estimated size 1543.0 KB, free 1999.3 MB)
17/12/20 20:26:51 INFO BlockManagerInfo: Added rdd_157_0 in memory on 127.0.0.1:61031 (size: 1543.0 KB, free: 2001.4 MB)
17/12/20 20:26:51 WARN Executor: 1 block locks were not released by TID = 44:
[rdd_157_0]
17/12/20 20:26:51 INFO Executor: Finished task 0.0 in stage 47.0 (TID 44). 3952 bytes result sent to driver
17/12/20 20:26:51 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 44) in 3025 ms on localhost (executor driver) (1/1)
17/12/20 20:26:51 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
17/12/20 20:26:51 INFO DAGScheduler: ResultStage 47 (take at <unknown>:0) finished in 3.025 s
17/12/20 20:26:51 INFO DAGScheduler: Job 21 finished: take at <unknown>:0, took 3.047612 s
17/12/20 20:26:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:51 INFO SparkSqlParser: Parsing command: sparklyr_tmp_1c0c332611b1
17/12/20 20:26:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:51 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0c332611b1` AS `zzz43`
WHERE (0 = 1)
17/12/20 20:26:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:51 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1c0c332611b1`
17/12/20 20:26:51 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:26:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:51 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz44`
WHERE (0 = 1)
17/12/20 20:26:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:51 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.054) AS `V1`, (`V2` < 0.1) AS `V2`, (`V3` < 0.0055) AS `V3`, (`V4` < 0.0028) AS `V4`, (`V5` < 0.009) AS `V5`, (`V6` < 0.15) AS `V6`, (`V7` < 0.009) AS `V7`, (`V8` < 0.0028) AS `V8`, (`V9` < 0.0185) AS `V9`, (`V10` < 0.0138) AS `V10`
FROM `analyis_tbl`
17/12/20 20:26:51 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:26:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:51 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz45`
WHERE (0 = 1)
17/12/20 20:26:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:51 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 20:26:52 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 20:26:52 INFO DAGScheduler: Got job 22 (collect at utils.scala:196) with 2 output partitions
17/12/20 20:26:52 INFO DAGScheduler: Final stage: ResultStage 49 (collect at utils.scala:196)
17/12/20 20:26:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)
17/12/20 20:26:52 INFO DAGScheduler: Missing parents: List()
17/12/20 20:26:52 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[164] at collect at utils.scala:196), which has no missing parents
17/12/20 20:26:52 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 907.6 KB, free 1998.4 MB)
17/12/20 20:26:52 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 830.1 KB, free 1997.6 MB)
17/12/20 20:26:52 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 127.0.0.1:61031 (size: 830.1 KB, free: 2000.6 MB)
17/12/20 20:26:52 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:996
17/12/20 20:26:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 49 (MapPartitionsRDD[164] at collect at utils.scala:196)
17/12/20 20:26:52 INFO TaskSchedulerImpl: Adding task set 49.0 with 2 tasks
17/12/20 20:26:52 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 45, localhost, executor driver, partition 0, PROCESS_LOCAL, 5947 bytes)
17/12/20 20:26:52 INFO TaskSetManager: Starting task 1.0 in stage 49.0 (TID 46, localhost, executor driver, partition 1, PROCESS_LOCAL, 5947 bytes)
17/12/20 20:26:52 INFO Executor: Running task 0.0 in stage 49.0 (TID 45)
17/12/20 20:26:52 INFO Executor: Running task 1.0 in stage 49.0 (TID 46)
17/12/20 20:26:52 INFO BlockManager: Found block rdd_157_0 locally
17/12/20 20:26:52 INFO BlockManager: Found block rdd_138_1 locally
17/12/20 20:26:52 INFO Executor: Finished task 0.0 in stage 49.0 (TID 45). 23360 bytes result sent to driver
17/12/20 20:26:52 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 45) in 75 ms on localhost (executor driver) (1/2)
17/12/20 20:26:55 INFO MemoryStore: Block rdd_157_1 stored as values in memory (estimated size 1543.0 KB, free 1996.1 MB)
17/12/20 20:26:55 INFO BlockManagerInfo: Added rdd_157_1 in memory on 127.0.0.1:61031 (size: 1543.0 KB, free: 1999.1 MB)
17/12/20 20:26:55 INFO Executor: Finished task 1.0 in stage 49.0 (TID 46). 23484 bytes result sent to driver
17/12/20 20:26:55 INFO TaskSetManager: Finished task 1.0 in stage 49.0 (TID 46) in 3032 ms on localhost (executor driver) (2/2)
17/12/20 20:26:55 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool 
17/12/20 20:26:55 INFO DAGScheduler: ResultStage 49 (collect at utils.scala:196) finished in 3.032 s
17/12/20 20:26:55 INFO DAGScheduler: Job 22 finished: collect at utils.scala:196, took 3.046296 s
17/12/20 20:26:55 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:55 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:26:55 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:26:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:26:55 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:26:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:26:55 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:26:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:26:55 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:55 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:26:55 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:26:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:26:55 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:26:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:26:55 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:26:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:26:55 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:55 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:26:55 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:26:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:26:55 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:26:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:26:55 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:26:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:26:55 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:26:55 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:26:55 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:26:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:26:55 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:26:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:26:55 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:26:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:29:50 INFO SparkContext: Invoking stop() from shutdown hook
17/12/20 20:29:50 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/12/20 20:29:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/12/20 20:29:50 INFO MemoryStore: MemoryStore cleared
17/12/20 20:29:50 INFO BlockManager: BlockManager stopped
17/12/20 20:29:50 INFO BlockManagerMaster: BlockManagerMaster stopped
17/12/20 20:29:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/12/20 20:29:50 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-ee5ae4cd-d8f2-47c6-b8c6-fa18c20899f7\userFiles-b6e5d874-5200-45d5-abeb-b3095777b78d
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-ee5ae4cd-d8f2-47c6-b8c6-fa18c20899f7\userFiles-b6e5d874-5200-45d5-abeb-b3095777b78d
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:102)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1842)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1841)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 20:29:50 INFO SparkContext: Successfully stopped SparkContext
17/12/20 20:29:50 INFO ShutdownHookManager: Shutdown hook called
17/12/20 20:29:50 INFO ShutdownHookManager: Deleting directory C:\Users\conan\AppData\Local\Temp\spark-ee5ae4cd-d8f2-47c6-b8c6-fa18c20899f7\userFiles-b6e5d874-5200-45d5-abeb-b3095777b78d
17/12/20 20:29:50 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-ee5ae4cd-d8f2-47c6-b8c6-fa18c20899f7\userFiles-b6e5d874-5200-45d5-abeb-b3095777b78d
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-ee5ae4cd-d8f2-47c6-b8c6-fa18c20899f7\userFiles-b6e5d874-5200-45d5-abeb-b3095777b78d
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 20:29:50 INFO ShutdownHookManager: Deleting directory C:\Users\conan\AppData\Local\Temp\spark-ee5ae4cd-d8f2-47c6-b8c6-fa18c20899f7
17/12/20 20:29:50 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-ee5ae4cd-d8f2-47c6-b8c6-fa18c20899f7
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-ee5ae4cd-d8f2-47c6-b8c6-fa18c20899f7
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 20:47:46 INFO SparkContext: Running Spark version 2.1.0
17/12/20 20:47:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/12/20 20:47:47 INFO SecurityManager: Changing view acls to: conan
17/12/20 20:47:47 INFO SecurityManager: Changing modify acls to: conan
17/12/20 20:47:47 INFO SecurityManager: Changing view acls groups to: 
17/12/20 20:47:47 INFO SecurityManager: Changing modify acls groups to: 
17/12/20 20:47:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(conan); groups with view permissions: Set(); users  with modify permissions: Set(conan); groups with modify permissions: Set()
17/12/20 20:47:47 INFO Utils: Successfully started service 'sparkDriver' on port 57636.
17/12/20 20:47:47 INFO SparkEnv: Registering MapOutputTracker
17/12/20 20:47:47 INFO SparkEnv: Registering BlockManagerMaster
17/12/20 20:47:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/12/20 20:47:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/12/20 20:47:47 INFO DiskBlockManager: Created local directory at C:\Users\conan\AppData\Local\Temp\blockmgr-b33885f3-c020-4e4d-bb15-979029000a60
17/12/20 20:47:47 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
17/12/20 20:47:47 INFO SparkEnv: Registering OutputCommitCoordinator
17/12/20 20:47:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/12/20 20:47:47 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
17/12/20 20:47:47 INFO SparkContext: Added JAR file:/C:/Users/conan/Documents/R/win-library/3.4/sparklyr/java/sparklyr-2.1-2.11.jar at spark://127.0.0.1:57636/jars/sparklyr-2.1-2.11.jar with timestamp 1513802867686
17/12/20 20:47:47 INFO Executor: Starting executor ID driver on host localhost
17/12/20 20:47:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57657.
17/12/20 20:47:47 INFO NettyBlockTransferService: Server created on 127.0.0.1:57657
17/12/20 20:47:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/12/20 20:47:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 20:47:47 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:57657 with 2004.6 MB RAM, BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 20:47:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 20:47:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 20:47:48 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
17/12/20 20:47:48 INFO SharedState: Warehouse path is 'C:UsersconanAppDataLocalsparkspark-2.1.0-bin-hadoop2.7	mphive'.
17/12/20 20:47:48 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/12/20 20:47:49 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/12/20 20:47:49 INFO ObjectStore: ObjectStore, initialize called
17/12/20 20:47:49 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/12/20 20:47:49 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/12/20 20:47:50 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/12/20 20:47:51 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 20:47:51 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 20:47:51 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 20:47:51 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 20:47:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/12/20 20:47:52 INFO ObjectStore: Initialized ObjectStore
17/12/20 20:47:52 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/12/20 20:47:52 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/12/20 20:47:52 INFO HiveMetaStore: Added admin role in metastore
17/12/20 20:47:52 INFO HiveMetaStore: Added public role in metastore
17/12/20 20:47:52 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/12/20 20:47:52 INFO HiveMetaStore: 0: get_all_databases
17/12/20 20:47:52 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_all_databases	
17/12/20 20:47:52 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/12/20 20:47:52 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/12/20 20:47:52 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 20:47:53 INFO SessionState: Created local directory: C:/Users/conan/AppData/Local/Temp/764e0945-91e1-47e7-9e39-9a090b6714d8_resources
17/12/20 20:47:53 INFO SessionState: Created HDFS directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/conan/764e0945-91e1-47e7-9e39-9a090b6714d8
17/12/20 20:47:53 INFO SessionState: Created local directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/764e0945-91e1-47e7-9e39-9a090b6714d8
17/12/20 20:47:53 INFO SessionState: Created HDFS directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/conan/764e0945-91e1-47e7-9e39-9a090b6714d8/_tmp_space.db
17/12/20 20:47:53 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:UsersconanAppDataLocalsparkspark-2.1.0-bin-hadoop2.7	mphive
17/12/20 20:47:53 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:47:53 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:47:53 INFO HiveMetaStore: 0: get_database: global_temp
17/12/20 20:47:53 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: global_temp	
17/12/20 20:47:53 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
17/12/20 20:47:53 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:47:55 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:47:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:47:55 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:47:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:47:55 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:47:55 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:48:30 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:48:30 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 20:48:30 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:48:30 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:48:30 INFO HiveMetaStore: 0: get_database: default
17/12/20 20:48:30 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 20:48:30 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 20:48:30 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 20:48:31 INFO CodeGenerator: Code generated in 259.712231 ms
17/12/20 20:48:31 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 20:48:31 INFO DAGScheduler: Got job 0 (collect at utils.scala:58) with 1 output partitions
17/12/20 20:48:31 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:58)
17/12/20 20:48:31 INFO DAGScheduler: Parents of final stage: List()
17/12/20 20:48:31 INFO DAGScheduler: Missing parents: List()
17/12/20 20:48:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at map at utils.scala:55), which has no missing parents
17/12/20 20:48:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 8.7 KB, free 2004.6 MB)
17/12/20 20:48:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2004.6 MB)
17/12/20 20:48:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:57657 (size: 4.6 KB, free: 2004.6 MB)
17/12/20 20:48:31 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996
17/12/20 20:48:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at utils.scala:55)
17/12/20 20:48:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/12/20 20:48:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6041 bytes)
17/12/20 20:48:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/12/20 20:48:31 INFO Executor: Fetching spark://127.0.0.1:57636/jars/sparklyr-2.1-2.11.jar with timestamp 1513802867686
17/12/20 20:48:31 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:57636 after 16 ms (0 ms spent in bootstraps)
17/12/20 20:48:31 INFO Utils: Fetching spark://127.0.0.1:57636/jars/sparklyr-2.1-2.11.jar to C:\Users\conan\AppData\Local\Temp\spark-60069a35-d76a-4c28-be45-e703b32e2117\userFiles-deb488d0-c74b-48aa-8d70-b421fab31fe7\fetchFileTemp8951560839362720393.tmp
17/12/20 20:48:31 INFO Executor: Adding file:/C:/Users/conan/AppData/Local/Temp/spark-60069a35-d76a-4c28-be45-e703b32e2117/userFiles-deb488d0-c74b-48aa-8d70-b421fab31fe7/sparklyr-2.1-2.11.jar to class loader
17/12/20 20:48:31 INFO CodeGenerator: Code generated in 13.176441 ms
17/12/20 20:48:31 INFO CodeGenerator: Code generated in 12.651981 ms
17/12/20 20:48:31 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1231 bytes result sent to driver
17/12/20 20:48:31 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 350 ms on localhost (executor driver) (1/1)
17/12/20 20:48:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/12/20 20:48:31 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:58) finished in 0.361 s
17/12/20 20:48:31 INFO DAGScheduler: Job 0 finished: collect at utils.scala:58, took 0.512977 s
17/12/20 20:48:37 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:48:37 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:48:37 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:48:37 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 20:48:37 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 20:48:37 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 20:48:37 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 20:48:37 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 20:48:37 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 20:48:37 INFO CodeGenerator: Code generated in 8.620927 ms
17/12/20 20:48:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 293.7 KB, free 2004.3 MB)
17/12/20 20:48:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2004.3 MB)
17/12/20 20:48:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 2004.6 MB)
17/12/20 20:48:37 INFO SparkContext: Created broadcast 1 from sql at <unknown>:0
17/12/20 20:48:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5727435 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 20:48:38 INFO CodeGenerator: Code generated in 12.117327 ms
17/12/20 20:48:38 INFO CodeGenerator: Code generated in 10.559428 ms
17/12/20 20:48:38 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 20:48:38 INFO DAGScheduler: Registering RDD 10 (sql at <unknown>:0)
17/12/20 20:48:38 INFO DAGScheduler: Registering RDD 15 (sql at <unknown>:0)
17/12/20 20:48:38 INFO DAGScheduler: Got job 1 (sql at <unknown>:0) with 1 output partitions
17/12/20 20:48:38 INFO DAGScheduler: Final stage: ResultStage 3 (sql at <unknown>:0)
17/12/20 20:48:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
17/12/20 20:48:38 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
17/12/20 20:48:38 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[10] at sql at <unknown>:0), which has no missing parents
17/12/20 20:48:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.1 KB, free 2004.3 MB)
17/12/20 20:48:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2004.3 MB)
17/12/20 20:48:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:57657 (size: 7.5 KB, free: 2004.6 MB)
17/12/20 20:48:38 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/12/20 20:48:38 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[10] at sql at <unknown>:0)
17/12/20 20:48:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks
17/12/20 20:48:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 6676 bytes)
17/12/20 20:48:38 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 6676 bytes)
17/12/20 20:48:38 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, localhost, executor driver, partition 2, PROCESS_LOCAL, 6676 bytes)
17/12/20 20:48:38 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, localhost, executor driver, partition 3, PROCESS_LOCAL, 6676 bytes)
17/12/20 20:48:38 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/12/20 20:48:38 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
17/12/20 20:48:38 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
17/12/20 20:48:38 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
17/12/20 20:48:38 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/Rtmp0CG1So/spark_serialize_457eed86bdc14089535e1075f06a43ef6382ca48e56a9614c1d75a5df5c4d65e.csv, range: 17182305-18715436, partition values: [empty row]
17/12/20 20:48:38 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/Rtmp0CG1So/spark_serialize_457eed86bdc14089535e1075f06a43ef6382ca48e56a9614c1d75a5df5c4d65e.csv, range: 5727435-11454870, partition values: [empty row]
17/12/20 20:48:38 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/Rtmp0CG1So/spark_serialize_457eed86bdc14089535e1075f06a43ef6382ca48e56a9614c1d75a5df5c4d65e.csv, range: 11454870-17182305, partition values: [empty row]
17/12/20 20:48:38 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/Rtmp0CG1So/spark_serialize_457eed86bdc14089535e1075f06a43ef6382ca48e56a9614c1d75a5df5c4d65e.csv, range: 0-5727435, partition values: [empty row]
17/12/20 20:48:38 INFO CodeGenerator: Code generated in 11.502247 ms
17/12/20 20:48:38 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 1632 bytes result sent to driver
17/12/20 20:48:38 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 469 ms on localhost (executor driver) (1/4)
17/12/20 20:48:38 INFO ContextCleaner: Cleaned accumulator 57
17/12/20 20:48:38 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 1705 bytes result sent to driver
17/12/20 20:48:38 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 563 ms on localhost (executor driver) (2/4)
17/12/20 20:48:38 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1882 bytes result sent to driver
17/12/20 20:48:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 578 ms on localhost (executor driver) (3/4)
17/12/20 20:48:38 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1882 bytes result sent to driver
17/12/20 20:48:38 INFO DAGScheduler: ShuffleMapStage 1 (sql at <unknown>:0) finished in 0.578 s
17/12/20 20:48:38 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:48:38 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 578 ms on localhost (executor driver) (4/4)
17/12/20 20:48:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/12/20 20:48:38 INFO DAGScheduler: running: Set()
17/12/20 20:48:38 INFO DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)
17/12/20 20:48:38 INFO DAGScheduler: failed: Set()
17/12/20 20:48:38 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[15] at sql at <unknown>:0), which has no missing parents
17/12/20 20:48:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.3 KB, free 2004.2 MB)
17/12/20 20:48:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2004.2 MB)
17/12/20 20:48:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:57657 (size: 6.9 KB, free: 2004.6 MB)
17/12/20 20:48:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/12/20 20:48:38 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[15] at sql at <unknown>:0)
17/12/20 20:48:38 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
17/12/20 20:48:38 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5, localhost, executor driver, partition 0, ANY, 5942 bytes)
17/12/20 20:48:38 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6, localhost, executor driver, partition 1, ANY, 5942 bytes)
17/12/20 20:48:38 INFO Executor: Running task 0.0 in stage 2.0 (TID 5)
17/12/20 20:48:38 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)
17/12/20 20:48:38 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/12/20 20:48:38 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/12/20 20:48:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:48:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:48:38 INFO MemoryStore: Block rdd_12_1 stored as values in memory (estimated size 3.8 MB, free 2000.4 MB)
17/12/20 20:48:38 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 2000.7 MB)
17/12/20 20:48:38 INFO MemoryStore: Block rdd_12_0 stored as values in memory (estimated size 3.8 MB, free 1996.6 MB)
17/12/20 20:48:38 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1996.9 MB)
17/12/20 20:48:38 INFO CodeGenerator: Code generated in 4.610263 ms
17/12/20 20:48:38 INFO CodeGenerator: Code generated in 16.720793 ms
17/12/20 20:48:38 INFO Executor: Finished task 0.0 in stage 2.0 (TID 5). 3064 bytes result sent to driver
17/12/20 20:48:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 235 ms on localhost (executor driver) (1/2)
17/12/20 20:48:39 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 3064 bytes result sent to driver
17/12/20 20:48:39 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 266 ms on localhost (executor driver) (2/2)
17/12/20 20:48:39 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/12/20 20:48:39 INFO DAGScheduler: ShuffleMapStage 2 (sql at <unknown>:0) finished in 0.266 s
17/12/20 20:48:39 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:48:39 INFO DAGScheduler: running: Set()
17/12/20 20:48:39 INFO DAGScheduler: waiting: Set(ResultStage 3)
17/12/20 20:48:39 INFO DAGScheduler: failed: Set()
17/12/20 20:48:39 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at sql at <unknown>:0), which has no missing parents
17/12/20 20:48:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 1996.6 MB)
17/12/20 20:48:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 1996.6 MB)
17/12/20 20:48:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:57657 (size: 3.7 KB, free: 1996.9 MB)
17/12/20 20:48:39 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:996
17/12/20 20:48:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at sql at <unknown>:0)
17/12/20 20:48:39 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/12/20 20:48:39 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 7, localhost, executor driver, partition 0, ANY, 5953 bytes)
17/12/20 20:48:39 INFO Executor: Running task 0.0 in stage 3.0 (TID 7)
17/12/20 20:48:39 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 20:48:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:48:39 INFO Executor: Finished task 0.0 in stage 3.0 (TID 7). 1707 bytes result sent to driver
17/12/20 20:48:39 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 7) in 0 ms on localhost (executor driver) (1/1)
17/12/20 20:48:39 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/12/20 20:48:39 INFO DAGScheduler: ResultStage 3 (sql at <unknown>:0) finished in 0.000 s
17/12/20 20:48:39 INFO DAGScheduler: Job 1 finished: sql at <unknown>:0, took 0.930870 s
17/12/20 20:48:39 INFO CodeGenerator: Code generated in 6.327123 ms
17/12/20 20:48:39 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:48:39 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 20:48:39 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 20:48:39 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 157 bytes
17/12/20 20:48:39 INFO DAGScheduler: Registering RDD 22 (collect at utils.scala:196)
17/12/20 20:48:39 INFO DAGScheduler: Got job 2 (collect at utils.scala:196) with 1 output partitions
17/12/20 20:48:39 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:196)
17/12/20 20:48:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/12/20 20:48:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/12/20 20:48:39 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[22] at collect at utils.scala:196), which has no missing parents
17/12/20 20:48:39 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.3 KB, free 1996.6 MB)
17/12/20 20:48:39 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.9 KB, free 1996.6 MB)
17/12/20 20:48:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:57657 (size: 6.9 KB, free: 1996.9 MB)
17/12/20 20:48:39 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:996
17/12/20 20:48:39 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[22] at collect at utils.scala:196)
17/12/20 20:48:39 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/12/20 20:48:39 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5934 bytes)
17/12/20 20:48:39 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 5934 bytes)
17/12/20 20:48:39 INFO Executor: Running task 0.0 in stage 5.0 (TID 8)
17/12/20 20:48:39 INFO Executor: Running task 1.0 in stage 5.0 (TID 9)
17/12/20 20:48:39 INFO BlockManager: Found block rdd_12_1 locally
17/12/20 20:48:39 INFO BlockManager: Found block rdd_12_0 locally
17/12/20 20:48:39 INFO Executor: Finished task 0.0 in stage 5.0 (TID 8). 1969 bytes result sent to driver
17/12/20 20:48:39 INFO Executor: Finished task 1.0 in stage 5.0 (TID 9). 1871 bytes result sent to driver
17/12/20 20:48:39 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 8) in 31 ms on localhost (executor driver) (1/2)
17/12/20 20:48:39 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:196) finished in 0.031 s
17/12/20 20:48:39 INFO DAGScheduler: looking for newly runnable stages
17/12/20 20:48:39 INFO DAGScheduler: running: Set()
17/12/20 20:48:39 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/12/20 20:48:39 INFO DAGScheduler: failed: Set()
17/12/20 20:48:39 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[25] at collect at utils.scala:196), which has no missing parents
17/12/20 20:48:39 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 9) in 31 ms on localhost (executor driver) (2/2)
17/12/20 20:48:39 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/12/20 20:48:39 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.0 KB, free 1996.6 MB)
17/12/20 20:48:39 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.7 KB, free 1996.5 MB)
17/12/20 20:48:39 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:57657 (size: 3.7 KB, free: 1996.9 MB)
17/12/20 20:48:39 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:996
17/12/20 20:48:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[25] at collect at utils.scala:196)
17/12/20 20:48:39 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/12/20 20:48:39 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 10, localhost, executor driver, partition 0, ANY, 5945 bytes)
17/12/20 20:48:39 INFO Executor: Running task 0.0 in stage 6.0 (TID 10)
17/12/20 20:48:39 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 20:48:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 20:48:39 INFO Executor: Finished task 0.0 in stage 6.0 (TID 10). 1707 bytes result sent to driver
17/12/20 20:48:39 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 10) in 0 ms on localhost (executor driver) (1/1)
17/12/20 20:48:39 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/12/20 20:48:39 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:196) finished in 0.000 s
17/12/20 20:48:39 INFO DAGScheduler: Job 2 finished: collect at utils.scala:196, took 0.056703 s
17/12/20 20:48:39 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:48:39 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz1`
WHERE (0 = 1)
17/12/20 20:48:42 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:48:42 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`, `V11` AS `V11`, `V12` AS `V12`, `V13` AS `V13`, `V14` AS `V14`, `V15` AS `V15`, `V16` AS `V16`, `V17` AS `V17`, `V18` AS `V18`, `V19` AS `V19`, `V20` AS `V20`, `V21` AS `V21`, `V22` AS `V22`, `V23` AS `V23`, `V24` AS `V24`, `V25` AS `V25`, `V26` AS `V26`, `V27` AS `V27`, `V28` AS `V28`, `V29` AS `V29`, `V30` AS `V30`, `V31` AS `V31`, `V32` AS `V32`, `V33` AS `V33`, `V34` AS `V34`, `V35` AS `V35`, `V36` AS `V36`, `V37` AS `V37`, `V38` AS `V38`, `V39` AS `V39`, `V40` AS `V40`, `V41` AS `V41`, `V42` AS `V42`, `V43` AS `V43`, `V44` AS `V44`, `V45` AS `V45`, `V46` AS `V46`, `V47` AS `V47`, `V48` AS `V48`, `V49` AS `V49`, `V50` AS `V50`, `V51` AS `V51`, `V52` AS `V52`, `V53` AS `V53`, `V54` AS `V54`, `V55` AS `V55`, `V56` AS `V56`, `V57` AS `V57`, `V58` AS `V58`, `V59` AS `V59`, `V60` AS `V60`, `V61` AS `V61`, `V62` AS `V62`, `V63` AS `V63`, `V64` AS `V64`, `V65` AS `V65`, `V66` AS `V66`, `V67` AS `V67`, `V68` AS `V68`, `V69` AS `V69`, `V70` AS `V70`, `V71` AS `V71`, `V72` AS `V72`, `V73` AS `V73`, `V74` AS `V74`, `V75` AS `V75`, `V76` AS `V76`, `V77` AS `V77`, `V78` AS `V78`, `V79` AS `V79`, `V80` AS `V80`, `V81` AS `V81`, `V82` AS `V82`, `V83` AS `V83`, `V84` AS `V84`, `V85` AS `V85`, `V86` AS `V86`, `V87` AS `V87`, `V88` AS `V88`, `V89` AS `V89`, `V90` AS `V90`, `V91` AS `V91`, `V92` AS `V92`, `V93` AS `V93`, `V94` AS `V94`, `V95` AS `V95`, `V96` AS `V96`, `V97` AS `V97`, `V98` AS `V98`, `V99` AS `V99`, `V100` AS `V100`, `V101` AS `V101`, `V102` AS `V102`, `V103` AS `V103`, `V104` AS `V104`, `V105` AS `V105`, `V106` AS `V106`, `V107` AS `V107`, `V108` AS `V108`, `V109` AS `V109`, `V110` AS `V110`, `V111` AS `V111`, `V112` AS `V112`, `V113` AS `V113`, `V114` AS `V114`, `V115` AS `V115`, `V116` AS `V116`, `V117` AS `V117`, `V118` AS `V118`, `V119` AS `V119`, `V120` AS `V120`, `V121` AS `V121`, `V122` AS `V122`, `V123` AS `V123`, `V124` AS `V124`, `V125` AS `V125`, `V126` AS `V126`, `V127` AS `V127`, `V128` AS `V128`, `V129` AS `V129`, `V130` AS `V130`, `V131` AS `V131`, `V132` AS `V132`, `V133` AS `V133`, `V134` AS `V134`, `V135` AS `V135`, `V136` AS `V136`, `V137` AS `V137`, `V138` AS `V138`, `V139` AS `V139`, `V140` AS `V140`, `V141` AS `V141`, `V142` AS `V142`, `V143` AS `V143`, `V144` AS `V144`, `V145` AS `V145`, `V146` AS `V146`, `V147` AS `V147`, `V148` AS `V148`, `V149` AS `V149`, `V150` AS `V150`, `V151` AS `V151`, `V152` AS `V152`, `V153` AS `V153`, `V154` AS `V154`, `V155` AS `V155`, `V156` AS `V156`, `V157` AS `V157`, `V158` AS `V158`, `V159` AS `V159`, `V160` AS `V160`, `V161` AS `V161`, `V162` AS `V162`, `V163` AS `V163`, `V164` AS `V164`, `V165` AS `V165`, `V166` AS `V166`, `V167` AS `V167`, `V168` AS `V168`, `V169` AS `V169`, `V170` AS `V170`, `V171` AS `V171`, `V172` AS `V172`, `V173` AS `V173`, `V174` AS `V174`, `V175` AS `V175`, `V176` AS `V176`, `V177` AS `V177`, `V178` AS `V178`, `V179` AS `V179`, `V180` AS `V180`, `V181` AS `V181`, `V182` AS `V182`, `V183` AS `V183`, `V184` AS `V184`, `V185` AS `V185`, `V186` AS `V186`, `V187` AS `V187`, `V188` AS `V188`, `V189` AS `V189`, `V190` AS `V190`, `V191` AS `V191`, `V192` AS `V192`, `V193` AS `V193`, `V194` AS `V194`, `V195` AS `V195`, `V196` AS `V196`, `V197` AS `V197`, `V198` AS `V198`, `V199` AS `V199`, `V200` AS `V200`, `V201` AS `V201`, `V202` AS `V202`, `V203` AS `V203`, `V204` AS `V204`, `V205` AS `V205`, `V206` AS `V206`, `V207` AS `V207`, `V208` AS `V208`, `V209` AS `V209`, `V210` AS `V210`, `V211` AS `V211`, `V212` AS `V212`, `V213` AS `V213`, `V214` AS `V214`, `V215` AS `V215`, `V216` AS `V216`, `V217` AS `V217`, `V218` AS `V218`, `V219` AS `V219`, `V220` AS `V220`, `V221` AS `V221`, `V222` AS `V222`, `V223` AS `V223`, `V224` AS `V224`, `V225` AS `V225`, `V226` AS `V226`, `V227` AS `V227`, `V228` AS `V228`, `V229` AS `V229`, `V230` AS `V230`, `V231` AS `V231`, `V232` AS `V232`, `V233` AS `V233`, `V234` AS `V234`, `V235` AS `V235`, `V236` AS `V236`, `V237` AS `V237`, `V238` AS `V238`, `V239` AS `V239`, `V240` AS `V240`, `V241` AS `V241`, `V242` AS `V242`, `V243` AS `V243`, `V244` AS `V244`, `V245` AS `V245`, `V246` AS `V246`, `V247` AS `V247`, `V248` AS `V248`, `V249` AS `V249`, `V250` AS `V250`, `V251` AS `V251`, `V252` AS `V252`, `V253` AS `V253`, `V254` AS `V254`, `V255` AS `V255`, `V256` AS `V256`, `V257` AS `V257`, `V258` AS `V258`, `V259` AS `V259`, `V260` AS `V260`, `V261` AS `V261`, `V262` AS `V262`, `V263` AS `V263`, `V264` AS `V264`, `V265` AS `V265`, `V266` AS `V266`, `V267` AS `V267`, `V268` AS `V268`, `V269` AS `V269`, `V270` AS `V270`, `V271` AS `V271`, `V272` AS `V272`, `V273` AS `V273`, `V274` AS `V274`, `V275` AS `V275`, `V276` AS `V276`, `V277` AS `V277`, `V278` AS `V278`, `V279` AS `V279`, `V280` AS `V280`, `V281` AS `V281`, `V282` AS `V282`, `V283` AS `V283`, `V284` AS `V284`, `V285` AS `V285`, `V286` AS `V286`, `V287` AS `V287`, `V288` AS `V288`, `V289` AS `V289`, `V290` AS `V290`, `V291` AS `V291`, `V292` AS `V292`, `V293` AS `V293`, `V294` AS `V294`, `V295` AS `V295`, `V296` AS `V296`, `V297` AS `V297`, `V298` AS `V298`, `V299` AS `V299`, `V300` AS `V300`, `V301` AS `V301`, `V302` AS `V302`, `V303` AS `V303`, `V304` AS `V304`, `V305` AS `V305`, `V306` AS `V306`, `V307` AS `V307`, `V308` AS `V308`, `V309` AS `V309`, `V310` AS `V310`, `V311` AS `V311`, `V312` AS `V312`, `V313` AS `V313`, `V314` AS `V314`, `V315` AS `V315`, `V316` AS `V316`, `V317` AS `V317`, `V318` AS `V318`, `V319` AS `V319`, `V320` AS `V320`, `V321` AS `V321`, `V322` AS `V322`, `V323` AS `V323`, `V324` AS `V324`, `V325` AS `V325`, `V326` AS `V326`, `V327` AS `V327`, `V328` AS `V328`, `V329` AS `V329`, `V330` AS `V330`, `V331` AS `V331`, `V332` AS `V332`, `V333` AS `V333`, `V334` AS `V334`, `V335` AS `V335`, `V336` AS `V336`, `V337` AS `V337`, `V338` AS `V338`, `V339` AS `V339`, `V340` AS `V340`, `V341` AS `V341`, `V342` AS `V342`, `V343` AS `V343`, `V344` AS `V344`, `V345` AS `V345`, `V346` AS `V346`, `V347` AS `V347`, `V348` AS `V348`, `V349` AS `V349`, `V350` AS `V350`, `V351` AS `V351`, `V352` AS `V352`, `V353` AS `V353`, `V354` AS `V354`, `V355` AS `V355`, `V356` AS `V356`, `V357` AS `V357`, `V358` AS `V358`, `V359` AS `V359`, `V360` AS `V360`, `V361` AS `V361`, `V362` AS `V362`, `V363` AS `V363`, `V364` AS `V364`, `V365` AS `V365`, `V366` AS `V366`, `V367` AS `V367`, `V368` AS `V368`, `V369` AS `V369`, `V370` AS `V370`, `V371` AS `V371`, `V372` AS `V372`, `V373` AS `V373`, `V374` AS `V374`, `V375` AS `V375`, `V376` AS `V376`, `V377` AS `V377`, `V378` AS `V378`, `V379` AS `V379`, `V380` AS `V380`, `V381` AS `V381`, `V382` AS `V382`, `V383` AS `V383`, `V384` AS `V384`, `V385` AS `V385`, `V386` AS `V386`, `V387` AS `V387`, `V388` AS `V388`, `V389` AS `V389`, `V390` AS `V390`, `V391` AS `V391`, `V392` AS `V392`, `V393` AS `V393`, `V394` AS `V394`, `V395` AS `V395`, `V396` AS `V396`, `V397` AS `V397`, `V398` AS `V398`, `V399` AS `V399`, `V400` AS `V400`, `V401` AS `V401`, `V402` AS `V402`, `V403` AS `V403`, `V404` AS `V404`, `V405` AS `V405`, `V406` AS `V406`, `V407` AS `V407`, `V408` AS `V408`, `V409` AS `V409`, `V410` AS `V410`, `V411` AS `V411`, `V412` AS `V412`, `V413` AS `V413`, `V414` AS `V414`, `V415` AS `V415`, `V416` AS `V416`, `V417` AS `V417`, `V418` AS `V418`, `V419` AS `V419`, `V420` AS `V420`, `V421` AS `V421`, `V422` AS `V422`, `V423` AS `V423`, `V424` AS `V424`, `V425` AS `V425`, `V426` AS `V426`, `V427` AS `V427`, `V428` AS `V428`, `V429` AS `V429`, `V430` AS `V430`, `V431` AS `V431`, `V432` AS `V432`, `V433` AS `V433`, `V434` AS `V434`, `V435` AS `V435`, `V436` AS `V436`, `V437` AS `V437`, `V438` AS `V438`, `V439` AS `V439`, `V440` AS `V440`, `V441` AS `V441`, `V442` AS `V442`, `V443` AS `V443`, `V444` AS `V444`, `V445` AS `V445`, `V446` AS `V446`, `V447` AS `V447`, `V448` AS `V448`, `V449` AS `V449`, `V450` AS `V450`, `V451` AS `V451`, `V452` AS `V452`, `V453` AS `V453`, `V454` AS `V454`, `V455` AS `V455`, `V456` AS `V456`, `V457` AS `V457`, `V458` AS `V458`, `V459` AS `V459`, `V460` AS `V460`, `V461` AS `V461`, `V462` AS `V462`, `V463` AS `V463`, `V464` AS `V464`, `V465` AS `V465`, `V466` AS `V466`, `V467` AS `V467`, `V468` AS `V468`, `V469` AS `V469`, `V470` AS `V470`, `V471` AS `V471`, `V472` AS `V472`, `V473` AS `V473`, `V474` AS `V474`, `V475` AS `V475`, `V476` AS `V476`, `V477` AS `V477`, `V478` AS `V478`, `V479` AS `V479`, `V480` AS `V480`, `V481` AS `V481`, `V482` AS `V482`, `V483` AS `V483`, `V484` AS `V484`, `V485` AS `V485`, `V486` AS `V486`, `V487` AS `V487`, `V488` AS `V488`, `V489` AS `V489`, `V490` AS `V490`, `V491` AS `V491`, `V492` AS `V492`, `V493` AS `V493`, `V494` AS `V494`, `V495` AS `V495`, `V496` AS `V496`, `V497` AS `V497`, `V498` AS `V498`, `V499` AS `V499`, `V500` AS `V500`, `V501` AS `V501`, `V502` AS `V502`, `V503` AS `V503`, `V504` AS `V504`, `V505` AS `V505`, `V506` AS `V506`, `V507` AS `V507`, `V508` AS `V508`, `V509` AS `V509`, `V510` AS `V510`, `V511` AS `V511`, `V512` AS `V512`, `V513` AS `V513`, `V514` AS `V514`, `V515` AS `V515`, `V516` AS `V516`, `V517` AS `V517`, `V518` AS `V518`, `V519` AS `V519`, `V520` AS `V520`, `V521` AS `V521`, `V522` AS `V522`, `V523` AS `V523`, `V524` AS `V524`, `V525` AS `V525`, `V526` AS `V526`, `V527` AS `V527`, `V528` AS `V528`, `V529` AS `V529`, `V530` AS `V530`, `V531` AS `V531`, `V532` AS `V532`, `V533` AS `V533`, `V534` AS `V534`, `V535` AS `V535`, `V536` AS `V536`, `V537` AS `V537`, `V538` AS `V538`, `V539` AS `V539`, `V540` AS `V540`, `V541` AS `V541`, `V542` AS `V542`, `V543` AS `V543`, `V544` AS `V544`, `V545` AS `V545`, `V546` AS `V546`, `V547` AS `V547`, `V548` AS `V548`, `V549` AS `V549`, `V550` AS `V550`, `V551` AS `V551`, `V552` AS `V552`, `V553` AS `V553`, `V554` AS `V554`, `V555` AS `V555`, `V556` AS `V556`, `V557` AS `V557`, `V558` AS `V558`, `V559` AS `V559`, `V560` AS `V560`, `V561` AS `V561`, `V562` AS `V562`, `V563` AS `V563`, `V564` AS `V564`, `V565` AS `V565`, `V566` AS `V566`, `V567` AS `V567`, `V568` AS `V568`, `V569` AS `V569`, `V570` AS `V570`, `V571` AS `V571`, `V572` AS `V572`, `V573` AS `V573`, `V574` AS `V574`, `V575` AS `V575`, `V576` AS `V576`, `V577` AS `V577`, `V578` AS `V578`, `V579` AS `V579`, `V580` AS `V580`, `V581` AS `V581`, `V582` AS `V582`, `V583` AS `V583`, `V584` AS `V584`, `V585` AS `V585`, `V586` AS `V586`, `V587` AS `V587`, `V588` AS `V588`, `V589` AS `V589`, `V590` AS `V590`, `V591` AS `V591`, `V592` AS `V592`, `V593` AS `V593`, `V594` AS `V594`, `V595` AS `V595`, `V596` AS `V596`, `V597` AS `V597`, `V598` AS `V598`, `V599` AS `V599`, `V600` AS `V600`, `V601` AS `V601`, `V602` AS `V602`, `V603` AS `V603`, `V604` AS `V604`, `V605` AS `V605`, `V606` AS `V606`, `V607` AS `V607`, `V608` AS `V608`, `V609` AS `V609`, `V610` AS `V610`, `V611` AS `V611`, `V612` AS `V612`, `V613` AS `V613`, `V614` AS `V614`, `V615` AS `V615`, `V616` AS `V616`, `V617` AS `V617`, `V618` AS `V618`, `V619` AS `V619`, `V620` AS `V620`, `V621` AS `V621`, `V622` AS `V622`, `V623` AS `V623`, `V624` AS `V624`, `V625` AS `V625`, `V626` AS `V626`, `V627` AS `V627`, `V628` AS `V628`, `V629` AS `V629`, `V630` AS `V630`, `V631` AS `V631`, `V632` AS `V632`, `V633` AS `V633`, `V634` AS `V634`, `V635` AS `V635`, `V636` AS `V636`, `V637` AS `V637`, `V638` AS `V638`, `V639` AS `V639`, `V640` AS `V640`, `V641` AS `V641`, `V642` AS `V642`, `V643` AS `V643`, `V644` AS `V644`, `V645` AS `V645`, `V646` AS `V646`, `V647` AS `V647`, `V648` AS `V648`, `V649` AS `V649`, `V650` AS `V650`, `V651` AS `V651`, `V652` AS `V652`, `V653` AS `V653`, `V654` AS `V654`, `V655` AS `V655`, `V656` AS `V656`, `V657` AS `V657`, `V658` AS `V658`, `V659` AS `V659`, `V660` AS `V660`, `V661` AS `V661`, `V662` AS `V662`, `V663` AS `V663`, `V664` AS `V664`, `V665` AS `V665`, `V666` AS `V666`, `V667` AS `V667`, `V668` AS `V668`, `V669` AS `V669`, `V670` AS `V670`, `V671` AS `V671`, `V672` AS `V672`, `V673` AS `V673`, `V674` AS `V674`, `V675` AS `V675`, `V676` AS `V676`, `V677` AS `V677`, `V678` AS `V678`, `V679` AS `V679`, `V680` AS `V680`, `V681` AS `V681`, `V682` AS `V682`, `V683` AS `V683`, `V684` AS `V684`, `V685` AS `V685`, `V686` AS `V686`, `V687` AS `V687`, `V688` AS `V688`, `V689` AS `V689`, `V690` AS `V690`, `V691` AS `V691`, `V692` AS `V692`, `V693` AS `V693`, `V694` AS `V694`, `V695` AS `V695`, `V696` AS `V696`, `V697` AS `V697`, `V698` AS `V698`, `V699` AS `V699`, `V700` AS `V700`, `V701` AS `V701`, `V702` AS `V702`, `V703` AS `V703`, `V704` AS `V704`, `V705` AS `V705`, `V706` AS `V706`, `V707` AS `V707`, `V708` AS `V708`, `V709` AS `V709`, `V710` AS `V710`, `V711` AS `V711`, `V712` AS `V712`, `V713` AS `V713`, `V714` AS `V714`, `V715` AS `V715`, `V716` AS `V716`, `V717` AS `V717`, `V718` AS `V718`, `V719` AS `V719`, `V720` AS `V720`, `V721` AS `V721`, `V722` AS `V722`, `V723` AS `V723`, `V724` AS `V724`, `V725` AS `V725`, `V726` AS `V726`, `V727` AS `V727`, `V728` AS `V728`, `V729` AS `V729`, `V730` AS `V730`, `V731` AS `V731`, `V732` AS `V732`, `V733` AS `V733`, `V734` AS `V734`, `V735` AS `V735`, `V736` AS `V736`, `V737` AS `V737`, `V738` AS `V738`, `V739` AS `V739`, `V740` AS `V740`, `V741` AS `V741`, `V742` AS `V742`, `V743` AS `V743`, `V744` AS `V744`, `V745` AS `V745`, `V746` AS `V746`, `V747` AS `V747`, `V748` AS `V748`, `V749` AS `V749`, `V750` AS `V750`, `V751` AS `V751`, `V752` AS `V752`, `V753` AS `V753`, `V754` AS `V754`, `V755` AS `V755`, `V756` AS `V756`, `V757` AS `V757`, `V758` AS `V758`, `V759` AS `V759`, `V760` AS `V760`, `V761` AS `V761`, `V762` AS `V762`, `V763` AS `V763`, `V764` AS `V764`, `V765` AS `V765`, `V766` AS `V766`, `V767` AS `V767`, `V768` AS `V768`, `V769` AS `V769`, `V770` AS `V770`, `V771` AS `V771`, `V772` AS `V772`, `V773` AS `V773`, `V774` AS `V774`, `V775` AS `V775`, `V776` AS `V776`, `V777` AS `V777`, `V778` AS `V778`, `V779` AS `V779`, `V780` AS `V780`, `V781` AS `V781`, `V782` AS `V782`, `V783` AS `V783`, `V784` AS `V784`, `V785` AS `V785`, `V786` AS `V786`, `V787` AS `V787`, `V788` AS `V788`, `V789` AS `V789`, `V790` AS `V790`, `V791` AS `V791`, `V792` AS `V792`, `V793` AS `V793`, `V794` AS `V794`, `V795` AS `V795`, `V796` AS `V796`, `V797` AS `V797`, `V798` AS `V798`, `V799` AS `V799`, `V800` AS `V800`, `V801` AS `V801`, `V802` AS `V802`, `V803` AS `V803`, `V804` AS `V804`, `V805` AS `V805`, `V806` AS `V806`, `V807` AS `V807`, `V808` AS `V808`, `V809` AS `V809`, `V810` AS `V810`, `V811` AS `V811`, `V812` AS `V812`, `V813` AS `V813`, `V814` AS `V814`, `V815` AS `V815`, `V816` AS `V816`, `V817` AS `V817`, `V818` AS `V818`, `V819` AS `V819`, `V820` AS `V820`, `V821` AS `V821`, `V822` AS `V822`, `V823` AS `V823`, `V824` AS `V824`, `V825` AS `V825`, `V826` AS `V826`, `V827` AS `V827`, `V828` AS `V828`, `V829` AS `V829`, `V830` AS `V830`, `V831` AS `V831`, `V832` AS `V832`, `V833` AS `V833`, `V834` AS `V834`, `V835` AS `V835`, `V836` AS `V836`, `V837` AS `V837`, `V838` AS `V838`, `V839` AS `V839`, `V840` AS `V840`, `V841` AS `V841`, `V842` AS `V842`, `V843` AS `V843`, `V844` AS `V844`, `V845` AS `V845`, `V846` AS `V846`, `V847` AS `V847`, `V848` AS `V848`, `V849` AS `V849`, `V850` AS `V850`, `V851` AS `V851`, `V852` AS `V852`, `V853` AS `V853`, `V854` AS `V854`, `V855` AS `V855`, `V856` AS `V856`, `V857` AS `V857`, `V858` AS `V858`, `V859` AS `V859`, `V860` AS `V860`, `V861` AS `V861`, `V862` AS `V862`, `V863` AS `V863`, `V864` AS `V864`, `V865` AS `V865`, `V866` AS `V866`, `V867` AS `V867`, `V868` AS `V868`, `V869` AS `V869`, `V870` AS `V870`, `V871` AS `V871`, `V872` AS `V872`, `V873` AS `V873`, `V874` AS `V874`, `V875` AS `V875`, `V876` AS `V876`, `V877` AS `V877`, `V878` AS `V878`, `V879` AS `V879`, `V880` AS `V880`, `V881` AS `V881`, `V882` AS `V882`, `V883` AS `V883`, `V884` AS `V884`, `V885` AS `V885`, `V886` AS `V886`, `V887` AS `V887`, `V888` AS `V888`, `V889` AS `V889`, `V890` AS `V890`, `V891` AS `V891`, `V892` AS `V892`, `V893` AS `V893`, `V894` AS `V894`, `V895` AS `V895`, `V896` AS `V896`, `V897` AS `V897`, `V898` AS `V898`, `V899` AS `V899`, `V900` AS `V900`, `V901` AS `V901`, `V902` AS `V902`, `V903` AS `V903`, `V904` AS `V904`, `V905` AS `V905`, `V906` AS `V906`, `V907` AS `V907`, `V908` AS `V908`, `V909` AS `V909`, `V910` AS `V910`, `V911` AS `V911`, `V912` AS `V912`, `V913` AS `V913`, `V914` AS `V914`, `V915` AS `V915`, `V916` AS `V916`, `V917` AS `V917`, `V918` AS `V918`, `V919` AS `V919`, `V920` AS `V920`, `V921` AS `V921`, `V922` AS `V922`, `V923` AS `V923`, `V924` AS `V924`, `V925` AS `V925`, `V926` AS `V926`, `V927` AS `V927`, `V928` AS `V928`, `V929` AS `V929`, `V930` AS `V930`, `V931` AS `V931`, `V932` AS `V932`, `V933` AS `V933`, `V934` AS `V934`, `V935` AS `V935`, `V936` AS `V936`, `V937` AS `V937`, `V938` AS `V938`, `V939` AS `V939`, `V940` AS `V940`, `V941` AS `V941`, `V942` AS `V942`, `V943` AS `V943`, `V944` AS `V944`, `V945` AS `V945`, `V946` AS `V946`, `V947` AS `V947`, `V948` AS `V948`, `V949` AS `V949`, `V950` AS `V950`, `V951` AS `V951`, `V952` AS `V952`, `V953` AS `V953`, `V954` AS `V954`, `V955` AS `V955`, `V956` AS `V956`, `V957` AS `V957`, `V958` AS `V958`, `V959` AS `V959`, `V960` AS `V960`, `V961` AS `V961`, `V962` AS `V962`, `V963` AS `V963`, `V964` AS `V964`, `V965` AS `V965`, `V966` AS `V966`, `V967` AS `V967`, `V968` AS `V968`, `V969` AS `V969`, `V970` AS `V970`, `V971` AS `V971`, `V972` AS `V972`, `V973` AS `V973`, `V974` AS `V974`, `V975` AS `V975`, `V976` AS `V976`, `V977` AS `V977`, `V978` AS `V978`, `V979` AS `V979`, `V980` AS `V980`, `V981` AS `V981`, `V982` AS `V982`, `V983` AS `V983`, `V984` AS `V984`, `V985` AS `V985`, `V986` AS `V986`, `V987` AS `V987`, `V988` AS `V988`, `V989` AS `V989`, `V990` AS `V990`, `V991` AS `V991`, `V992` AS `V992`, `V993` AS `V993`, `V994` AS `V994`, `V995` AS `V995`, `V996` AS `V996`, `V997` AS `V997`, `V998` AS `V998`, `V999` AS `V999`, `V1000` AS `V1000`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S8` + 0.92195445 * RANDN() AS `V1`, `S9` + 0.92736185 * RANDN() AS `V2`, `S3` + 0.92195445 * RANDN() AS `V3`, `S7` + 0.93808315 * RANDN() AS `V4`, `S3` + 0.92195445 * RANDN() AS `V5`, `S3` + 0.92195445 * RANDN() AS `V6`, `S2` + 0.94339811 * RANDN() AS `V7`, `S4` + 0.93808315 * RANDN() AS `V8`, `S6` + 0.92195445 * RANDN() AS `V9`, `S5` + 0.90553851 * RANDN() AS `V10`, `S3` + 0.92195445 * RANDN() AS `V11`, `S9` + 0.92736185 * RANDN() AS `V12`, `S5` + 0.90553851 * RANDN() AS `V13`, `S7` + 0.93808315 * RANDN() AS `V14`, `S9` + 0.92736185 * RANDN() AS `V15`, `S8` + 0.92195445 * RANDN() AS `V16`, `S1` + 0.9 * RANDN() AS `V17`, `S7` + 0.93808315 * RANDN() AS `V18`, `S6` + 0.92195445 * RANDN() AS `V19`, `S1` + 0.9 * RANDN() AS `V20`, `S2` + 0.94339811 * RANDN() AS `V21`, `S10` + 0.9486833 * RANDN() AS `V22`, `S1` + 0.9 * RANDN() AS `V23`, `S6` + 0.92195445 * RANDN() AS `V24`, `S4` + 0.93808315 * RANDN() AS `V25`, `S3` + 0.92195445 * RANDN() AS `V26`, `S6` + 0.92195445 * RANDN() AS `V27`, `S8` + 0.92195445 * RANDN() AS `V28`, `S4` + 0.93808315 * RANDN() AS `V29`, `S4` + 0.93808315 * RANDN() AS `V30`, `S10` + 0.9486833 * RANDN() AS `V31`, `S10` + 0.9486833 * RANDN() AS `V32`, `S6` + 0.92195445 * RANDN() AS `V33`, `S4` + 0.93808315 * RANDN() AS `V34`, `S6` + 0.92195445 * RANDN() AS `V35`, `S2` + 0.94339811 * RANDN() AS `V36`, `S9` + 0.92736185 * RANDN() AS `V37`, `S3` + 0.92195445 * RANDN() AS `V38`, `S1` + 0.9 * RANDN() AS `V39`, `S7` + 0.93808315 * RANDN() AS `V40`, `S3` + 0.92195445 * RANDN() AS `V41`, `S1` + 0.9 * RANDN() AS `V42`, `S6` + 0.92195445 * RANDN() AS `V43`, `S10` + 0.9486833 * RANDN() AS `V44`, `S2` + 0.94339811 * RANDN() AS `V45`, `S7` + 0.93808315 * RANDN() AS `V46`, `S7` + 0.93808315 * RANDN() AS `V47`, `S4` + 0.93808315 * RANDN() AS `V48`, `S4` + 0.93808315 * RANDN() AS `V49`, `S5` + 0.90553851 * RANDN() AS `V50`, `S4` + 0.93808315 * RANDN() AS `V51`, `S9` + 0.92736185 * RANDN() AS `V52`, `S5` + 0.90553851 * RANDN() AS `V53`, `S8` + 0.92195445 * RANDN() AS `V54`, `S7` + 0.93808315 * RANDN() AS `V55`, `S5` + 0.90553851 * RANDN() AS `V56`, `S9` + 0.92736185 * RANDN() AS `V57`, `S9` + 0.92736185 * RANDN() AS `V58`, `S5` + 0.90553851 * RANDN() AS `V59`, `S10` + 0.9486833 * RANDN() AS `V60`, `S1` + 0.9 * RANDN() AS `V61`, `S7` + 0.93808315 * RANDN() AS `V62`, `S8` + 0.92195445 * RANDN() AS `V63`, `S1` + 0.9 * RANDN() AS `V64`, `S2` + 0.94339811 * RANDN() AS `V65`, `S8` + 0.92195445 * RANDN() AS `V66`, `S2` + 0.94339811 * RANDN() AS `V67`, `S9` + 0.92736185 * RANDN() AS `V68`, `S9` + 0.92736185 * RANDN() AS `V69`, `S3` + 0.92195445 * RANDN() AS `V70`, `S10` + 0.9486833 * RANDN() AS `V71`, `S5` + 0.90553851 * RANDN() AS `V72`, `S9` + 0.92736185 * RANDN() AS `V73`, `S1` + 0.9 * RANDN() AS `V74`, `S1` + 0.9 * RANDN() AS `V75`, `S6` + 0.92195445 * RANDN() AS `V76`, `S2` + 0.94339811 * RANDN() AS `V77`, `S9` + 0.92736185 * RANDN() AS `V78`, `S5` + 0.90553851 * RANDN() AS `V79`, `S2` + 0.94339811 * RANDN() AS `V80`, `S1` + 0.9 * RANDN() AS `V81`, `S2` + 0.94339811 * RANDN() AS `V82`, `S4` + 0.93808315 * RANDN() AS `V83`, `S5` + 0.90553851 * RANDN() AS `V84`, `S6` + 0.92195445 * RANDN() AS `V85`, `S6` + 0.92195445 * RANDN() AS `V86`, `S3` + 0.92195445 * RANDN() AS `V87`, `S2` + 0.94339811 * RANDN() AS `V88`, `S2` + 0.94339811 * RANDN() AS `V89`, `S7` + 0.93808315 * RANDN() AS `V90`, `S2` + 0.94339811 * RANDN() AS `V91`, `S1` + 0.9 * RANDN() AS `V92`, `S7` + 0.93808315 * RANDN() AS `V93`, `S2` + 0.94339811 * RANDN() AS `V94`, `S8` + 0.92195445 * RANDN() AS `V95`, `S2` + 0.94339811 * RANDN() AS `V96`, `S3` + 0.92195445 * RANDN() AS `V97`, `S9` + 0.92736185 * RANDN() AS `V98`, `S4` + 0.93808315 * RANDN() AS `V99`, `S9` + 0.92736185 * RANDN() AS `V100`, `S8` + 0.92195445 * RANDN() AS `V101`, `S10` + 0.9486833 * RANDN() AS `V102`, `S6` + 0.92195445 * RANDN() AS `V103`, `S1` + 0.9 * RANDN() AS `V104`, `S2` + 0.94339811 * RANDN() AS `V105`, `S3` + 0.92195445 * RANDN() AS `V106`, `S5` + 0.90553851 * RANDN() AS `V107`, `S2` + 0.94339811 * RANDN() AS `V108`, `S6` + 0.92195445 * RANDN() AS `V109`, `S8` + 0.92195445 * RANDN() AS `V110`, `S1` + 0.9 * RANDN() AS `V111`, `S1` + 0.9 * RANDN() AS `V112`, `S5` + 0.90553851 * RANDN() AS `V113`, `S5` + 0.90553851 * RANDN() AS `V114`, `S10` + 0.9486833 * RANDN() AS `V115`, `S8` + 0.92195445 * RANDN() AS `V116`, `S6` + 0.92195445 * RANDN() AS `V117`, `S4` + 0.93808315 * RANDN() AS `V118`, `S5` + 0.90553851 * RANDN() AS `V119`, `S7` + 0.93808315 * RANDN() AS `V120`, `S5` + 0.90553851 * RANDN() AS `V121`, `S7` + 0.93808315 * RANDN() AS `V122`, `S2` + 0.94339811 * RANDN() AS `V123`, `S1` + 0.9 * RANDN() AS `V124`, `S6` + 0.92195445 * RANDN() AS `V125`, `S9` + 0.92736185 * RANDN() AS `V126`, `S5` + 0.90553851 * RANDN() AS `V127`, `S8` + 0.92195445 * RANDN() AS `V128`, `S9` + 0.92736185 * RANDN() AS `V129`, `S5` + 0.90553851 * RANDN() AS `V130`, `S7` + 0.93808315 * RANDN() AS `V131`, `S4` + 0.93808315 * RANDN() AS `V132`, `S4` + 0.93808315 * RANDN() AS `V133`, `S8` + 0.92195445 * RANDN() AS `V134`, `S10` + 0.9486833 * RANDN() AS `V135`, `S9` + 0.92736185 * RANDN() AS `V136`, `S10` + 0.9486833 * RANDN() AS `V137`, `S1` + 0.9 * RANDN() AS `V138`, `S8` + 0.92195445 * RANDN() AS `V139`, `S8` + 0.92195445 * RANDN() AS `V140`, `S7` + 0.93808315 * RANDN() AS `V141`, `S3` + 0.92195445 * RANDN() AS `V142`, `S9` + 0.92736185 * RANDN() AS `V143`, `S5` + 0.90553851 * RANDN() AS `V144`, `S8` + 0.92195445 * RANDN() AS `V145`, `S9` + 0.92736185 * RANDN() AS `V146`, `S5` + 0.90553851 * RANDN() AS `V147`, `S1` + 0.9 * RANDN() AS `V148`, `S3` + 0.92195445 * RANDN() AS `V149`, `S1` + 0.9 * RANDN() AS `V150`, `S9` + 0.92736185 * RANDN() AS `V151`, `S7` + 0.93808315 * RANDN() AS `V152`, `S3` + 0.92195445 * RANDN() AS `V153`, `S4` + 0.93808315 * RANDN() AS `V154`, `S4` + 0.93808315 * RANDN() AS `V155`, `S3` + 0.92195445 * RANDN() AS `V156`, `S2` + 0.94339811 * RANDN() AS `V157`, `S4` + 0.93808315 * RANDN() AS `V158`, `S1` + 0.9 * RANDN() AS `V159`, `S4` + 0.93808315 * RANDN() AS `V160`, `S7` + 0.93808315 * RANDN() AS `V161`, `S3` + 0.92195445 * RANDN() AS `V162`, `S1` + 0.9 * RANDN() AS `V163`, `S6` + 0.92195445 * RANDN() AS `V164`, `S7` + 0.93808315 * RANDN() AS `V165`, `S7` + 0.93808315 * RANDN() AS `V166`, `S5` + 0.90553851 * RANDN() AS `V167`, `S7` + 0.93808315 * RANDN() AS `V168`, `S10` + 0.9486833 * RANDN() AS `V169`, `S6` + 0.92195445 * RANDN() AS `V170`, `S5` + 0.90553851 * RANDN() AS `V171`, `S4` + 0.93808315 * RANDN() AS `V172`, `S1` + 0.9 * RANDN() AS `V173`, `S7` + 0.93808315 * RANDN() AS `V174`, `S10` + 0.9486833 * RANDN() AS `V175`, `S8` + 0.92195445 * RANDN() AS `V176`, `S7` + 0.93808315 * RANDN() AS `V177`, `S8` + 0.92195445 * RANDN() AS `V178`, `S8` + 0.92195445 * RANDN() AS `V179`, `S2` + 0.94339811 * RANDN() AS `V180`, `S7` + 0.93808315 * RANDN() AS `V181`, `S3` + 0.92195445 * RANDN() AS `V182`, `S10` + 0.9486833 * RANDN() AS `V183`, `S3` + 0.92195445 * RANDN() AS `V184`, `S5` + 0.90553851 * RANDN() AS `V185`, `S2` + 0.94339811 * RANDN() AS `V186`, `S6` + 0.92195445 * RANDN() AS `V187`, `S3` + 0.92195445 * RANDN() AS `V188`, `S9` + 0.92736185 * RANDN() AS `V189`, `S1` + 0.9 * RANDN() AS `V190`, `S9` + 0.92736185 * RANDN() AS `V191`, `S7` + 0.93808315 * RANDN() AS `V192`, `S4` + 0.93808315 * RANDN() AS `V193`, `S3` + 0.92195445 * RANDN() AS `V194`, `S6` + 0.92195445 * RANDN() AS `V195`, `S6` + 0.92195445 * RANDN() AS `V196`, `S7` + 0.93808315 * RANDN() AS `V197`, `S8` + 0.92195445 * RANDN() AS `V198`, `S5` + 0.90553851 * RANDN() AS `V199`, `S9` + 0.92736185 * RANDN() AS `V200`, `S6` + 0.92195445 * RANDN() AS `V201`, `S10` + 0.9486833 * RANDN() AS `V202`, `S2` + 0.94339811 * RANDN() AS `V203`, `S4` + 0.93808315 * RANDN() AS `V204`, `S6` + 0.92195445 * RANDN() AS `V205`, `S6` + 0.92195445 * RANDN() AS `V206`, `S8` + 0.92195445 * RANDN() AS `V207`, `S4` + 0.93808315 * RANDN() AS `V208`, `S3` + 0.92195445 * RANDN() AS `V209`, `S10` + 0.9486833 * RANDN() AS `V210`, `S8` + 0.92195445 * RANDN() AS `V211`, `S9` + 0.92736185 * RANDN() AS `V212`, `S2` + 0.94339811 * RANDN() AS `V213`, `S4` + 0.93808315 * RANDN() AS `V214`, `S5` + 0.90553851 * RANDN() AS `V215`, `S1` + 0.9 * RANDN() AS `V216`, `S7` + 0.93808315 * RANDN() AS `V217`, `S5` + 0.90553851 * RANDN() AS `V218`, `S6` + 0.92195445 * RANDN() AS `V219`, `S2` + 0.94339811 * RANDN() AS `V220`, `S6` + 0.92195445 * RANDN() AS `V221`, `S8` + 0.92195445 * RANDN() AS `V222`, `S1` + 0.9 * RANDN() AS `V223`, `S6` + 0.92195445 * RANDN() AS `V224`, `S10` + 0.9486833 * RANDN() AS `V225`, `S1` + 0.9 * RANDN() AS `V226`, `S5` + 0.90553851 * RANDN() AS `V227`, `S9` + 0.92736185 * RANDN() AS `V228`, `S5` + 0.90553851 * RANDN() AS `V229`, `S5` + 0.90553851 * RANDN() AS `V230`, `S5` + 0.90553851 * RANDN() AS `V231`, `S4` + 0.93808315 * RANDN() AS `V232`, `S9` + 0.92736185 * RANDN() AS `V233`, `S10` + 0.9486833 * RANDN() AS `V234`, `S5` + 0.90553851 * RANDN() AS `V235`, `S10` + 0.9486833 * RANDN() AS `V236`, `S2` + 0.94339811 * RANDN() AS `V237`, `S10` + 0.9486833 * RANDN() AS `V238`, `S2` + 0.94339811 * RANDN() AS `V239`, `S2` + 0.94339811 * RANDN() AS `V240`, `S1` + 0.9 * RANDN() AS `V241`, `S9` + 0.92736185 * RANDN() AS `V242`, `S6` + 0.92195445 * RANDN() AS `V243`, `S2` + 0.94339811 * RANDN() AS `V244`, `S3` + 0.92195445 * RANDN() AS `V245`, `S10` + 0.9486833 * RANDN() AS `V246`, `S8` + 0.92195445 * RANDN() AS `V247`, `S3` + 0.92195445 * RANDN() AS `V248`, `S10` + 0.9486833 * RANDN() AS `V249`, `S1` + 0.9 * RANDN() AS `V250`, `S7` + 0.93808315 * RANDN() AS `V251`, `S8` + 0.92195445 * RANDN() AS `V252`, `S1` + 0.9 * RANDN() AS `V253`, `S3` + 0.92195445 * RANDN() AS `V254`, `S6` + 0.92195445 * RANDN() AS `V255`, `S8` + 0.92195445 * RANDN() AS `V256`, `S10` + 0.9486833 * RANDN() AS `V257`, `S8` + 0.92195445 * RANDN() AS `V258`, `S1` + 0.9 * RANDN() AS `V259`, `S3` + 0.92195445 * RANDN() AS `V260`, `S2` + 0.94339811 * RANDN() AS `V261`, `S2` + 0.94339811 * RANDN() AS `V262`, `S1` + 0.9 * RANDN() AS `V263`, `S4` + 0.93808315 * RANDN() AS `V264`, `S7` + 0.93808315 * RANDN() AS `V265`, `S6` + 0.92195445 * RANDN() AS `V266`, `S9` + 0.92736185 * RANDN() AS `V267`, `S6` + 0.92195445 * RANDN() AS `V268`, `S3` + 0.92195445 * RANDN() AS `V269`, `S2` + 0.94339811 * RANDN() AS `V270`, `S8` + 0.92195445 * RANDN() AS `V271`, `S2` + 0.94339811 * RANDN() AS `V272`, `S5` + 0.90553851 * RANDN() AS `V273`, `S4` + 0.93808315 * RANDN() AS `V274`, `S10` + 0.9486833 * RANDN() AS `V275`, `S3` + 0.92195445 * RANDN() AS `V276`, `S5` + 0.90553851 * RANDN() AS `V277`, `S6` + 0.92195445 * RANDN() AS `V278`, `S1` + 0.9 * RANDN() AS `V279`, `S10` + 0.9486833 * RANDN() AS `V280`, `S6` + 0.92195445 * RANDN() AS `V281`, `S9` + 0.92736185 * RANDN() AS `V282`, `S4` + 0.93808315 * RANDN() AS `V283`, `S5` + 0.90553851 * RANDN() AS `V284`, `S10` + 0.9486833 * RANDN() AS `V285`, `S1` + 0.9 * RANDN() AS `V286`, `S7` + 0.93808315 * RANDN() AS `V287`, `S8` + 0.92195445 * RANDN() AS `V288`, `S7` + 0.93808315 * RANDN() AS `V289`, `S8` + 0.92195445 * RANDN() AS `V290`, `S5` + 0.90553851 * RANDN() AS `V291`, `S8` + 0.92195445 * RANDN() AS `V292`, `S3` + 0.92195445 * RANDN() AS `V293`, `S8` + 0.92195445 * RANDN() AS `V294`, `S6` + 0.92195445 * RANDN() AS `V295`, `S5` + 0.90553851 * RANDN() AS `V296`, `S6` + 0.92195445 * RANDN() AS `V297`, `S4` + 0.93808315 * RANDN() AS `V298`, `S10` + 0.9486833 * RANDN() AS `V299`, `S7` + 0.93808315 * RANDN() AS `V300`, `S9` + 0.92736185 * RANDN() AS `V301`, `S9` + 0.92736185 * RANDN() AS `V302`, `S10` + 0.9486833 * RANDN() AS `V303`, `S1` + 0.9 * RANDN() AS `V304`, `S2` + 0.94339811 * RANDN() AS `V305`, `S9` + 0.92736185 * RANDN() AS `V306`, `S7` + 0.93808315 * RANDN() AS `V307`, `S6` + 0.92195445 * RANDN() AS `V308`, `S5` + 0.90553851 * RANDN() AS `V309`, `S7` + 0.93808315 * RANDN() AS `V310`, `S7` + 0.93808315 * RANDN() AS `V311`, `S9` + 0.92736185 * RANDN() AS `V312`, `S10` + 0.9486833 * RANDN() AS `V313`, `S2` + 0.94339811 * RANDN() AS `V314`, `S2` + 0.94339811 * RANDN() AS `V315`, `S4` + 0.93808315 * RANDN() AS `V316`, `S5` + 0.90553851 * RANDN() AS `V317`, `S9` + 0.92736185 * RANDN() AS `V318`, `S4` + 0.93808315 * RANDN() AS `V319`, `S1` + 0.9 * RANDN() AS `V320`, `S8` + 0.92195445 * RANDN() AS `V321`, `S1` + 0.9 * RANDN() AS `V322`, `S7` + 0.93808315 * RANDN() AS `V323`, `S7` + 0.93808315 * RANDN() AS `V324`, `S9` + 0.92736185 * RANDN() AS `V325`, `S10` + 0.9486833 * RANDN() AS `V326`, `S10` + 0.9486833 * RANDN() AS `V327`, `S4` + 0.93808315 * RANDN() AS `V328`, `S10` + 0.9486833 * RANDN() AS `V329`, `S2` + 0.94339811 * RANDN() AS `V330`, `S1` + 0.9 * RANDN() AS `V331`, `S2` + 0.94339811 * RANDN() AS `V332`, `S6` + 0.92195445 * RANDN() AS `V333`, `S5` + 0.90553851 * RANDN() AS `V334`, `S3` + 0.92195445 * RANDN() AS `V335`, `S5` + 0.90553851 * RANDN() AS `V336`, `S7` + 0.93808315 * RANDN() AS `V337`, `S7` + 0.93808315 * RANDN() AS `V338`, `S7` + 0.93808315 * RANDN() AS `V339`, `S9` + 0.92736185 * RANDN() AS `V340`, `S3` + 0.92195445 * RANDN() AS `V341`, `S2` + 0.94339811 * RANDN() AS `V342`, `S9` + 0.92736185 * RANDN() AS `V343`, `S6` + 0.92195445 * RANDN() AS `V344`, `S7` + 0.93808315 * RANDN() AS `V345`, `S5` + 0.90553851 * RANDN() AS `V346`, `S9` + 0.92736185 * RANDN() AS `V347`, `S10` + 0.9486833 * RANDN() AS `V348`, `S7` + 0.93808315 * RANDN() AS `V349`, `S5` + 0.90553851 * RANDN() AS `V350`, `S3` + 0.92195445 * RANDN() AS `V351`, `S3` + 0.92195445 * RANDN() AS `V352`, `S1` + 0.9 * RANDN() AS `V353`, `S1` + 0.9 * RANDN() AS `V354`, `S4` + 0.93808315 * RANDN() AS `V355`, `S9` + 0.92736185 * RANDN() AS `V356`, `S7` + 0.93808315 * RANDN() AS `V357`, `S10` + 0.9486833 * RANDN() AS `V358`, `S2` + 0.94339811 * RANDN() AS `V359`, `S1` + 0.9 * RANDN() AS `V360`, `S3` + 0.92195445 * RANDN() AS `V361`, `S1` + 0.9 * RANDN() AS `V362`, `S4` + 0.93808315 * RANDN() AS `V363`, `S8` + 0.92195445 * RANDN() AS `V364`, `S6` + 0.92195445 * RANDN() AS `V365`, `S10` + 0.9486833 * RANDN() AS `V366`, `S4` + 0.93808315 * RANDN() AS `V367`, `S5` + 0.90553851 * RANDN() AS `V368`, `S5` + 0.90553851 * RANDN() AS `V369`, `S8` + 0.92195445 * RANDN() AS `V370`, `S5` + 0.90553851 * RANDN() AS `V371`, `S6` + 0.92195445 * RANDN() AS `V372`, `S10` + 0.9486833 * RANDN() AS `V373`, `S10` + 0.9486833 * RANDN() AS `V374`, `S5` + 0.90553851 * RANDN() AS `V375`, `S10` + 0.9486833 * RANDN() AS `V376`, `S7` + 0.93808315 * RANDN() AS `V377`, `S2` + 0.94339811 * RANDN() AS `V378`, `S1` + 0.9 * RANDN() AS `V379`, `S8` + 0.92195445 * RANDN() AS `V380`, `S7` + 0.93808315 * RANDN() AS `V381`, `S2` + 0.94339811 * RANDN() AS `V382`, `S7` + 0.93808315 * RANDN() AS `V383`, `S10` + 0.9486833 * RANDN() AS `V384`, `S2` + 0.94339811 * RANDN() AS `V385`, `S9` + 0.92736185 * RANDN() AS `V386`, `S4` + 0.93808315 * RANDN() AS `V387`, `S4` + 0.93808315 * RANDN() AS `V388`, `S1` + 0.9 * RANDN() AS `V389`, `S6` + 0.92195445 * RANDN() AS `V390`, `S5` + 0.90553851 * RANDN() AS `V391`, `S10` + 0.9486833 * RANDN() AS `V392`, `S4` + 0.93808315 * RANDN() AS `V393`, `S9` + 0.92736185 * RANDN() AS `V394`, `S1` + 0.9 * RANDN() AS `V395`, `S3` + 0.92195445 * RANDN() AS `V396`, `S1` + 0.9 * RANDN() AS `V397`, `S6` + 0.92195445 * RANDN() AS `V398`, `S3` + 0.92195445 * RANDN() AS `V399`, `S2` + 0.94339811 * RANDN() AS `V400`, `S1` + 0.9 * RANDN() AS `V401`, `S9` + 0.92736185 * RANDN() AS `V402`, `S1` + 0.9 * RANDN() AS `V403`, `S1` + 0.9 * RANDN() AS `V404`, `S9` + 0.92736185 * RANDN() AS `V405`, `S7` + 0.93808315 * RANDN() AS `V406`, `S2` + 0.94339811 * RANDN() AS `V407`, `S2` + 0.94339811 * RANDN() AS `V408`, `S3` + 0.92195445 * RANDN() AS `V409`, `S5` + 0.90553851 * RANDN() AS `V410`, `S8` + 0.92195445 * RANDN() AS `V411`, `S10` + 0.9486833 * RANDN() AS `V412`, `S8` + 0.92195445 * RANDN() AS `V413`, `S7` + 0.93808315 * RANDN() AS `V414`, `S6` + 0.92195445 * RANDN() AS `V415`, `S3` + 0.92195445 * RANDN() AS `V416`, `S9` + 0.92736185 * RANDN() AS `V417`, `S9` + 0.92736185 * RANDN() AS `V418`, `S2` + 0.94339811 * RANDN() AS `V419`, `S3` + 0.92195445 * RANDN() AS `V420`, `S4` + 0.93808315 * RANDN() AS `V421`, `S4` + 0.93808315 * RANDN() AS `V422`, `S7` + 0.93808315 * RANDN() AS `V423`, `S7` + 0.93808315 * RANDN() AS `V424`, `S10` + 0.9486833 * RANDN() AS `V425`, `S8` + 0.92195445 * RANDN() AS `V426`, `S8` + 0.92195445 * RANDN() AS `V427`, `S6` + 0.92195445 * RANDN() AS `V428`, `S8` + 0.92195445 * RANDN() AS `V429`, `S9` + 0.92736185 * RANDN() AS `V430`, `S7` + 0.93808315 * RANDN() AS `V431`, `S5` + 0.90553851 * RANDN() AS `V432`, `S10` + 0.9486833 * RANDN() AS `V433`, `S3` + 0.92195445 * RANDN() AS `V434`, `S6` + 0.92195445 * RANDN() AS `V435`, `S4` + 0.93808315 * RANDN() AS `V436`, `S6` + 0.92195445 * RANDN() AS `V437`, `S2` + 0.94339811 * RANDN() AS `V438`, `S1` + 0.9 * RANDN() AS `V439`, `S8` + 0.92195445 * RANDN() AS `V440`, `S3` + 0.92195445 * RANDN() AS `V441`, `S3` + 0.92195445 * RANDN() AS `V442`, `S4` + 0.93808315 * RANDN() AS `V443`, `S10` + 0.9486833 * RANDN() AS `V444`, `S2` + 0.94339811 * RANDN() AS `V445`, `S1` + 0.9 * RANDN() AS `V446`, `S7` + 0.93808315 * RANDN() AS `V447`, `S8` + 0.92195445 * RANDN() AS `V448`, `S9` + 0.92736185 * RANDN() AS `V449`, `S10` + 0.9486833 * RANDN() AS `V450`, `S8` + 0.92195445 * RANDN() AS `V451`, `S8` + 0.92195445 * RANDN() AS `V452`, `S7` + 0.93808315 * RANDN() AS `V453`, `S1` + 0.9 * RANDN() AS `V454`, `S9` + 0.92736185 * RANDN() AS `V455`, `S9` + 0.92736185 * RANDN() AS `V456`, `S9` + 0.92736185 * RANDN() AS `V457`, `S2` + 0.94339811 * RANDN() AS `V458`, `S7` + 0.93808315 * RANDN() AS `V459`, `S5` + 0.90553851 * RANDN() AS `V460`, `S5` + 0.90553851 * RANDN() AS `V461`, `S5` + 0.90553851 * RANDN() AS `V462`, `S5` + 0.90553851 * RANDN() AS `V463`, `S2` + 0.94339811 * RANDN() AS `V464`, `S8` + 0.92195445 * RANDN() AS `V465`, `S3` + 0.92195445 * RANDN() AS `V466`, `S2` + 0.94339811 * RANDN() AS `V467`, `S6` + 0.92195445 * RANDN() AS `V468`, `S1` + 0.9 * RANDN() AS `V469`, `S9` + 0.92736185 * RANDN() AS `V470`, `S4` + 0.93808315 * RANDN() AS `V471`, `S4` + 0.93808315 * RANDN() AS `V472`, `S8` + 0.92195445 * RANDN() AS `V473`, `S3` + 0.92195445 * RANDN() AS `V474`, `S9` + 0.92736185 * RANDN() AS `V475`, `S5` + 0.90553851 * RANDN() AS `V476`, `S4` + 0.93808315 * RANDN() AS `V477`, `S9` + 0.92736185 * RANDN() AS `V478`, `S10` + 0.9486833 * RANDN() AS `V479`, `S7` + 0.93808315 * RANDN() AS `V480`, `S10` + 0.9486833 * RANDN() AS `V481`, `S7` + 0.93808315 * RANDN() AS `V482`, `S10` + 0.9486833 * RANDN() AS `V483`, `S6` + 0.92195445 * RANDN() AS `V484`, `S9` + 0.92736185 * RANDN() AS `V485`, `S10` + 0.9486833 * RANDN() AS `V486`, `S1` + 0.9 * RANDN() AS `V487`, `S10` + 0.9486833 * RANDN() AS `V488`, `S4` + 0.93808315 * RANDN() AS `V489`, `S8` + 0.92195445 * RANDN() AS `V490`, `S10` + 0.9486833 * RANDN() AS `V491`, `S2` + 0.94339811 * RANDN() AS `V492`, `S4` + 0.93808315 * RANDN() AS `V493`, `S2` + 0.94339811 * RANDN() AS `V494`, `S6` + 0.92195445 * RANDN() AS `V495`, `S5` + 0.90553851 * RANDN() AS `V496`, `S6` + 0.92195445 * RANDN() AS `V497`, `S6` + 0.92195445 * RANDN() AS `V498`, `S1` + 0.9 * RANDN() AS `V499`, `S3` + 0.92195445 * RANDN() AS `V500`, `S2` + 0.94339811 * RANDN() AS `V501`, `S10` + 0.9486833 * RANDN() AS `V502`, `S2` + 0.94339811 * RANDN() AS `V503`, `S3` + 0.92195445 * RANDN() AS `V504`, `S2` + 0.94339811 * RANDN() AS `V505`, `S9` + 0.92736185 * RANDN() AS `V506`, `S7` + 0.93808315 * RANDN() AS `V507`, `S1` + 0.9 * RANDN() AS `V508`, `S1` + 0.9 * RANDN() AS `V509`, `S2` + 0.94339811 * RANDN() AS `V510`, `S9` + 0.92736185 * RANDN() AS `V511`, `S8` + 0.92195445 * RANDN() AS `V512`, `S4` + 0.93808315 * RANDN() AS `V513`, `S6` + 0.92195445 * RANDN() AS `V514`, `S10` + 0.9486833 * RANDN() AS `V515`, `S1` + 0.9 * RANDN() AS `V516`, `S3` + 0.92195445 * RANDN() AS `V517`, `S7` + 0.93808315 * RANDN() AS `V518`, `S2` + 0.94339811 * RANDN() AS `V519`, `S7` + 0.93808315 * RANDN() AS `V520`, `S4` + 0.93808315 * RANDN() AS `V521`, `S8` + 0.92195445 * RANDN() AS `V522`, `S1` + 0.9 * RANDN() AS `V523`, `S9` + 0.92736185 * RANDN() AS `V524`, `S1` + 0.9 * RANDN() AS `V525`, `S3` + 0.92195445 * RANDN() AS `V526`, `S8` + 0.92195445 * RANDN() AS `V527`, `S5` + 0.90553851 * RANDN() AS `V528`, `S6` + 0.92195445 * RANDN() AS `V529`, `S4` + 0.93808315 * RANDN() AS `V530`, `S4` + 0.93808315 * RANDN() AS `V531`, `S9` + 0.92736185 * RANDN() AS `V532`, `S7` + 0.93808315 * RANDN() AS `V533`, `S2` + 0.94339811 * RANDN() AS `V534`, `S2` + 0.94339811 * RANDN() AS `V535`, `S7` + 0.93808315 * RANDN() AS `V536`, `S9` + 0.92736185 * RANDN() AS `V537`, `S2` + 0.94339811 * RANDN() AS `V538`, `S8` + 0.92195445 * RANDN() AS `V539`, `S7` + 0.93808315 * RANDN() AS `V540`, `S10` + 0.9486833 * RANDN() AS `V541`, `S9` + 0.92736185 * RANDN() AS `V542`, `S6` + 0.92195445 * RANDN() AS `V543`, `S3` + 0.92195445 * RANDN() AS `V544`, `S7` + 0.93808315 * RANDN() AS `V545`, `S8` + 0.92195445 * RANDN() AS `V546`, `S9` + 0.92736185 * RANDN() AS `V547`, `S2` + 0.94339811 * RANDN() AS `V548`, `S2` + 0.94339811 * RANDN() AS `V549`, `S2` + 0.94339811 * RANDN() AS `V550`, `S6` + 0.92195445 * RANDN() AS `V551`, `S8` + 0.92195445 * RANDN() AS `V552`, `S7` + 0.93808315 * RANDN() AS `V553`, `S9` + 0.92736185 * RANDN() AS `V554`, `S8` + 0.92195445 * RANDN() AS `V555`, `S3` + 0.92195445 * RANDN() AS `V556`, `S3` + 0.92195445 * RANDN() AS `V557`, `S8` + 0.92195445 * RANDN() AS `V558`, `S5` + 0.90553851 * RANDN() AS `V559`, `S1` + 0.9 * RANDN() AS `V560`, `S7` + 0.93808315 * RANDN() AS `V561`, `S2` + 0.94339811 * RANDN() AS `V562`, `S1` + 0.9 * RANDN() AS `V563`, `S6` + 0.92195445 * RANDN() AS `V564`, `S3` + 0.92195445 * RANDN() AS `V565`, `S10` + 0.9486833 * RANDN() AS `V566`, `S4` + 0.93808315 * RANDN() AS `V567`, `S7` + 0.93808315 * RANDN() AS `V568`, `S1` + 0.9 * RANDN() AS `V569`, `S10` + 0.9486833 * RANDN() AS `V570`, `S1` + 0.9 * RANDN() AS `V571`, `S2` + 0.94339811 * RANDN() AS `V572`, `S9` + 0.92736185 * RANDN() AS `V573`, `S8` + 0.92195445 * RANDN() AS `V574`, `S9` + 0.92736185 * RANDN() AS `V575`, `S10` + 0.9486833 * RANDN() AS `V576`, `S8` + 0.92195445 * RANDN() AS `V577`, `S3` + 0.92195445 * RANDN() AS `V578`, `S9` + 0.92736185 * RANDN() AS `V579`, `S7` + 0.93808315 * RANDN() AS `V580`, `S3` + 0.92195445 * RANDN() AS `V581`, `S5` + 0.90553851 * RANDN() AS `V582`, `S1` + 0.9 * RANDN() AS `V583`, `S10` + 0.9486833 * RANDN() AS `V584`, `S9` + 0.92736185 * RANDN() AS `V585`, `S7` + 0.93808315 * RANDN() AS `V586`, `S3` + 0.92195445 * RANDN() AS `V587`, `S1` + 0.9 * RANDN() AS `V588`, `S9` + 0.92736185 * RANDN() AS `V589`, `S5` + 0.90553851 * RANDN() AS `V590`, `S2` + 0.94339811 * RANDN() AS `V591`, `S6` + 0.92195445 * RANDN() AS `V592`, `S3` + 0.92195445 * RANDN() AS `V593`, `S5` + 0.90553851 * RANDN() AS `V594`, `S8` + 0.92195445 * RANDN() AS `V595`, `S8` + 0.92195445 * RANDN() AS `V596`, `S5` + 0.90553851 * RANDN() AS `V597`, `S9` + 0.92736185 * RANDN() AS `V598`, `S4` + 0.93808315 * RANDN() AS `V599`, `S7` + 0.93808315 * RANDN() AS `V600`, `S9` + 0.92736185 * RANDN() AS `V601`, `S3` + 0.92195445 * RANDN() AS `V602`, `S5` + 0.90553851 * RANDN() AS `V603`, `S9` + 0.92736185 * RANDN() AS `V604`, `S6` + 0.92195445 * RANDN() AS `V605`, `S10` + 0.9486833 * RANDN() AS `V606`, `S1` + 0.9 * RANDN() AS `V607`, `S2` + 0.94339811 * RANDN() AS `V608`, `S2` + 0.94339811 * RANDN() AS `V609`, `S3` + 0.92195445 * RANDN() AS `V610`, `S1` + 0.9 * RANDN() AS `V611`, `S7` + 0.93808315 * RANDN() AS `V612`, `S4` + 0.93808315 * RANDN() AS `V613`, `S4` + 0.93808315 * RANDN() AS `V614`, `S7` + 0.93808315 * RANDN() AS `V615`, `S2` + 0.94339811 * RANDN() AS `V616`, `S3` + 0.92195445 * RANDN() AS `V617`, `S4` + 0.93808315 * RANDN() AS `V618`, `S1` + 0.9 * RANDN() AS `V619`, `S5` + 0.90553851 * RANDN() AS `V620`, `S1` + 0.9 * RANDN() AS `V621`, `S10` + 0.9486833 * RANDN() AS `V622`, `S4` + 0.93808315 * RANDN() AS `V623`, `S7` + 0.93808315 * RANDN() AS `V624`, `S7` + 0.93808315 * RANDN() AS `V625`, `S4` + 0.93808315 * RANDN() AS `V626`, `S8` + 0.92195445 * RANDN() AS `V627`, `S6` + 0.92195445 * RANDN() AS `V628`, `S7` + 0.93808315 * RANDN() AS `V629`, `S1` + 0.9 * RANDN() AS `V630`, `S5` + 0.90553851 * RANDN() AS `V631`, `S3` + 0.92195445 * RANDN() AS `V632`, `S5` + 0.90553851 * RANDN() AS `V633`, `S6` + 0.92195445 * RANDN() AS `V634`, `S4` + 0.93808315 * RANDN() AS `V635`, `S6` + 0.92195445 * RANDN() AS `V636`, `S1` + 0.9 * RANDN() AS `V637`, `S10` + 0.9486833 * RANDN() AS `V638`, `S5` + 0.90553851 * RANDN() AS `V639`, `S3` + 0.92195445 * RANDN() AS `V640`, `S10` + 0.9486833 * RANDN() AS `V641`, `S6` + 0.92195445 * RANDN() AS `V642`, `S10` + 0.9486833 * RANDN() AS `V643`, `S1` + 0.9 * RANDN() AS `V644`, `S3` + 0.92195445 * RANDN() AS `V645`, `S3` + 0.92195445 * RANDN() AS `V646`, `S6` + 0.92195445 * RANDN() AS `V647`, `S7` + 0.93808315 * RANDN() AS `V648`, `S6` + 0.92195445 * RANDN() AS `V649`, `S10` + 0.9486833 * RANDN() AS `V650`, `S3` + 0.92195445 * RANDN() AS `V651`, `S5` + 0.90553851 * RANDN() AS `V652`, `S10` + 0.9486833 * RANDN() AS `V653`, `S3` + 0.92195445 * RANDN() AS `V654`, `S6` + 0.92195445 * RANDN() AS `V655`, `S4` + 0.93808315 * RANDN() AS `V656`, `S6` + 0.92195445 * RANDN() AS `V657`, `S4` + 0.93808315 * RANDN() AS `V658`, `S5` + 0.90553851 * RANDN() AS `V659`, `S8` + 0.92195445 * RANDN() AS `V660`, `S3` + 0.92195445 * RANDN() AS `V661`, `S2` + 0.94339811 * RANDN() AS `V662`, `S7` + 0.93808315 * RANDN() AS `V663`, `S5` + 0.90553851 * RANDN() AS `V664`, `S3` + 0.92195445 * RANDN() AS `V665`, `S6` + 0.92195445 * RANDN() AS `V666`, `S6` + 0.92195445 * RANDN() AS `V667`, `S7` + 0.93808315 * RANDN() AS `V668`, `S2` + 0.94339811 * RANDN() AS `V669`, `S7` + 0.93808315 * RANDN() AS `V670`, `S9` + 0.92736185 * RANDN() AS `V671`, `S4` + 0.93808315 * RANDN() AS `V672`, `S1` + 0.9 * RANDN() AS `V673`, `S7` + 0.93808315 * RANDN() AS `V674`, `S6` + 0.92195445 * RANDN() AS `V675`, `S8` + 0.92195445 * RANDN() AS `V676`, `S1` + 0.9 * RANDN() AS `V677`, `S2` + 0.94339811 * RANDN() AS `V678`, `S7` + 0.93808315 * RANDN() AS `V679`, `S5` + 0.90553851 * RANDN() AS `V680`, `S6` + 0.92195445 * RANDN() AS `V681`, `S7` + 0.93808315 * RANDN() AS `V682`, `S7` + 0.93808315 * RANDN() AS `V683`, `S6` + 0.92195445 * RANDN() AS `V684`, `S4` + 0.93808315 * RANDN() AS `V685`, `S2` + 0.94339811 * RANDN() AS `V686`, `S3` + 0.92195445 * RANDN() AS `V687`, `S2` + 0.94339811 * RANDN() AS `V688`, `S1` + 0.9 * RANDN() AS `V689`, `S1` + 0.9 * RANDN() AS `V690`, `S4` + 0.93808315 * RANDN() AS `V691`, `S6` + 0.92195445 * RANDN() AS `V692`, `S7` + 0.93808315 * RANDN() AS `V693`, `S8` + 0.92195445 * RANDN() AS `V694`, `S6` + 0.92195445 * RANDN() AS `V695`, `S7` + 0.93808315 * RANDN() AS `V696`, `S9` + 0.92736185 * RANDN() AS `V697`, `S1` + 0.9 * RANDN() AS `V698`, `S3` + 0.92195445 * RANDN() AS `V699`, `S6` + 0.92195445 * RANDN() AS `V700`, `S10` + 0.9486833 * RANDN() AS `V701`, `S4` + 0.93808315 * RANDN() AS `V702`, `S4` + 0.93808315 * RANDN() AS `V703`, `S6` + 0.92195445 * RANDN() AS `V704`, `S8` + 0.92195445 * RANDN() AS `V705`, `S3` + 0.92195445 * RANDN() AS `V706`, `S9` + 0.92736185 * RANDN() AS `V707`, `S6` + 0.92195445 * RANDN() AS `V708`, `S5` + 0.90553851 * RANDN() AS `V709`, `S9` + 0.92736185 * RANDN() AS `V710`, `S10` + 0.9486833 * RANDN() AS `V711`, `S9` + 0.92736185 * RANDN() AS `V712`, `S6` + 0.92195445 * RANDN() AS `V713`, `S7` + 0.93808315 * RANDN() AS `V714`, `S3` + 0.92195445 * RANDN() AS `V715`, `S4` + 0.93808315 * RANDN() AS `V716`, `S3` + 0.92195445 * RANDN() AS `V717`, `S10` + 0.9486833 * RANDN() AS `V718`, `S8` + 0.92195445 * RANDN() AS `V719`, `S7` + 0.93808315 * RANDN() AS `V720`, `S4` + 0.93808315 * RANDN() AS `V721`, `S2` + 0.94339811 * RANDN() AS `V722`, `S7` + 0.93808315 * RANDN() AS `V723`, `S7` + 0.93808315 * RANDN() AS `V724`, `S6` + 0.92195445 * RANDN() AS `V725`, `S10` + 0.9486833 * RANDN() AS `V726`, `S10` + 0.9486833 * RANDN() AS `V727`, `S3` + 0.92195445 * RANDN() AS `V728`, `S8` + 0.92195445 * RANDN() AS `V729`, `S7` + 0.93808315 * RANDN() AS `V730`, `S3` + 0.92195445 * RANDN() AS `V731`, `S4` + 0.93808315 * RANDN() AS `V732`, `S3` + 0.92195445 * RANDN() AS `V733`, `S2` + 0.94339811 * RANDN() AS `V734`, `S7` + 0.93808315 * RANDN() AS `V735`, `S5` + 0.90553851 * RANDN() AS `V736`, `S10` + 0.9486833 * RANDN() AS `V737`, `S7` + 0.93808315 * RANDN() AS `V738`, `S7` + 0.93808315 * RANDN() AS `V739`, `S9` + 0.92736185 * RANDN() AS `V740`, `S3` + 0.92195445 * RANDN() AS `V741`, `S10` + 0.9486833 * RANDN() AS `V742`, `S3` + 0.92195445 * RANDN() AS `V743`, `S6` + 0.92195445 * RANDN() AS `V744`, `S10` + 0.9486833 * RANDN() AS `V745`, `S8` + 0.92195445 * RANDN() AS `V746`, `S1` + 0.9 * RANDN() AS `V747`, `S7` + 0.93808315 * RANDN() AS `V748`, `S8` + 0.92195445 * RANDN() AS `V749`, `S7` + 0.93808315 * RANDN() AS `V750`, `S6` + 0.92195445 * RANDN() AS `V751`, `S7` + 0.93808315 * RANDN() AS `V752`, `S8` + 0.92195445 * RANDN() AS `V753`, `S4` + 0.93808315 * RANDN() AS `V754`, `S2` + 0.94339811 * RANDN() AS `V755`, `S8` + 0.92195445 * RANDN() AS `V756`, `S8` + 0.92195445 * RANDN() AS `V757`, `S9` + 0.92736185 * RANDN() AS `V758`, `S5` + 0.90553851 * RANDN() AS `V759`, `S10` + 0.9486833 * RANDN() AS `V760`, `S2` + 0.94339811 * RANDN() AS `V761`, `S7` + 0.93808315 * RANDN() AS `V762`, `S7` + 0.93808315 * RANDN() AS `V763`, `S5` + 0.90553851 * RANDN() AS `V764`, `S6` + 0.92195445 * RANDN() AS `V765`, `S7` + 0.93808315 * RANDN() AS `V766`, `S8` + 0.92195445 * RANDN() AS `V767`, `S1` + 0.9 * RANDN() AS `V768`, `S2` + 0.94339811 * RANDN() AS `V769`, `S9` + 0.92736185 * RANDN() AS `V770`, `S4` + 0.93808315 * RANDN() AS `V771`, `S1` + 0.9 * RANDN() AS `V772`, `S3` + 0.92195445 * RANDN() AS `V773`, `S3` + 0.92195445 * RANDN() AS `V774`, `S3` + 0.92195445 * RANDN() AS `V775`, `S4` + 0.93808315 * RANDN() AS `V776`, `S5` + 0.90553851 * RANDN() AS `V777`, `S10` + 0.9486833 * RANDN() AS `V778`, `S4` + 0.93808315 * RANDN() AS `V779`, `S5` + 0.90553851 * RANDN() AS `V780`, `S3` + 0.92195445 * RANDN() AS `V781`, `S4` + 0.93808315 * RANDN() AS `V782`, `S10` + 0.9486833 * RANDN() AS `V783`, `S8` + 0.92195445 * RANDN() AS `V784`, `S3` + 0.92195445 * RANDN() AS `V785`, `S2` + 0.94339811 * RANDN() AS `V786`, `S8` + 0.92195445 * RANDN() AS `V787`, `S9` + 0.92736185 * RANDN() AS `V788`, `S9` + 0.92736185 * RANDN() AS `V789`, `S1` + 0.9 * RANDN() AS `V790`, `S8` + 0.92195445 * RANDN() AS `V791`, `S2` + 0.94339811 * RANDN() AS `V792`, `S10` + 0.9486833 * RANDN() AS `V793`, `S5` + 0.90553851 * RANDN() AS `V794`, `S1` + 0.9 * RANDN() AS `V795`, `S4` + 0.93808315 * RANDN() AS `V796`, `S6` + 0.92195445 * RANDN() AS `V797`, `S5` + 0.90553851 * RANDN() AS `V798`, `S7` + 0.93808315 * RANDN() AS `V799`, `S10` + 0.9486833 * RANDN() AS `V800`, `S9` + 0.92736185 * RANDN() AS `V801`, `S2` + 0.94339811 * RANDN() AS `V802`, `S2` + 0.94339811 * RANDN() AS `V803`, `S9` + 0.92736185 * RANDN() AS `V804`, `S9` + 0.92736185 * RANDN() AS `V805`, `S2` + 0.94339811 * RANDN() AS `V806`, `S7` + 0.93808315 * RANDN() AS `V807`, `S5` + 0.90553851 * RANDN() AS `V808`, `S6` + 0.92195445 * RANDN() AS `V809`, `S5` + 0.90553851 * RANDN() AS `V810`, `S8` + 0.92195445 * RANDN() AS `V811`, `S8` + 0.92195445 * RANDN() AS `V812`, `S4` + 0.93808315 * RANDN() AS `V813`, `S4` + 0.93808315 * RANDN() AS `V814`, `S8` + 0.92195445 * RANDN() AS `V815`, `S8` + 0.92195445 * RANDN() AS `V816`, `S4` + 0.93808315 * RANDN() AS `V817`, `S10` + 0.9486833 * RANDN() AS `V818`, `S3` + 0.92195445 * RANDN() AS `V819`, `S8` + 0.92195445 * RANDN() AS `V820`, `S5` + 0.90553851 * RANDN() AS `V821`, `S7` + 0.93808315 * RANDN() AS `V822`, `S5` + 0.90553851 * RANDN() AS `V823`, `S6` + 0.92195445 * RANDN() AS `V824`, `S1` + 0.9 * RANDN() AS `V825`, `S7` + 0.93808315 * RANDN() AS `V826`, `S8` + 0.92195445 * RANDN() AS `V827`, `S2` + 0.94339811 * RANDN() AS `V828`, `S4` + 0.93808315 * RANDN() AS `V829`, `S10` + 0.9486833 * RANDN() AS `V830`, `S2` + 0.94339811 * RANDN() AS `V831`, `S2` + 0.94339811 * RANDN() AS `V832`, `S2` + 0.94339811 * RANDN() AS `V833`, `S4` + 0.93808315 * RANDN() AS `V834`, `S8` + 0.92195445 * RANDN() AS `V835`, `S2` + 0.94339811 * RANDN() AS `V836`, `S5` + 0.90553851 * RANDN() AS `V837`, `S7` + 0.93808315 * RANDN() AS `V838`, `S5` + 0.90553851 * RANDN() AS `V839`, `S3` + 0.92195445 * RANDN() AS `V840`, `S1` + 0.9 * RANDN() AS `V841`, `S9` + 0.92736185 * RANDN() AS `V842`, `S8` + 0.92195445 * RANDN() AS `V843`, `S3` + 0.92195445 * RANDN() AS `V844`, `S10` + 0.9486833 * RANDN() AS `V845`, `S7` + 0.93808315 * RANDN() AS `V846`, `S1` + 0.9 * RANDN() AS `V847`, `S8` + 0.92195445 * RANDN() AS `V848`, `S6` + 0.92195445 * RANDN() AS `V849`, `S7` + 0.93808315 * RANDN() AS `V850`, `S2` + 0.94339811 * RANDN() AS `V851`, `S7` + 0.93808315 * RANDN() AS `V852`, `S8` + 0.92195445 * RANDN() AS `V853`, `S5` + 0.90553851 * RANDN() AS `V854`, `S1` + 0.9 * RANDN() AS `V855`, `S5` + 0.90553851 * RANDN() AS `V856`, `S4` + 0.93808315 * RANDN() AS `V857`, `S9` + 0.92736185 * RANDN() AS `V858`, `S3` + 0.92195445 * RANDN() AS `V859`, `S1` + 0.9 * RANDN() AS `V860`, `S7` + 0.93808315 * RANDN() AS `V861`, `S6` + 0.92195445 * RANDN() AS `V862`, `S8` + 0.92195445 * RANDN() AS `V863`, `S9` + 0.92736185 * RANDN() AS `V864`, `S5` + 0.90553851 * RANDN() AS `V865`, `S8` + 0.92195445 * RANDN() AS `V866`, `S6` + 0.92195445 * RANDN() AS `V867`, `S5` + 0.90553851 * RANDN() AS `V868`, `S7` + 0.93808315 * RANDN() AS `V869`, `S5` + 0.90553851 * RANDN() AS `V870`, `S7` + 0.93808315 * RANDN() AS `V871`, `S3` + 0.92195445 * RANDN() AS `V872`, `S7` + 0.93808315 * RANDN() AS `V873`, `S1` + 0.9 * RANDN() AS `V874`, `S2` + 0.94339811 * RANDN() AS `V875`, `S6` + 0.92195445 * RANDN() AS `V876`, `S5` + 0.90553851 * RANDN() AS `V877`, `S3` + 0.92195445 * RANDN() AS `V878`, `S7` + 0.93808315 * RANDN() AS `V879`, `S6` + 0.92195445 * RANDN() AS `V880`, `S2` + 0.94339811 * RANDN() AS `V881`, `S1` + 0.9 * RANDN() AS `V882`, `S4` + 0.93808315 * RANDN() AS `V883`, `S3` + 0.92195445 * RANDN() AS `V884`, `S3` + 0.92195445 * RANDN() AS `V885`, `S8` + 0.92195445 * RANDN() AS `V886`, `S7` + 0.93808315 * RANDN() AS `V887`, `S5` + 0.90553851 * RANDN() AS `V888`, `S3` + 0.92195445 * RANDN() AS `V889`, `S7` + 0.93808315 * RANDN() AS `V890`, `S2` + 0.94339811 * RANDN() AS `V891`, `S3` + 0.92195445 * RANDN() AS `V892`, `S2` + 0.94339811 * RANDN() AS `V893`, `S6` + 0.92195445 * RANDN() AS `V894`, `S10` + 0.9486833 * RANDN() AS `V895`, `S3` + 0.92195445 * RANDN() AS `V896`, `S10` + 0.9486833 * RANDN() AS `V897`, `S4` + 0.93808315 * RANDN() AS `V898`, `S10` + 0.9486833 * RANDN() AS `V899`, `S7` + 0.93808315 * RANDN() AS `V900`, `S3` + 0.92195445 * RANDN() AS `V901`, `S6` + 0.92195445 * RANDN() AS `V902`, `S9` + 0.92736185 * RANDN() AS `V903`, `S7` + 0.93808315 * RANDN() AS `V904`, `S5` + 0.90553851 * RANDN() AS `V905`, `S9` + 0.92736185 * RANDN() AS `V906`, `S8` + 0.92195445 * RANDN() AS `V907`, `S9` + 0.92736185 * RANDN() AS `V908`, `S1` + 0.9 * RANDN() AS `V909`, `S4` + 0.93808315 * RANDN() AS `V910`, `S9` + 0.92736185 * RANDN() AS `V911`, `S10` + 0.9486833 * RANDN() AS `V912`, `S9` + 0.92736185 * RANDN() AS `V913`, `S2` + 0.94339811 * RANDN() AS `V914`, `S1` + 0.9 * RANDN() AS `V915`, `S9` + 0.92736185 * RANDN() AS `V916`, `S6` + 0.92195445 * RANDN() AS `V917`, `S5` + 0.90553851 * RANDN() AS `V918`, `S7` + 0.93808315 * RANDN() AS `V919`, `S10` + 0.9486833 * RANDN() AS `V920`, `S10` + 0.9486833 * RANDN() AS `V921`, `S3` + 0.92195445 * RANDN() AS `V922`, `S1` + 0.9 * RANDN() AS `V923`, `S1` + 0.9 * RANDN() AS `V924`, `S5` + 0.90553851 * RANDN() AS `V925`, `S6` + 0.92195445 * RANDN() AS `V926`, `S4` + 0.93808315 * RANDN() AS `V927`, `S2` + 0.94339811 * RANDN() AS `V928`, `S8` + 0.92195445 * RANDN() AS `V929`, `S8` + 0.92195445 * RANDN() AS `V930`, `S10` + 0.9486833 * RANDN() AS `V931`, `S10` + 0.9486833 * RANDN() AS `V932`, `S6` + 0.92195445 * RANDN() AS `V933`, `S9` + 0.92736185 * RANDN() AS `V934`, `S8` + 0.92195445 * RANDN() AS `V935`, `S3` + 0.92195445 * RANDN() AS `V936`, `S3` + 0.92195445 * RANDN() AS `V937`, `S1` + 0.9 * RANDN() AS `V938`, `S7` + 0.93808315 * RANDN() AS `V939`, `S6` + 0.92195445 * RANDN() AS `V940`, `S5` + 0.90553851 * RANDN() AS `V941`, `S5` + 0.90553851 * RANDN() AS `V942`, `S10` + 0.9486833 * RANDN() AS `V943`, `S6` + 0.92195445 * RANDN() AS `V944`, `S9` + 0.92736185 * RANDN() AS `V945`, `S7` + 0.93808315 * RANDN() AS `V946`, `S9` + 0.92736185 * RANDN() AS `V947`, `S7` + 0.93808315 * RANDN() AS `V948`, `S6` + 0.92195445 * RANDN() AS `V949`, `S4` + 0.93808315 * RANDN() AS `V950`, `S8` + 0.92195445 * RANDN() AS `V951`, `S2` + 0.94339811 * RANDN() AS `V952`, `S2` + 0.94339811 * RANDN() AS `V953`, `S2` + 0.94339811 * RANDN() AS `V954`, `S6` + 0.92195445 * RANDN() AS `V955`, `S9` + 0.92736185 * RANDN() AS `V956`, `S4` + 0.93808315 * RANDN() AS `V957`, `S10` + 0.9486833 * RANDN() AS `V958`, `S7` + 0.93808315 * RANDN() AS `V959`, `S2` + 0.94339811 * RANDN() AS `V960`, `S9` + 0.92736185 * RANDN() AS `V961`, `S9` + 0.92736185 * RANDN() AS `V962`, `S4` + 0.93808315 * RANDN() AS `V963`, `S2` + 0.94339811 * RANDN() AS `V964`, `S7` + 0.93808315 * RANDN() AS `V965`, `S6` + 0.92195445 * RANDN() AS `V966`, `S1` + 0.9 * RANDN() AS `V967`, `S6` + 0.92195445 * RANDN() AS `V968`, `S8` + 0.92195445 * RANDN() AS `V969`, `S8` + 0.92195445 * RANDN() AS `V970`, `S1` + 0.9 * RANDN() AS `V971`, `S7` + 0.93808315 * RANDN() AS `V972`, `S3` + 0.92195445 * RANDN() AS `V973`, `S3` + 0.92195445 * RANDN() AS `V974`, `S2` + 0.94339811 * RANDN() AS `V975`, `S6` + 0.92195445 * RANDN() AS `V976`, `S9` + 0.92736185 * RANDN() AS `V977`, `S8` + 0.92195445 * RANDN() AS `V978`, `S10` + 0.9486833 * RANDN() AS `V979`, `S7` + 0.93808315 * RANDN() AS `V980`, `S6` + 0.92195445 * RANDN() AS `V981`, `S1` + 0.9 * RANDN() AS `V982`, `S7` + 0.93808315 * RANDN() AS `V983`, `S2` + 0.94339811 * RANDN() AS `V984`, `S3` + 0.92195445 * RANDN() AS `V985`, `S7` + 0.93808315 * RANDN() AS `V986`, `S9` + 0.92736185 * RANDN() AS `V987`, `S3` + 0.92195445 * RANDN() AS `V988`, `S3` + 0.92195445 * RANDN() AS `V989`, `S8` + 0.92195445 * RANDN() AS `V990`, `S1` + 0.9 * RANDN() AS `V991`, `S4` + 0.93808315 * RANDN() AS `V992`, `S2` + 0.94339811 * RANDN() AS `V993`, `S10` + 0.9486833 * RANDN() AS `V994`, `S6` + 0.92195445 * RANDN() AS `V995`, `S6` + 0.92195445 * RANDN() AS `V996`, `S5` + 0.90553851 * RANDN() AS `V997`, `S5` + 0.90553851 * RANDN() AS `V998`, `S1` + 0.9 * RANDN() AS `V999`, `S8` + 0.92195445 * RANDN() AS `V1000`
FROM `analyis_tbl`) `yopcweupsu`
17/12/20 20:48:42 INFO ContextCleaner: Cleaned accumulator 58
17/12/20 20:48:42 INFO ContextCleaner: Cleaned accumulator 59
17/12/20 20:48:42 INFO ContextCleaner: Cleaned accumulator 60
17/12/20 20:48:42 INFO ContextCleaner: Cleaned accumulator 61
17/12/20 20:48:42 INFO ContextCleaner: Cleaned accumulator 62
17/12/20 20:48:42 INFO ContextCleaner: Cleaned accumulator 63
17/12/20 20:48:42 INFO ContextCleaner: Cleaned accumulator 64
17/12/20 20:48:42 INFO ContextCleaner: Cleaned accumulator 65
17/12/20 20:48:42 INFO ContextCleaner: Cleaned accumulator 66
17/12/20 20:48:42 INFO ContextCleaner: Cleaned accumulator 67
17/12/20 20:48:42 INFO ContextCleaner: Cleaned accumulator 68
17/12/20 20:48:42 INFO ContextCleaner: Cleaned accumulator 69
17/12/20 20:48:42 INFO ContextCleaner: Cleaned shuffle 1
17/12/20 20:48:43 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:57657 in memory (size: 7.5 KB, free: 1996.9 MB)
17/12/20 20:48:43 INFO ContextCleaner: Cleaned accumulator 51
17/12/20 20:48:43 INFO ContextCleaner: Cleaned accumulator 50
17/12/20 20:48:43 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:57657 in memory (size: 4.6 KB, free: 1996.9 MB)
17/12/20 20:48:43 INFO ContextCleaner: Cleaned accumulator 1
17/12/20 20:48:43 INFO ContextCleaner: Cleaned accumulator 0
17/12/20 20:48:43 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:57657 in memory (size: 6.9 KB, free: 1996.9 MB)
17/12/20 20:48:43 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:57657 in memory (size: 3.7 KB, free: 1996.9 MB)
17/12/20 20:48:43 INFO ContextCleaner: Cleaned accumulator 310
17/12/20 20:48:43 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:57657 in memory (size: 6.9 KB, free: 1996.9 MB)
17/12/20 20:48:43 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 127.0.0.1:57657 in memory (size: 3.7 KB, free: 1996.9 MB)
17/12/20 20:48:44 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 20:48:44 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:48:44 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz2`
WHERE (0 = 1)
17/12/20 20:48:44 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 20:48:44 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 20:48:45 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 20:48:45 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 157 bytes
17/12/20 20:48:45 INFO DAGScheduler: Got job 3 (take at <unknown>:0) with 1 output partitions
17/12/20 20:48:45 INFO DAGScheduler: Final stage: ResultStage 8 (take at <unknown>:0)
17/12/20 20:48:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
17/12/20 20:48:45 INFO DAGScheduler: Missing parents: List()
17/12/20 20:48:45 INFO DAGScheduler: Submitting ResultStage 8 (WorkerRDD[31] at RDD at rdd.scala:18), which has no missing parents
17/12/20 20:48:45 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 8.2 MB, free 1988.5 MB)
17/12/20 20:48:45 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.0 MB, free 1984.5 MB)
17/12/20 20:48:45 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1992.9 MB)
17/12/20 20:48:45 INFO MemoryStore: Block broadcast_7_piece1 stored as bytes in memory (estimated size 3.8 MB, free 1980.7 MB)
17/12/20 20:48:45 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 20:48:45 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:996
17/12/20 20:48:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (WorkerRDD[31] at RDD at rdd.scala:18)
17/12/20 20:48:45 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/12/20 20:48:45 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 5917 bytes)
17/12/20 20:48:45 INFO Executor: Running task 0.0 in stage 8.0 (TID 11)
17/12/20 20:48:45 INFO BlockManager: Found block rdd_12_0 locally
17/12/20 20:48:45 INFO CodeGenerator: Code generated in 23.129474 ms
17/12/20 20:48:46 INFO CodeGenerator: Code generated in 515.73021 ms
17/12/20 20:48:46 INFO CodeGenerator: Code generated in 127.482264 ms
17/12/20 21:22:38 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 163729 ms exceeds timeout 120000 ms
17/12/20 21:22:38 ERROR TaskSchedulerImpl: Lost executor driver on localhost: Executor heartbeat timed out after 163729 ms
17/12/20 21:22:39 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 11, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 163729 ms
17/12/20 21:22:39 ERROR TaskSetManager: Task 0 in stage 8.0 failed 1 times; aborting job
17/12/20 21:22:39 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/12/20 21:22:39 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:39 INFO TaskSchedulerImpl: Cancelling stage 8
17/12/20 21:22:40 INFO DAGScheduler: ResultStage 8 (take at <unknown>:0) failed in 2034.132 s due to Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 11, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 163729 ms
Driver stacktrace:
17/12/20 21:22:40 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:40 INFO DAGScheduler: Job 3 failed: take at <unknown>:0, took 2034.365348 s
17/12/20 21:22:40 INFO DAGScheduler: Executor lost: driver (epoch 3)
17/12/20 21:22:40 INFO BlockManagerMasterEndpoint: Trying to remove executor driver from BlockManagerMaster.
17/12/20 21:22:40 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 WARN SparkContext: Killing executors is only supported in coarse-grained mode
17/12/20 21:22:40 INFO BlockManagerMaster: Removed driver successfully in removeExecutor
17/12/20 21:22:40 ERROR BlockManager: Failed to report broadcast_7_piece1 to master; giving up.
17/12/20 21:22:40 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:40 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:57657 with 2004.6 MB RAM, BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 2000.8 MB)
17/12/20 21:22:40 INFO DAGScheduler: Shuffle files lost for executor: driver (epoch 3)
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1996.8 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1993.0 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1992.9 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:40 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:40 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:40 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:40 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:40 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:40 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:40 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:40 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:40 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:40 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:40 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO DAGScheduler: Host added was in lost list earlier: localhost
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:41 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:41 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:41 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:41 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:41 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:41 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:41 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:41 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:41 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:41 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:41 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:41 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:41 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:41 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:41 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:41 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:41 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:42 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:42 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:42 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:42 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:42 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:42 INFO BlockManager: Reporting 8 blocks to the master.
17/12/20 21:22:42 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:42 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:42 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:42 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:43 WARN BlockManager: Putting block rdd_31_0 failed due to an exception
17/12/20 21:22:43 WARN BlockManager: Block rdd_31_0 could not be removed as it was not found on disk or in memory
17/12/20 21:22:45 ERROR Executor: Exception in task 0.0 in stage 8.0 (TID 11)
java.lang.Exception: sparklyr worker rscript failure with status -1, check worker logs for details.
	at sparklyr.Rscript.init(rscript.scala:67)
	at sparklyr.WorkerRDD$$anon$2.run(rdd.scala:92)
17/12/20 21:22:48 ERROR TaskSchedulerImpl: Ignoring update with state FAILED for TID 11 because its task set is gone (this is likely the result of receiving duplicate task finished status updates) or its executor has been marked as failed.
17/12/20 21:22:50 INFO Executor: Told to re-register on heartbeat
17/12/20 21:22:50 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:22:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:22:50 INFO BlockManager: Reporting 7 blocks to the master.
17/12/20 21:22:50 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:50 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:22:50 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:22:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:22:50 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:23:00 INFO Executor: Told to re-register on heartbeat
17/12/20 21:23:00 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:23:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:23:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:23:00 INFO BlockManager: Reporting 7 blocks to the master.
17/12/20 21:23:00 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:23:00 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:23:00 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:23:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:23:00 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:23:03 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:23:03 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:23:04 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:23:04 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:23:05 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:23:05 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:23:05 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:23:05 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:23:06 INFO CodeGenerator: Code generated in 320.583936 ms
17/12/20 21:23:06 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:23:06 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:23:06 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:23:06 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:23:06 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:23:06 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:23:06 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:23:06 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:23:10 INFO Executor: Told to re-register on heartbeat
17/12/20 21:23:10 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:23:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:23:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:23:10 INFO BlockManager: Reporting 7 blocks to the master.
17/12/20 21:23:10 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:23:10 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:23:10 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:23:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:23:10 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:23:20 INFO Executor: Told to re-register on heartbeat
17/12/20 21:23:20 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:23:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:23:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:23:20 INFO BlockManager: Reporting 7 blocks to the master.
17/12/20 21:23:20 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:23:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:23:20 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:23:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:23:20 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:23:30 INFO Executor: Told to re-register on heartbeat
17/12/20 21:23:30 INFO BlockManager: BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None) re-registering with master
17/12/20 21:23:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:23:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 57657, None)
17/12/20 21:23:30 INFO BlockManager: Reporting 7 blocks to the master.
17/12/20 21:23:30 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:23:30 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:57657 (size: 4.0 MB, free: 1989.1 MB)
17/12/20 21:23:30 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:23:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:57657 (size: 24.0 KB, free: 1989.1 MB)
17/12/20 21:23:30 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:57657 (size: 3.8 MB, free: 1989.1 MB)
17/12/20 21:23:35 INFO SparkContext: Invoking stop() from shutdown hook
17/12/20 21:23:35 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/12/20 21:23:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/12/20 21:23:36 INFO MemoryStore: MemoryStore cleared
17/12/20 21:23:36 INFO BlockManager: BlockManager stopped
17/12/20 21:23:36 INFO BlockManagerMaster: BlockManagerMaster stopped
17/12/20 21:23:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/12/20 21:23:36 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-60069a35-d76a-4c28-be45-e703b32e2117\userFiles-deb488d0-c74b-48aa-8d70-b421fab31fe7
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-60069a35-d76a-4c28-be45-e703b32e2117\userFiles-deb488d0-c74b-48aa-8d70-b421fab31fe7
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:102)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1842)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1841)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 21:23:36 INFO SparkContext: Successfully stopped SparkContext
17/12/20 21:23:36 INFO ShutdownHookManager: Shutdown hook called
17/12/20 21:23:36 INFO ShutdownHookManager: Deleting directory C:\Users\conan\AppData\Local\Temp\spark-60069a35-d76a-4c28-be45-e703b32e2117
17/12/20 21:23:36 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-60069a35-d76a-4c28-be45-e703b32e2117
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-60069a35-d76a-4c28-be45-e703b32e2117
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 21:23:36 INFO ShutdownHookManager: Deleting directory C:\Users\conan\AppData\Local\Temp\spark-60069a35-d76a-4c28-be45-e703b32e2117\userFiles-deb488d0-c74b-48aa-8d70-b421fab31fe7
17/12/20 21:23:36 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-60069a35-d76a-4c28-be45-e703b32e2117\userFiles-deb488d0-c74b-48aa-8d70-b421fab31fe7
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-60069a35-d76a-4c28-be45-e703b32e2117\userFiles-deb488d0-c74b-48aa-8d70-b421fab31fe7
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 21:32:05 INFO SparkContext: Running Spark version 2.1.0
17/12/20 21:32:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/12/20 21:32:06 INFO SecurityManager: Changing view acls to: conan
17/12/20 21:32:06 INFO SecurityManager: Changing modify acls to: conan
17/12/20 21:32:06 INFO SecurityManager: Changing view acls groups to: 
17/12/20 21:32:06 INFO SecurityManager: Changing modify acls groups to: 
17/12/20 21:32:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(conan); groups with view permissions: Set(); users  with modify permissions: Set(conan); groups with modify permissions: Set()
17/12/20 21:32:06 INFO Utils: Successfully started service 'sparkDriver' on port 55117.
17/12/20 21:32:06 INFO SparkEnv: Registering MapOutputTracker
17/12/20 21:32:06 INFO SparkEnv: Registering BlockManagerMaster
17/12/20 21:32:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/12/20 21:32:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/12/20 21:32:06 INFO DiskBlockManager: Created local directory at C:\Users\conan\AppData\Local\Temp\blockmgr-99619a36-8793-4747-a322-668a6fb585f9
17/12/20 21:32:06 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
17/12/20 21:32:06 INFO SparkEnv: Registering OutputCommitCoordinator
17/12/20 21:32:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/12/20 21:32:06 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
17/12/20 21:32:06 INFO SparkContext: Added JAR file:/C:/Users/conan/Documents/R/win-library/3.4/sparklyr/java/sparklyr-2.1-2.11.jar at spark://127.0.0.1:55117/jars/sparklyr-2.1-2.11.jar with timestamp 1513805526624
17/12/20 21:32:06 INFO Executor: Starting executor ID driver on host localhost
17/12/20 21:32:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55138.
17/12/20 21:32:06 INFO NettyBlockTransferService: Server created on 127.0.0.1:55138
17/12/20 21:32:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/12/20 21:32:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 55138, None)
17/12/20 21:32:06 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:55138 with 2004.6 MB RAM, BlockManagerId(driver, 127.0.0.1, 55138, None)
17/12/20 21:32:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 55138, None)
17/12/20 21:32:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 55138, None)
17/12/20 21:32:07 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
17/12/20 21:32:07 INFO SharedState: Warehouse path is 'C:UsersconanAppDataLocalsparkspark-2.1.0-bin-hadoop2.7	mphive'.
17/12/20 21:32:07 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/12/20 21:32:08 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/12/20 21:32:08 INFO ObjectStore: ObjectStore, initialize called
17/12/20 21:32:08 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/12/20 21:32:08 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/12/20 21:32:09 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/12/20 21:32:10 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:32:10 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:32:10 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:32:10 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:32:11 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/12/20 21:32:11 INFO ObjectStore: Initialized ObjectStore
17/12/20 21:32:11 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/12/20 21:32:11 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/12/20 21:32:11 INFO HiveMetaStore: Added admin role in metastore
17/12/20 21:32:11 INFO HiveMetaStore: Added public role in metastore
17/12/20 21:32:11 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/12/20 21:32:11 INFO HiveMetaStore: 0: get_all_databases
17/12/20 21:32:11 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_all_databases	
17/12/20 21:32:11 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/12/20 21:32:11 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/12/20 21:32:11 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:32:12 INFO SessionState: Created local directory: C:/Users/conan/AppData/Local/Temp/78d72c6d-01d9-4c56-be30-ab5050e49420_resources
17/12/20 21:32:12 INFO SessionState: Created HDFS directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/conan/78d72c6d-01d9-4c56-be30-ab5050e49420
17/12/20 21:32:12 INFO SessionState: Created local directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/78d72c6d-01d9-4c56-be30-ab5050e49420
17/12/20 21:32:12 INFO SessionState: Created HDFS directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/conan/78d72c6d-01d9-4c56-be30-ab5050e49420/_tmp_space.db
17/12/20 21:32:12 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:UsersconanAppDataLocalsparkspark-2.1.0-bin-hadoop2.7	mphive
17/12/20 21:32:12 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:32:12 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:32:12 INFO HiveMetaStore: 0: get_database: global_temp
17/12/20 21:32:12 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: global_temp	
17/12/20 21:32:12 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
17/12/20 21:32:12 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:32:14 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:32:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:32:14 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:32:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:32:14 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:32:14 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:32:25 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:32:25 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:32:25 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:32:25 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:32:25 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:32:25 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:32:25 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:32:25 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:32:26 INFO CodeGenerator: Code generated in 284.370529 ms
17/12/20 21:32:26 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 21:32:26 INFO DAGScheduler: Got job 0 (collect at utils.scala:58) with 1 output partitions
17/12/20 21:32:26 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:58)
17/12/20 21:32:26 INFO DAGScheduler: Parents of final stage: List()
17/12/20 21:32:26 INFO DAGScheduler: Missing parents: List()
17/12/20 21:32:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at map at utils.scala:55), which has no missing parents
17/12/20 21:32:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 8.7 KB, free 2004.6 MB)
17/12/20 21:32:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2004.6 MB)
17/12/20 21:32:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:55138 (size: 4.6 KB, free: 2004.6 MB)
17/12/20 21:32:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996
17/12/20 21:32:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at utils.scala:55)
17/12/20 21:32:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/12/20 21:32:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6041 bytes)
17/12/20 21:32:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/12/20 21:32:26 INFO Executor: Fetching spark://127.0.0.1:55117/jars/sparklyr-2.1-2.11.jar with timestamp 1513805526624
17/12/20 21:32:26 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:55117 after 14 ms (0 ms spent in bootstraps)
17/12/20 21:32:26 INFO Utils: Fetching spark://127.0.0.1:55117/jars/sparklyr-2.1-2.11.jar to C:\Users\conan\AppData\Local\Temp\spark-8efd1c73-e3db-46bf-850b-0dc835d2ddde\userFiles-648ba4bd-2ba8-4f60-9e91-170624c60054\fetchFileTemp4995132217036116718.tmp
17/12/20 21:32:26 INFO Executor: Adding file:/C:/Users/conan/AppData/Local/Temp/spark-8efd1c73-e3db-46bf-850b-0dc835d2ddde/userFiles-648ba4bd-2ba8-4f60-9e91-170624c60054/sparklyr-2.1-2.11.jar to class loader
17/12/20 21:32:26 INFO CodeGenerator: Code generated in 13.380712 ms
17/12/20 21:32:26 INFO CodeGenerator: Code generated in 13.627272 ms
17/12/20 21:32:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1231 bytes result sent to driver
17/12/20 21:32:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 356 ms on localhost (executor driver) (1/1)
17/12/20 21:32:26 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:58) finished in 0.387 s
17/12/20 21:32:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/12/20 21:32:26 INFO DAGScheduler: Job 0 finished: collect at utils.scala:58, took 0.635107 s
17/12/20 21:32:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:32:29 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 21:32:30 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:32:30 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 21:32:30 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 21:32:30 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 21:32:30 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 21:32:30 INFO FileSourceStrategy: Output Data Schema: struct<S1: double, S2: double, S3: double, S4: double, S5: double ... 8 more fields>
17/12/20 21:32:30 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 21:32:30 INFO CodeGenerator: Code generated in 7.422864 ms
17/12/20 21:32:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 293.7 KB, free 2004.3 MB)
17/12/20 21:32:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2004.3 MB)
17/12/20 21:32:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:55138 (size: 24.0 KB, free: 2004.6 MB)
17/12/20 21:32:30 INFO SparkContext: Created broadcast 1 from sql at <unknown>:0
17/12/20 21:32:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5726629 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 21:32:30 INFO CodeGenerator: Code generated in 11.556241 ms
17/12/20 21:32:30 INFO CodeGenerator: Code generated in 9.174838 ms
17/12/20 21:32:30 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 21:32:30 INFO DAGScheduler: Registering RDD 10 (sql at <unknown>:0)
17/12/20 21:32:30 INFO DAGScheduler: Registering RDD 15 (sql at <unknown>:0)
17/12/20 21:32:30 INFO DAGScheduler: Got job 1 (sql at <unknown>:0) with 1 output partitions
17/12/20 21:32:30 INFO DAGScheduler: Final stage: ResultStage 3 (sql at <unknown>:0)
17/12/20 21:32:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
17/12/20 21:32:30 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
17/12/20 21:32:30 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[10] at sql at <unknown>:0), which has no missing parents
17/12/20 21:32:30 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:55138 in memory (size: 4.6 KB, free: 2004.6 MB)
17/12/20 21:32:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.1 KB, free 2004.3 MB)
17/12/20 21:32:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.5 KB, free 2004.3 MB)
17/12/20 21:32:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:55138 (size: 7.5 KB, free: 2004.6 MB)
17/12/20 21:32:30 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/12/20 21:32:30 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[10] at sql at <unknown>:0)
17/12/20 21:32:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks
17/12/20 21:32:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 6676 bytes)
17/12/20 21:32:30 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 6676 bytes)
17/12/20 21:32:30 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, localhost, executor driver, partition 2, PROCESS_LOCAL, 6676 bytes)
17/12/20 21:32:30 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, localhost, executor driver, partition 3, PROCESS_LOCAL, 6676 bytes)
17/12/20 21:32:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/12/20 21:32:30 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
17/12/20 21:32:30 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
17/12/20 21:32:30 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
17/12/20 21:32:30 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/Rtmp0CG1So/spark_serialize_e9e0e1b50b9ca98a2c791ff782c1dac60f24ea2fec70fe7b72454f9b0e48c219.csv, range: 11453258-17179887, partition values: [empty row]
17/12/20 21:32:30 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/Rtmp0CG1So/spark_serialize_e9e0e1b50b9ca98a2c791ff782c1dac60f24ea2fec70fe7b72454f9b0e48c219.csv, range: 5726629-11453258, partition values: [empty row]
17/12/20 21:32:30 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/Rtmp0CG1So/spark_serialize_e9e0e1b50b9ca98a2c791ff782c1dac60f24ea2fec70fe7b72454f9b0e48c219.csv, range: 0-5726629, partition values: [empty row]
17/12/20 21:32:30 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/Rtmp0CG1So/spark_serialize_e9e0e1b50b9ca98a2c791ff782c1dac60f24ea2fec70fe7b72454f9b0e48c219.csv, range: 17179887-18712212, partition values: [empty row]
17/12/20 21:32:30 INFO CodeGenerator: Code generated in 17.74668 ms
17/12/20 21:32:31 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 1719 bytes result sent to driver
17/12/20 21:32:31 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 541 ms on localhost (executor driver) (1/4)
17/12/20 21:32:31 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 1792 bytes result sent to driver
17/12/20 21:32:31 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 736 ms on localhost (executor driver) (2/4)
17/12/20 21:32:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1705 bytes result sent to driver
17/12/20 21:32:31 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1792 bytes result sent to driver
17/12/20 21:32:31 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 746 ms on localhost (executor driver) (3/4)
17/12/20 21:32:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 763 ms on localhost (executor driver) (4/4)
17/12/20 21:32:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/12/20 21:32:31 INFO DAGScheduler: ShuffleMapStage 1 (sql at <unknown>:0) finished in 0.764 s
17/12/20 21:32:31 INFO DAGScheduler: looking for newly runnable stages
17/12/20 21:32:31 INFO DAGScheduler: running: Set()
17/12/20 21:32:31 INFO DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)
17/12/20 21:32:31 INFO DAGScheduler: failed: Set()
17/12/20 21:32:31 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[15] at sql at <unknown>:0), which has no missing parents
17/12/20 21:32:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.3 KB, free 2004.3 MB)
17/12/20 21:32:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2004.2 MB)
17/12/20 21:32:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:55138 (size: 6.9 KB, free: 2004.6 MB)
17/12/20 21:32:31 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/12/20 21:32:31 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[15] at sql at <unknown>:0)
17/12/20 21:32:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
17/12/20 21:32:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5, localhost, executor driver, partition 0, ANY, 5942 bytes)
17/12/20 21:32:31 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6, localhost, executor driver, partition 1, ANY, 5942 bytes)
17/12/20 21:32:31 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)
17/12/20 21:32:31 INFO Executor: Running task 0.0 in stage 2.0 (TID 5)
17/12/20 21:32:31 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/12/20 21:32:31 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks out of 4 blocks
17/12/20 21:32:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
17/12/20 21:32:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
17/12/20 21:32:31 INFO MemoryStore: Block rdd_12_1 stored as values in memory (estimated size 3.8 MB, free 2000.4 MB)
17/12/20 21:32:31 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:55138 (size: 3.8 MB, free: 2000.7 MB)
17/12/20 21:32:31 INFO CodeGenerator: Code generated in 5.946522 ms
17/12/20 21:32:31 INFO MemoryStore: Block rdd_12_0 stored as values in memory (estimated size 3.8 MB, free 1996.6 MB)
17/12/20 21:32:31 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:55138 (size: 3.8 MB, free: 1996.9 MB)
17/12/20 21:32:31 INFO CodeGenerator: Code generated in 37.058811 ms
17/12/20 21:32:31 INFO Executor: Finished task 0.0 in stage 2.0 (TID 5). 3151 bytes result sent to driver
17/12/20 21:32:31 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 3151 bytes result sent to driver
17/12/20 21:32:31 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 333 ms on localhost (executor driver) (1/2)
17/12/20 21:32:31 INFO DAGScheduler: ShuffleMapStage 2 (sql at <unknown>:0) finished in 0.333 s
17/12/20 21:32:31 INFO DAGScheduler: looking for newly runnable stages
17/12/20 21:32:31 INFO DAGScheduler: running: Set()
17/12/20 21:32:31 INFO DAGScheduler: waiting: Set(ResultStage 3)
17/12/20 21:32:31 INFO DAGScheduler: failed: Set()
17/12/20 21:32:31 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at sql at <unknown>:0), which has no missing parents
17/12/20 21:32:31 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 331 ms on localhost (executor driver) (2/2)
17/12/20 21:32:31 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/12/20 21:32:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 1996.6 MB)
17/12/20 21:32:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 1996.6 MB)
17/12/20 21:32:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:55138 (size: 3.7 KB, free: 1996.9 MB)
17/12/20 21:32:31 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:996
17/12/20 21:32:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at sql at <unknown>:0)
17/12/20 21:32:31 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/12/20 21:32:31 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 7, localhost, executor driver, partition 0, ANY, 5953 bytes)
17/12/20 21:32:31 INFO Executor: Running task 0.0 in stage 3.0 (TID 7)
17/12/20 21:32:31 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 21:32:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 21:32:31 INFO Executor: Finished task 0.0 in stage 3.0 (TID 7). 1707 bytes result sent to driver
17/12/20 21:32:31 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 7) in 0 ms on localhost (executor driver) (1/1)
17/12/20 21:32:31 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/12/20 21:32:31 INFO DAGScheduler: ResultStage 3 (sql at <unknown>:0) finished in 0.000 s
17/12/20 21:32:31 INFO DAGScheduler: Job 1 finished: sql at <unknown>:0, took 1.222472 s
17/12/20 21:32:31 INFO CodeGenerator: Code generated in 6.68658 ms
17/12/20 21:32:31 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:32:31 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 21:32:31 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 21:32:31 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 157 bytes
17/12/20 21:32:31 INFO DAGScheduler: Registering RDD 22 (collect at utils.scala:196)
17/12/20 21:32:31 INFO DAGScheduler: Got job 2 (collect at utils.scala:196) with 1 output partitions
17/12/20 21:32:31 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:196)
17/12/20 21:32:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/12/20 21:32:31 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/12/20 21:32:31 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[22] at collect at utils.scala:196), which has no missing parents
17/12/20 21:32:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.3 KB, free 1996.6 MB)
17/12/20 21:32:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.9 KB, free 1996.6 MB)
17/12/20 21:32:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:55138 (size: 6.9 KB, free: 1996.9 MB)
17/12/20 21:32:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:996
17/12/20 21:32:31 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[22] at collect at utils.scala:196)
17/12/20 21:32:31 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/12/20 21:32:31 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5934 bytes)
17/12/20 21:32:31 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 5934 bytes)
17/12/20 21:32:31 INFO Executor: Running task 0.0 in stage 5.0 (TID 8)
17/12/20 21:32:31 INFO BlockManager: Found block rdd_12_0 locally
17/12/20 21:32:31 INFO Executor: Running task 1.0 in stage 5.0 (TID 9)
17/12/20 21:32:31 INFO BlockManager: Found block rdd_12_1 locally
17/12/20 21:32:31 INFO Executor: Finished task 0.0 in stage 5.0 (TID 8). 1950 bytes result sent to driver
17/12/20 21:32:31 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 8) in 16 ms on localhost (executor driver) (1/2)
17/12/20 21:32:31 INFO Executor: Finished task 1.0 in stage 5.0 (TID 9). 2037 bytes result sent to driver
17/12/20 21:32:31 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:196) finished in 0.031 s
17/12/20 21:32:31 INFO DAGScheduler: looking for newly runnable stages
17/12/20 21:32:31 INFO DAGScheduler: running: Set()
17/12/20 21:32:31 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/12/20 21:32:31 INFO DAGScheduler: failed: Set()
17/12/20 21:32:31 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 9) in 31 ms on localhost (executor driver) (2/2)
17/12/20 21:32:31 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[25] at collect at utils.scala:196), which has no missing parents
17/12/20 21:32:31 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/12/20 21:32:31 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.0 KB, free 1996.6 MB)
17/12/20 21:32:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.7 KB, free 1996.6 MB)
17/12/20 21:32:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:55138 (size: 3.7 KB, free: 1996.9 MB)
17/12/20 21:32:31 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:996
17/12/20 21:32:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[25] at collect at utils.scala:196)
17/12/20 21:32:31 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/12/20 21:32:31 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 10, localhost, executor driver, partition 0, ANY, 5945 bytes)
17/12/20 21:32:31 INFO Executor: Running task 0.0 in stage 6.0 (TID 10)
17/12/20 21:32:31 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 21:32:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 21:32:31 INFO Executor: Finished task 0.0 in stage 6.0 (TID 10). 1707 bytes result sent to driver
17/12/20 21:32:31 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:196) finished in 0.016 s
17/12/20 21:32:31 INFO DAGScheduler: Job 2 finished: collect at utils.scala:196, took 0.059756 s
17/12/20 21:32:31 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 10) in 0 ms on localhost (executor driver) (1/1)
17/12/20 21:32:31 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/12/20 21:32:31 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:32:31 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz3`
WHERE (0 = 1)
17/12/20 21:32:32 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:32:32 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`, `V11` AS `V11`, `V12` AS `V12`, `V13` AS `V13`, `V14` AS `V14`, `V15` AS `V15`, `V16` AS `V16`, `V17` AS `V17`, `V18` AS `V18`, `V19` AS `V19`, `V20` AS `V20`, `V21` AS `V21`, `V22` AS `V22`, `V23` AS `V23`, `V24` AS `V24`, `V25` AS `V25`, `V26` AS `V26`, `V27` AS `V27`, `V28` AS `V28`, `V29` AS `V29`, `V30` AS `V30`, `V31` AS `V31`, `V32` AS `V32`, `V33` AS `V33`, `V34` AS `V34`, `V35` AS `V35`, `V36` AS `V36`, `V37` AS `V37`, `V38` AS `V38`, `V39` AS `V39`, `V40` AS `V40`, `V41` AS `V41`, `V42` AS `V42`, `V43` AS `V43`, `V44` AS `V44`, `V45` AS `V45`, `V46` AS `V46`, `V47` AS `V47`, `V48` AS `V48`, `V49` AS `V49`, `V50` AS `V50`, `V51` AS `V51`, `V52` AS `V52`, `V53` AS `V53`, `V54` AS `V54`, `V55` AS `V55`, `V56` AS `V56`, `V57` AS `V57`, `V58` AS `V58`, `V59` AS `V59`, `V60` AS `V60`, `V61` AS `V61`, `V62` AS `V62`, `V63` AS `V63`, `V64` AS `V64`, `V65` AS `V65`, `V66` AS `V66`, `V67` AS `V67`, `V68` AS `V68`, `V69` AS `V69`, `V70` AS `V70`, `V71` AS `V71`, `V72` AS `V72`, `V73` AS `V73`, `V74` AS `V74`, `V75` AS `V75`, `V76` AS `V76`, `V77` AS `V77`, `V78` AS `V78`, `V79` AS `V79`, `V80` AS `V80`, `V81` AS `V81`, `V82` AS `V82`, `V83` AS `V83`, `V84` AS `V84`, `V85` AS `V85`, `V86` AS `V86`, `V87` AS `V87`, `V88` AS `V88`, `V89` AS `V89`, `V90` AS `V90`, `V91` AS `V91`, `V92` AS `V92`, `V93` AS `V93`, `V94` AS `V94`, `V95` AS `V95`, `V96` AS `V96`, `V97` AS `V97`, `V98` AS `V98`, `V99` AS `V99`, `V100` AS `V100`
FROM (SELECT `S1`, `S2`, `S3`, `S4`, `S5`, `S6`, `S7`, `S8`, `S9`, `S10`, `S8` + 0.94339811 * RANDN() AS `V1`, `S2` + 0.9486833 * RANDN() AS `V2`, `S2` + 0.9486833 * RANDN() AS `V3`, `S9` + 0.91104336 * RANDN() AS `V4`, `S4` + 0.93808315 * RANDN() AS `V5`, `S8` + 0.94339811 * RANDN() AS `V6`, `S10` + 0.89442719 * RANDN() AS `V7`, `S7` + 0.90553851 * RANDN() AS `V8`, `S10` + 0.89442719 * RANDN() AS `V9`, `S6` + 0.91104336 * RANDN() AS `V10`, `S1` + 0.9 * RANDN() AS `V11`, `S1` + 0.9 * RANDN() AS `V12`, `S5` + 0.93808315 * RANDN() AS `V13`, `S6` + 0.91104336 * RANDN() AS `V14`, `S2` + 0.9486833 * RANDN() AS `V15`, `S5` + 0.93808315 * RANDN() AS `V16`, `S10` + 0.89442719 * RANDN() AS `V17`, `S4` + 0.93808315 * RANDN() AS `V18`, `S8` + 0.94339811 * RANDN() AS `V19`, `S7` + 0.90553851 * RANDN() AS `V20`, `S6` + 0.91104336 * RANDN() AS `V21`, `S2` + 0.9486833 * RANDN() AS `V22`, `S9` + 0.91104336 * RANDN() AS `V23`, `S2` + 0.9486833 * RANDN() AS `V24`, `S3` + 0.9486833 * RANDN() AS `V25`, `S6` + 0.91104336 * RANDN() AS `V26`, `S4` + 0.93808315 * RANDN() AS `V27`, `S10` + 0.89442719 * RANDN() AS `V28`, `S7` + 0.90553851 * RANDN() AS `V29`, `S10` + 0.89442719 * RANDN() AS `V30`, `S10` + 0.89442719 * RANDN() AS `V31`, `S10` + 0.89442719 * RANDN() AS `V32`, `S9` + 0.91104336 * RANDN() AS `V33`, `S3` + 0.9486833 * RANDN() AS `V34`, `S9` + 0.91104336 * RANDN() AS `V35`, `S6` + 0.91104336 * RANDN() AS `V36`, `S8` + 0.94339811 * RANDN() AS `V37`, `S8` + 0.94339811 * RANDN() AS `V38`, `S9` + 0.91104336 * RANDN() AS `V39`, `S5` + 0.93808315 * RANDN() AS `V40`, `S9` + 0.91104336 * RANDN() AS `V41`, `S3` + 0.9486833 * RANDN() AS `V42`, `S10` + 0.89442719 * RANDN() AS `V43`, `S2` + 0.9486833 * RANDN() AS `V44`, `S2` + 0.9486833 * RANDN() AS `V45`, `S6` + 0.91104336 * RANDN() AS `V46`, `S8` + 0.94339811 * RANDN() AS `V47`, `S6` + 0.91104336 * RANDN() AS `V48`, `S2` + 0.9486833 * RANDN() AS `V49`, `S2` + 0.9486833 * RANDN() AS `V50`, `S7` + 0.90553851 * RANDN() AS `V51`, `S6` + 0.91104336 * RANDN() AS `V52`, `S4` + 0.93808315 * RANDN() AS `V53`, `S3` + 0.9486833 * RANDN() AS `V54`, `S4` + 0.93808315 * RANDN() AS `V55`, `S5` + 0.93808315 * RANDN() AS `V56`, `S7` + 0.90553851 * RANDN() AS `V57`, `S9` + 0.91104336 * RANDN() AS `V58`, `S8` + 0.94339811 * RANDN() AS `V59`, `S8` + 0.94339811 * RANDN() AS `V60`, `S2` + 0.9486833 * RANDN() AS `V61`, `S2` + 0.9486833 * RANDN() AS `V62`, `S6` + 0.91104336 * RANDN() AS `V63`, `S8` + 0.94339811 * RANDN() AS `V64`, `S6` + 0.91104336 * RANDN() AS `V65`, `S7` + 0.90553851 * RANDN() AS `V66`, `S8` + 0.94339811 * RANDN() AS `V67`, `S1` + 0.9 * RANDN() AS `V68`, `S7` + 0.90553851 * RANDN() AS `V69`, `S2` + 0.9486833 * RANDN() AS `V70`, `S5` + 0.93808315 * RANDN() AS `V71`, `S2` + 0.9486833 * RANDN() AS `V72`, `S3` + 0.9486833 * RANDN() AS `V73`, `S4` + 0.93808315 * RANDN() AS `V74`, `S9` + 0.91104336 * RANDN() AS `V75`, `S6` + 0.91104336 * RANDN() AS `V76`, `S3` + 0.9486833 * RANDN() AS `V77`, `S2` + 0.9486833 * RANDN() AS `V78`, `S1` + 0.9 * RANDN() AS `V79`, `S9` + 0.91104336 * RANDN() AS `V80`, `S7` + 0.90553851 * RANDN() AS `V81`, `S3` + 0.9486833 * RANDN() AS `V82`, `S2` + 0.9486833 * RANDN() AS `V83`, `S9` + 0.91104336 * RANDN() AS `V84`, `S1` + 0.9 * RANDN() AS `V85`, `S3` + 0.9486833 * RANDN() AS `V86`, `S1` + 0.9 * RANDN() AS `V87`, `S4` + 0.93808315 * RANDN() AS `V88`, `S8` + 0.94339811 * RANDN() AS `V89`, `S9` + 0.91104336 * RANDN() AS `V90`, `S3` + 0.9486833 * RANDN() AS `V91`, `S6` + 0.91104336 * RANDN() AS `V92`, `S1` + 0.9 * RANDN() AS `V93`, `S2` + 0.9486833 * RANDN() AS `V94`, `S3` + 0.9486833 * RANDN() AS `V95`, `S4` + 0.93808315 * RANDN() AS `V96`, `S10` + 0.89442719 * RANDN() AS `V97`, `S7` + 0.90553851 * RANDN() AS `V98`, `S6` + 0.91104336 * RANDN() AS `V99`, `S5` + 0.93808315 * RANDN() AS `V100`
FROM `analyis_tbl`) `posadqxivq`
17/12/20 21:32:32 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 21:32:32 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:32:32 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz4`
WHERE (0 = 1)
17/12/20 21:32:33 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:32:33 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 21:32:33 INFO CodeGenerator: Code generated in 126.260035 ms
17/12/20 21:32:33 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:55138 in memory (size: 6.9 KB, free: 1996.9 MB)
17/12/20 21:32:33 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 21:32:33 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 157 bytes
17/12/20 21:32:33 INFO DAGScheduler: Got job 3 (take at <unknown>:0) with 1 output partitions
17/12/20 21:32:33 INFO DAGScheduler: Final stage: ResultStage 8 (take at <unknown>:0)
17/12/20 21:32:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
17/12/20 21:32:33 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:55138 in memory (size: 6.9 KB, free: 1996.9 MB)
17/12/20 21:32:33 INFO DAGScheduler: Missing parents: List()
17/12/20 21:32:33 INFO DAGScheduler: Submitting ResultStage 8 (WorkerRDD[31] at RDD at rdd.scala:18), which has no missing parents
17/12/20 21:32:33 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 7.8 MB, free 1988.8 MB)
17/12/20 21:32:33 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 127.0.0.1:55138 in memory (size: 3.7 KB, free: 1996.9 MB)
17/12/20 21:32:33 INFO ContextCleaner: Cleaned shuffle 1
17/12/20 21:32:33 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.0 MB, free 1984.8 MB)
17/12/20 21:32:33 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:55138 (size: 4.0 MB, free: 1992.9 MB)
17/12/20 21:32:33 INFO MemoryStore: Block broadcast_7_piece1 stored as bytes in memory (estimated size 3.7 MB, free 1981.1 MB)
17/12/20 21:32:33 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:55138 in memory (size: 7.5 KB, free: 1992.9 MB)
17/12/20 21:32:33 INFO BlockManagerInfo: Added broadcast_7_piece1 in memory on 127.0.0.1:55138 (size: 3.7 MB, free: 1989.2 MB)
17/12/20 21:32:33 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:996
17/12/20 21:32:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (WorkerRDD[31] at RDD at rdd.scala:18)
17/12/20 21:32:33 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/12/20 21:32:33 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 5917 bytes)
17/12/20 21:32:33 INFO Executor: Running task 0.0 in stage 8.0 (TID 11)
17/12/20 21:32:33 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:55138 in memory (size: 3.7 KB, free: 1989.2 MB)
17/12/20 21:32:33 INFO BlockManager: Found block rdd_12_0 locally
17/12/20 21:32:33 INFO ContextCleaner: Cleaned accumulator 310
17/12/20 21:32:33 INFO CodeGenerator: Code generated in 17.909418 ms
17/12/20 21:32:33 INFO CodeGenerator: Code generated in 40.543505 ms
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 69
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 68
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 67
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 66
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 65
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 64
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 63
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 62
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 61
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 60
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 59
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 58
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 57
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 51
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 50
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 1
17/12/20 21:32:35 INFO ContextCleaner: Cleaned accumulator 0
17/12/20 21:35:29 INFO MemoryStore: Block rdd_31_0 stored as values in memory (estimated size 135.2 MB, free 1845.9 MB)
17/12/20 21:35:29 INFO BlockManagerInfo: Added rdd_31_0 in memory on 127.0.0.1:55138 (size: 135.2 MB, free: 1854.0 MB)
17/12/20 21:35:29 WARN Executor: 1 block locks were not released by TID = 11:
[rdd_31_0]
17/12/20 21:35:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 11). 16777 bytes result sent to driver
17/12/20 21:35:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 11) in 175825 ms on localhost (executor driver) (1/1)
17/12/20 21:35:29 INFO DAGScheduler: ResultStage 8 (take at <unknown>:0) finished in 175.825 s
17/12/20 21:35:29 INFO DAGScheduler: Job 3 finished: take at <unknown>:0, took 175.867186 s
17/12/20 21:35:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/12/20 21:35:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:35:29 INFO SparkSqlParser: Parsing command: sparklyr_tmp_1714141259fa
17/12/20 21:35:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:35:29 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1714141259fa` AS `zzz5`
WHERE (0 = 1)
17/12/20 21:35:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:35:29 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1714141259fa`
17/12/20 21:35:29 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 21:35:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:35:29 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz6`
WHERE (0 = 1)
17/12/20 21:35:30 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:35:30 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.0185) AS `V1`, (`V2` < 0.026) AS `V2`, (`V3` < 0.0035) AS `V3`, (`V4` < 0.1) AS `V4`, (`V5` < 0.0035) AS `V5`, (`V6` < 0.0018) AS `V6`, (`V7` < 0.0023) AS `V7`, (`V8` < 0.054) AS `V8`, (`V9` < 0.0138) AS `V9`, (`V10` < 0.0023) AS `V10`, (`V11` < 3e-04) AS `V11`, (`V12` < 0.054) AS `V12`, (`V13` < 0.0185) AS `V13`, (`V14` < 0.054) AS `V14`, (`V15` < 0.009) AS `V15`, (`V16` < 0.0028) AS `V16`, (`V17` < 0.026) AS `V17`, (`V18` < 0.15) AS `V18`, (`V19` < 0.0185) AS `V19`, (`V20` < 0.0018) AS `V20`, (`V21` < 0.0023) AS `V21`, (`V22` < 0.0028) AS `V22`, (`V23` < 0.009) AS `V23`, (`V24` < 0.0375) AS `V24`, (`V25` < 0.0185) AS `V25`, (`V26` < 0.3) AS `V26`, (`V27` < 0.3) AS `V27`, (`V28` < 0.075) AS `V28`, (`V29` < 0.0035) AS `V29`, (`V30` < 0.0375) AS `V30`, (`V31` < 0.026) AS `V31`, (`V32` < 0.0035) AS `V32`, (`V33` < 0.0018) AS `V33`, (`V34` < 0.0055) AS `V34`, (`V35` < 0.026) AS `V35`, (`V36` < 0.026) AS `V36`, (`V37` < 0.0138) AS `V37`, (`V38` < 0.15) AS `V38`, (`V39` < 0.026) AS `V39`, (`V40` < 0.054) AS `V40`, (`V41` < 0.0185) AS `V41`, (`V42` < 0.0023) AS `V42`, (`V43` < 0.026) AS `V43`, (`V44` < 0.0028) AS `V44`, (`V45` < 0.026) AS `V45`, (`V46` < 0.0018) AS `V46`, (`V47` < 0.0045) AS `V47`, (`V48` < 8e-04) AS `V48`, (`V49` < 0.0138) AS `V49`, (`V50` < 0.009) AS `V50`, (`V51` < 0.0375) AS `V51`, (`V52` < 0.0185) AS `V52`, (`V53` < 0.0035) AS `V53`, (`V54` < 0.026) AS `V54`, (`V55` < 0.0138) AS `V55`, (`V56` < 0.009) AS `V56`, (`V57` < 0.0375) AS `V57`, (`V58` < 0.075) AS `V58`, (`V59` < 0.0018) AS `V59`, (`V60` < 0.0018) AS `V60`, (`V61` < 0.075) AS `V61`, (`V62` < 0.0023) AS `V62`, (`V63` < 0.0028) AS `V63`, (`V64` < 0.0035) AS `V64`, (`V65` < 0.0138) AS `V65`, (`V66` < 0.0028) AS `V66`, (`V67` < 0.0138) AS `V67`, (`V68` < 0.075) AS `V68`, (`V69` < 0.0023) AS `V69`, (`V70` < 0.009) AS `V70`, (`V71` < 0.0045) AS `V71`, (`V72` < 0.026) AS `V72`, (`V73` < 0.075) AS `V73`, (`V74` < 0.009) AS `V74`, (`V75` < 0.0138) AS `V75`, (`V76` < 0.0185) AS `V76`, (`V77` < 0.026) AS `V77`, (`V78` < 0.0375) AS `V78`, (`V79` < 0.0055) AS `V79`, (`V80` < 0.3) AS `V80`, (`V81` < 0.054) AS `V81`, (`V82` < 0.009) AS `V82`, (`V83` < 0.0138) AS `V83`, (`V84` < 0.054) AS `V84`, (`V85` < 0.0138) AS `V85`, (`V86` < 0.15) AS `V86`, (`V87` < 0.0035) AS `V87`, (`V88` < 0.0028) AS `V88`, (`V89` < 0.0138) AS `V89`, (`V90` < 0.0055) AS `V90`, (`V91` < 0.1) AS `V91`, (`V92` < 0.0023) AS `V92`, (`V93` < 0.0035) AS `V93`, (`V94` < 0.054) AS `V94`, (`V95` < 0.0185) AS `V95`, (`V96` < 0.0055) AS `V96`, (`V97` < 0.009) AS `V97`, (`V98` < 0.0138) AS `V98`, (`V99` < 0.3) AS `V99`, (`V100` < 0.0035) AS `V100`
FROM `analyis_tbl`
17/12/20 21:35:30 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 21:35:30 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:35:30 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz7`
WHERE (0 = 1)
17/12/20 21:35:30 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:35:30 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 21:35:30 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
17/12/20 21:35:30 INFO CodeGenerator: Code generated in 76.740298 ms
17/12/20 21:35:30 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 21:35:30 INFO DAGScheduler: Got job 4 (collect at utils.scala:196) with 2 output partitions
17/12/20 21:35:30 INFO DAGScheduler: Final stage: ResultStage 10 (collect at utils.scala:196)
17/12/20 21:35:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
17/12/20 21:35:30 INFO DAGScheduler: Missing parents: List()
17/12/20 21:35:30 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[38] at collect at utils.scala:196), which has no missing parents
17/12/20 21:35:30 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 7.9 MB, free 1838.0 MB)
17/12/20 21:35:30 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.0 MB, free 1834.0 MB)
17/12/20 21:35:30 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:55138 (size: 4.0 MB, free: 1850.0 MB)
17/12/20 21:35:30 INFO MemoryStore: Block broadcast_8_piece1 stored as bytes in memory (estimated size 3.7 MB, free 1830.2 MB)
17/12/20 21:35:30 INFO BlockManagerInfo: Added broadcast_8_piece1 in memory on 127.0.0.1:55138 (size: 3.7 MB, free: 1846.3 MB)
17/12/20 21:35:30 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:996
17/12/20 21:35:30 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[38] at collect at utils.scala:196)
17/12/20 21:35:30 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
17/12/20 21:35:30 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 5949 bytes)
17/12/20 21:35:30 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 5949 bytes)
17/12/20 21:35:30 INFO Executor: Running task 0.0 in stage 10.0 (TID 12)
17/12/20 21:35:30 INFO Executor: Running task 1.0 in stage 10.0 (TID 13)
17/12/20 21:35:30 INFO BlockManager: Found block rdd_31_0 locally
17/12/20 21:35:30 INFO BlockManager: Found block rdd_12_1 locally
17/12/20 21:35:30 INFO CodeGenerator: Code generated in 51.799192 ms
17/12/20 21:35:30 INFO CodeGenerator: Code generated in 293.947869 ms
17/12/20 21:35:32 INFO MemoryStore: Block taskresult_12 stored as bytes in memory (estimated size 1315.1 KB, free 1829.0 MB)
17/12/20 21:35:32 INFO BlockManagerInfo: Added taskresult_12 in memory on 127.0.0.1:55138 (size: 1315.1 KB, free: 1845.0 MB)
17/12/20 21:35:32 INFO Executor: Finished task 0.0 in stage 10.0 (TID 12). 1346615 bytes result sent via BlockManager)
17/12/20 21:35:32 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:55138 after 1 ms (0 ms spent in bootstraps)
17/12/20 21:35:32 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 12) in 1902 ms on localhost (executor driver) (1/2)
17/12/20 21:35:32 INFO BlockManagerInfo: Removed taskresult_12 on 127.0.0.1:55138 in memory (size: 1315.1 KB, free: 1846.3 MB)
17/12/20 21:38:25 INFO BlockManagerInfo: Removed broadcast_7_piece1 on 127.0.0.1:55138 in memory (size: 3.7 MB, free: 1850.0 MB)
17/12/20 21:38:25 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 127.0.0.1:55138 in memory (size: 4.0 MB, free: 1854.0 MB)
17/12/20 21:38:26 INFO MemoryStore: Block rdd_31_1 stored as values in memory (estimated size 135.2 MB, free 1710.5 MB)
17/12/20 21:38:26 INFO BlockManagerInfo: Added rdd_31_1 in memory on 127.0.0.1:55138 (size: 135.2 MB, free: 1718.8 MB)
17/12/20 21:38:26 INFO MemoryStore: Block taskresult_13 stored as bytes in memory (estimated size 1312.6 KB, free 1709.3 MB)
17/12/20 21:38:26 INFO BlockManagerInfo: Added taskresult_13 in memory on 127.0.0.1:55138 (size: 1312.6 KB, free: 1717.5 MB)
17/12/20 21:38:26 INFO Executor: Finished task 1.0 in stage 10.0 (TID 13). 1344093 bytes result sent via BlockManager)
17/12/20 21:38:26 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 13) in 176205 ms on localhost (executor driver) (2/2)
17/12/20 21:38:26 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
17/12/20 21:38:26 INFO DAGScheduler: ResultStage 10 (collect at utils.scala:196) finished in 176.207 s
17/12/20 21:38:26 INFO DAGScheduler: Job 4 finished: collect at utils.scala:196, took 176.430904 s
17/12/20 21:38:26 INFO BlockManagerInfo: Removed taskresult_13 on 127.0.0.1:55138 in memory (size: 1312.6 KB, free: 1718.8 MB)
17/12/20 21:38:26 INFO CodeGenerator: Code generated in 25.171808 ms
17/12/20 21:38:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:38:28 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:38:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:38:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:38:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:38:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:38:28 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:38:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:38:28 INFO CodeGenerator: Code generated in 6.935406 ms
17/12/20 21:38:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:38:28 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:38:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:38:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:38:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:38:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:38:28 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:38:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:38:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:38:28 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:38:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:38:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:38:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:38:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:38:28 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:38:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:38:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:38:28 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:38:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:38:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:38:28 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:38:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:38:28 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:38:28 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:47:10 INFO SparkContext: Invoking stop() from shutdown hook
17/12/20 21:47:10 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/12/20 21:47:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/12/20 21:47:10 INFO MemoryStore: MemoryStore cleared
17/12/20 21:47:10 INFO BlockManager: BlockManager stopped
17/12/20 21:47:10 INFO BlockManagerMaster: BlockManagerMaster stopped
17/12/20 21:47:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/12/20 21:47:10 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-8efd1c73-e3db-46bf-850b-0dc835d2ddde\userFiles-648ba4bd-2ba8-4f60-9e91-170624c60054
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-8efd1c73-e3db-46bf-850b-0dc835d2ddde\userFiles-648ba4bd-2ba8-4f60-9e91-170624c60054
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:102)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1842)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1841)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 21:47:10 INFO SparkContext: Successfully stopped SparkContext
17/12/20 21:47:10 INFO ShutdownHookManager: Shutdown hook called
17/12/20 21:47:10 INFO ShutdownHookManager: Deleting directory C:\Users\conan\AppData\Local\Temp\spark-8efd1c73-e3db-46bf-850b-0dc835d2ddde
17/12/20 21:47:10 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-8efd1c73-e3db-46bf-850b-0dc835d2ddde
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-8efd1c73-e3db-46bf-850b-0dc835d2ddde
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 21:47:10 INFO ShutdownHookManager: Deleting directory C:\Users\conan\AppData\Local\Temp\spark-8efd1c73-e3db-46bf-850b-0dc835d2ddde\userFiles-648ba4bd-2ba8-4f60-9e91-170624c60054
17/12/20 21:47:10 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-8efd1c73-e3db-46bf-850b-0dc835d2ddde\userFiles-648ba4bd-2ba8-4f60-9e91-170624c60054
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-8efd1c73-e3db-46bf-850b-0dc835d2ddde\userFiles-648ba4bd-2ba8-4f60-9e91-170624c60054
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 21:47:19 INFO SparkContext: Running Spark version 2.1.0
17/12/20 21:47:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/12/20 21:47:20 INFO SecurityManager: Changing view acls to: conan
17/12/20 21:47:20 INFO SecurityManager: Changing modify acls to: conan
17/12/20 21:47:20 INFO SecurityManager: Changing view acls groups to: 
17/12/20 21:47:20 INFO SecurityManager: Changing modify acls groups to: 
17/12/20 21:47:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(conan); groups with view permissions: Set(); users  with modify permissions: Set(conan); groups with modify permissions: Set()
17/12/20 21:47:20 INFO Utils: Successfully started service 'sparkDriver' on port 55340.
17/12/20 21:47:20 INFO SparkEnv: Registering MapOutputTracker
17/12/20 21:47:20 INFO SparkEnv: Registering BlockManagerMaster
17/12/20 21:47:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/12/20 21:47:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/12/20 21:47:20 INFO DiskBlockManager: Created local directory at C:\Users\conan\AppData\Local\Temp\blockmgr-971a135b-0205-4d26-961d-c9e65171328c
17/12/20 21:47:20 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
17/12/20 21:47:20 INFO SparkEnv: Registering OutputCommitCoordinator
17/12/20 21:47:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/12/20 21:47:20 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
17/12/20 21:47:20 INFO SparkContext: Added JAR file:/C:/Users/conan/Documents/R/win-library/3.4/sparklyr/java/sparklyr-2.1-2.11.jar at spark://127.0.0.1:55340/jars/sparklyr-2.1-2.11.jar with timestamp 1513806440633
17/12/20 21:47:20 INFO Executor: Starting executor ID driver on host localhost
17/12/20 21:47:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55361.
17/12/20 21:47:20 INFO NettyBlockTransferService: Server created on 127.0.0.1:55361
17/12/20 21:47:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/12/20 21:47:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 55361, None)
17/12/20 21:47:20 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:55361 with 2004.6 MB RAM, BlockManagerId(driver, 127.0.0.1, 55361, None)
17/12/20 21:47:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 55361, None)
17/12/20 21:47:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 55361, None)
17/12/20 21:47:21 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
17/12/20 21:47:21 INFO SharedState: Warehouse path is 'C:UsersconanAppDataLocalsparkspark-2.1.0-bin-hadoop2.7	mphive'.
17/12/20 21:47:21 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/12/20 21:47:21 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/12/20 21:47:21 INFO ObjectStore: ObjectStore, initialize called
17/12/20 21:47:22 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/12/20 21:47:22 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/12/20 21:47:23 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/12/20 21:47:24 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:47:24 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:47:24 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:47:24 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:47:25 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/12/20 21:47:25 INFO ObjectStore: Initialized ObjectStore
17/12/20 21:47:25 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/12/20 21:47:25 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/12/20 21:47:25 INFO HiveMetaStore: Added admin role in metastore
17/12/20 21:47:25 INFO HiveMetaStore: Added public role in metastore
17/12/20 21:47:25 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/12/20 21:47:25 INFO HiveMetaStore: 0: get_all_databases
17/12/20 21:47:25 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_all_databases	
17/12/20 21:47:25 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/12/20 21:47:25 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/12/20 21:47:25 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:47:25 INFO SessionState: Created local directory: C:/Users/conan/AppData/Local/Temp/f9a8b03a-d8e7-4418-92e6-b29d4e849e7e_resources
17/12/20 21:47:25 INFO SessionState: Created HDFS directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/conan/f9a8b03a-d8e7-4418-92e6-b29d4e849e7e
17/12/20 21:47:26 INFO SessionState: Created local directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/f9a8b03a-d8e7-4418-92e6-b29d4e849e7e
17/12/20 21:47:26 INFO SessionState: Created HDFS directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/conan/f9a8b03a-d8e7-4418-92e6-b29d4e849e7e/_tmp_space.db
17/12/20 21:47:26 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:UsersconanAppDataLocalsparkspark-2.1.0-bin-hadoop2.7	mphive
17/12/20 21:47:26 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:47:26 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:47:26 INFO HiveMetaStore: 0: get_database: global_temp
17/12/20 21:47:26 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: global_temp	
17/12/20 21:47:26 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
17/12/20 21:47:26 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:47:27 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:47:27 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:47:27 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:47:27 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:47:27 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:47:27 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:53:39 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:53:39 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:53:39 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:53:39 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:53:39 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:53:39 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:53:39 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:53:39 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:53:40 INFO CodeGenerator: Code generated in 260.014295 ms
17/12/20 21:53:40 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 21:53:40 INFO DAGScheduler: Got job 0 (collect at utils.scala:58) with 1 output partitions
17/12/20 21:53:40 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:58)
17/12/20 21:53:40 INFO DAGScheduler: Parents of final stage: List()
17/12/20 21:53:40 INFO DAGScheduler: Missing parents: List()
17/12/20 21:53:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at map at utils.scala:55), which has no missing parents
17/12/20 21:53:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 8.7 KB, free 2004.6 MB)
17/12/20 21:53:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2004.6 MB)
17/12/20 21:53:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:55361 (size: 4.6 KB, free: 2004.6 MB)
17/12/20 21:53:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996
17/12/20 21:53:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at utils.scala:55)
17/12/20 21:53:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/12/20 21:53:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6041 bytes)
17/12/20 21:53:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/12/20 21:53:40 INFO Executor: Fetching spark://127.0.0.1:55340/jars/sparklyr-2.1-2.11.jar with timestamp 1513806440633
17/12/20 21:53:40 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:55340 after 12 ms (0 ms spent in bootstraps)
17/12/20 21:53:40 INFO Utils: Fetching spark://127.0.0.1:55340/jars/sparklyr-2.1-2.11.jar to C:\Users\conan\AppData\Local\Temp\spark-2a63e84e-bf59-46b0-a336-847453d03e3a\userFiles-b0f617c8-b9bd-4ce4-9918-91ec7dfd9191\fetchFileTemp4717133478819862225.tmp
17/12/20 21:53:40 INFO Executor: Adding file:/C:/Users/conan/AppData/Local/Temp/spark-2a63e84e-bf59-46b0-a336-847453d03e3a/userFiles-b0f617c8-b9bd-4ce4-9918-91ec7dfd9191/sparklyr-2.1-2.11.jar to class loader
17/12/20 21:53:40 INFO CodeGenerator: Code generated in 18.965511 ms
17/12/20 21:53:40 INFO CodeGenerator: Code generated in 17.0474 ms
17/12/20 21:53:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1231 bytes result sent to driver
17/12/20 21:53:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 333 ms on localhost (executor driver) (1/1)
17/12/20 21:53:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/12/20 21:53:41 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:58) finished in 0.375 s
17/12/20 21:53:41 INFO DAGScheduler: Job 0 finished: collect at utils.scala:58, took 0.534439 s
17/12/20 21:53:41 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:53:41 INFO ContextCleaner: Cleaned accumulator 0
17/12/20 21:53:41 INFO ContextCleaner: Cleaned accumulator 1
17/12/20 21:53:41 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 21:53:41 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:53:41 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:55361 in memory (size: 4.6 KB, free: 2004.6 MB)
17/12/20 21:53:41 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 21:53:41 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 21:53:41 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 21:53:41 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 21:53:41 INFO FileSourceStrategy: Output Data Schema: struct<S1: double>
17/12/20 21:53:41 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 21:53:41 INFO CodeGenerator: Code generated in 6.262557 ms
17/12/20 21:53:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 293.7 KB, free 2004.3 MB)
17/12/20 21:53:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2004.3 MB)
17/12/20 21:53:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:55361 (size: 24.0 KB, free: 2004.6 MB)
17/12/20 21:53:41 INFO SparkContext: Created broadcast 1 from sql at <unknown>:0
17/12/20 21:53:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 21:53:41 INFO CodeGenerator: Code generated in 11.632512 ms
17/12/20 21:53:41 INFO CodeGenerator: Code generated in 10.798814 ms
17/12/20 21:53:41 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 21:53:41 INFO DAGScheduler: Registering RDD 10 (sql at <unknown>:0)
17/12/20 21:53:41 INFO DAGScheduler: Registering RDD 15 (sql at <unknown>:0)
17/12/20 21:53:41 INFO DAGScheduler: Got job 1 (sql at <unknown>:0) with 1 output partitions
17/12/20 21:53:41 INFO DAGScheduler: Final stage: ResultStage 3 (sql at <unknown>:0)
17/12/20 21:53:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
17/12/20 21:53:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
17/12/20 21:53:41 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[10] at sql at <unknown>:0), which has no missing parents
17/12/20 21:53:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.8 KB, free 2004.3 MB)
17/12/20 21:53:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.2 KB, free 2004.3 MB)
17/12/20 21:53:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:55361 (size: 7.2 KB, free: 2004.6 MB)
17/12/20 21:53:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/12/20 21:53:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[10] at sql at <unknown>:0)
17/12/20 21:53:41 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/12/20 21:53:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 6676 bytes)
17/12/20 21:53:41 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/12/20 21:53:41 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/Rtmp0CG1So/spark_serialize_cef55e15806d97893e714781e98ff59d1d32b59075dd0bc8a043495fbb8f7748.csv, range: 0-198260, partition values: [empty row]
17/12/20 21:53:41 INFO CodeGenerator: Code generated in 5.724504 ms
17/12/20 21:53:42 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1809 bytes result sent to driver
17/12/20 21:53:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 251 ms on localhost (executor driver) (1/1)
17/12/20 21:53:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/12/20 21:53:42 INFO DAGScheduler: ShuffleMapStage 1 (sql at <unknown>:0) finished in 0.251 s
17/12/20 21:53:42 INFO DAGScheduler: looking for newly runnable stages
17/12/20 21:53:42 INFO DAGScheduler: running: Set()
17/12/20 21:53:42 INFO DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)
17/12/20 21:53:42 INFO DAGScheduler: failed: Set()
17/12/20 21:53:42 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[15] at sql at <unknown>:0), which has no missing parents
17/12/20 21:53:42 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.3 MB)
17/12/20 21:53:42 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.6 KB, free 2004.3 MB)
17/12/20 21:53:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:55361 (size: 5.6 KB, free: 2004.6 MB)
17/12/20 21:53:42 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/12/20 21:53:42 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[15] at sql at <unknown>:0)
17/12/20 21:53:42 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
17/12/20 21:53:42 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5942 bytes)
17/12/20 21:53:42 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3, localhost, executor driver, partition 1, ANY, 5942 bytes)
17/12/20 21:53:42 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/12/20 21:53:42 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)
17/12/20 21:53:42 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 21:53:42 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 21:53:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 21:53:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 21:53:42 INFO MemoryStore: Block rdd_12_0 stored as values in memory (estimated size 39.3 KB, free 2004.2 MB)
17/12/20 21:53:42 INFO MemoryStore: Block rdd_12_1 stored as values in memory (estimated size 39.3 KB, free 2004.2 MB)
17/12/20 21:53:42 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:55361 (size: 39.3 KB, free: 2004.5 MB)
17/12/20 21:53:42 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:55361 (size: 39.3 KB, free: 2004.5 MB)
17/12/20 21:53:42 INFO CodeGenerator: Code generated in 3.830936 ms
17/12/20 21:53:42 INFO CodeGenerator: Code generated in 16.435342 ms
17/12/20 21:53:42 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3151 bytes result sent to driver
17/12/20 21:53:42 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 3151 bytes result sent to driver
17/12/20 21:53:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 133 ms on localhost (executor driver) (1/2)
17/12/20 21:53:42 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 132 ms on localhost (executor driver) (2/2)
17/12/20 21:53:42 INFO DAGScheduler: ShuffleMapStage 2 (sql at <unknown>:0) finished in 0.134 s
17/12/20 21:53:42 INFO DAGScheduler: looking for newly runnable stages
17/12/20 21:53:42 INFO DAGScheduler: running: Set()
17/12/20 21:53:42 INFO DAGScheduler: waiting: Set(ResultStage 3)
17/12/20 21:53:42 INFO DAGScheduler: failed: Set()
17/12/20 21:53:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/12/20 21:53:42 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at sql at <unknown>:0), which has no missing parents
17/12/20 21:53:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 2004.2 MB)
17/12/20 21:53:42 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2004.2 MB)
17/12/20 21:53:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:55361 (size: 3.7 KB, free: 2004.5 MB)
17/12/20 21:53:42 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:996
17/12/20 21:53:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at sql at <unknown>:0)
17/12/20 21:53:42 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/12/20 21:53:42 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4, localhost, executor driver, partition 0, ANY, 5953 bytes)
17/12/20 21:53:42 INFO Executor: Running task 0.0 in stage 3.0 (TID 4)
17/12/20 21:53:42 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 21:53:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 21:53:42 INFO Executor: Finished task 0.0 in stage 3.0 (TID 4). 1707 bytes result sent to driver
17/12/20 21:53:42 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 0 ms on localhost (executor driver) (1/1)
17/12/20 21:53:42 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/12/20 21:53:42 INFO DAGScheduler: ResultStage 3 (sql at <unknown>:0) finished in 0.000 s
17/12/20 21:53:42 INFO DAGScheduler: Job 1 finished: sql at <unknown>:0, took 0.474197 s
17/12/20 21:53:42 INFO CodeGenerator: Code generated in 5.687501 ms
17/12/20 21:53:42 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:53:42 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 21:53:42 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 21:53:42 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 145 bytes
17/12/20 21:53:42 INFO DAGScheduler: Registering RDD 22 (collect at utils.scala:196)
17/12/20 21:53:42 INFO DAGScheduler: Got job 2 (collect at utils.scala:196) with 1 output partitions
17/12/20 21:53:42 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:196)
17/12/20 21:53:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/12/20 21:53:42 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/12/20 21:53:42 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[22] at collect at utils.scala:196), which has no missing parents
17/12/20 21:53:42 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 10.9 KB, free 2004.2 MB)
17/12/20 21:53:42 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.6 KB, free 2004.2 MB)
17/12/20 21:53:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:55361 (size: 5.6 KB, free: 2004.5 MB)
17/12/20 21:53:42 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:996
17/12/20 21:53:42 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[22] at collect at utils.scala:196)
17/12/20 21:53:42 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/12/20 21:53:42 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 5934 bytes)
17/12/20 21:53:42 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6, localhost, executor driver, partition 1, PROCESS_LOCAL, 5934 bytes)
17/12/20 21:53:42 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
17/12/20 21:53:42 INFO Executor: Running task 1.0 in stage 5.0 (TID 6)
17/12/20 21:53:42 INFO BlockManager: Found block rdd_12_1 locally
17/12/20 21:53:42 INFO BlockManager: Found block rdd_12_0 locally
17/12/20 21:53:42 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1871 bytes result sent to driver
17/12/20 21:53:42 INFO Executor: Finished task 1.0 in stage 5.0 (TID 6). 1950 bytes result sent to driver
17/12/20 21:53:42 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 31 ms on localhost (executor driver) (1/2)
17/12/20 21:53:42 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 16 ms on localhost (executor driver) (2/2)
17/12/20 21:53:42 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:196) finished in 0.031 s
17/12/20 21:53:42 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/12/20 21:53:42 INFO DAGScheduler: looking for newly runnable stages
17/12/20 21:53:42 INFO DAGScheduler: running: Set()
17/12/20 21:53:42 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/12/20 21:53:42 INFO DAGScheduler: failed: Set()
17/12/20 21:53:42 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[25] at collect at utils.scala:196), which has no missing parents
17/12/20 21:53:42 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.0 KB, free 2004.1 MB)
17/12/20 21:53:42 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2004.1 MB)
17/12/20 21:53:42 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:55361 (size: 3.7 KB, free: 2004.5 MB)
17/12/20 21:53:42 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:996
17/12/20 21:53:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[25] at collect at utils.scala:196)
17/12/20 21:53:42 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/12/20 21:53:42 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 7, localhost, executor driver, partition 0, ANY, 5945 bytes)
17/12/20 21:53:42 INFO Executor: Running task 0.0 in stage 6.0 (TID 7)
17/12/20 21:53:42 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 21:53:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 21:53:42 INFO Executor: Finished task 0.0 in stage 6.0 (TID 7). 1707 bytes result sent to driver
17/12/20 21:53:42 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:196) finished in 0.016 s
17/12/20 21:53:42 INFO DAGScheduler: Job 2 finished: collect at utils.scala:196, took 0.054156 s
17/12/20 21:53:42 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 7) in 16 ms on localhost (executor driver) (1/1)
17/12/20 21:53:42 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/12/20 21:53:42 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:53:42 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz8`
WHERE (0 = 1)
17/12/20 21:53:42 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:53:42 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`, `V11` AS `V11`, `V12` AS `V12`, `V13` AS `V13`, `V14` AS `V14`, `V15` AS `V15`, `V16` AS `V16`, `V17` AS `V17`, `V18` AS `V18`, `V19` AS `V19`, `V20` AS `V20`, `V21` AS `V21`, `V22` AS `V22`, `V23` AS `V23`, `V24` AS `V24`, `V25` AS `V25`, `V26` AS `V26`, `V27` AS `V27`, `V28` AS `V28`, `V29` AS `V29`, `V30` AS `V30`, `V31` AS `V31`, `V32` AS `V32`, `V33` AS `V33`, `V34` AS `V34`, `V35` AS `V35`, `V36` AS `V36`, `V37` AS `V37`, `V38` AS `V38`, `V39` AS `V39`, `V40` AS `V40`, `V41` AS `V41`, `V42` AS `V42`, `V43` AS `V43`, `V44` AS `V44`, `V45` AS `V45`, `V46` AS `V46`, `V47` AS `V47`, `V48` AS `V48`, `V49` AS `V49`, `V50` AS `V50`, `V51` AS `V51`, `V52` AS `V52`, `V53` AS `V53`, `V54` AS `V54`, `V55` AS `V55`, `V56` AS `V56`, `V57` AS `V57`, `V58` AS `V58`, `V59` AS `V59`, `V60` AS `V60`, `V61` AS `V61`, `V62` AS `V62`, `V63` AS `V63`, `V64` AS `V64`, `V65` AS `V65`, `V66` AS `V66`, `V67` AS `V67`, `V68` AS `V68`, `V69` AS `V69`, `V70` AS `V70`, `V71` AS `V71`, `V72` AS `V72`, `V73` AS `V73`, `V74` AS `V74`, `V75` AS `V75`, `V76` AS `V76`, `V77` AS `V77`, `V78` AS `V78`, `V79` AS `V79`, `V80` AS `V80`, `V81` AS `V81`, `V82` AS `V82`, `V83` AS `V83`, `V84` AS `V84`, `V85` AS `V85`, `V86` AS `V86`, `V87` AS `V87`, `V88` AS `V88`, `V89` AS `V89`, `V90` AS `V90`, `V91` AS `V91`, `V92` AS `V92`, `V93` AS `V93`, `V94` AS `V94`, `V95` AS `V95`, `V96` AS `V96`, `V97` AS `V97`, `V98` AS `V98`, `V99` AS `V99`, `V100` AS `V100`
FROM (SELECT `S1`, `S1` + 0.9797959 * RANDN() AS `V1`, `S1` + 0.9797959 * RANDN() AS `V2`, `S1` + 0.9797959 * RANDN() AS `V3`, `S1` + 0.9797959 * RANDN() AS `V4`, `S1` + 0.9797959 * RANDN() AS `V5`, `S1` + 0.9797959 * RANDN() AS `V6`, `S1` + 0.9797959 * RANDN() AS `V7`, `S1` + 0.9797959 * RANDN() AS `V8`, `S1` + 0.9797959 * RANDN() AS `V9`, `S1` + 0.9797959 * RANDN() AS `V10`, `S1` + 0.9797959 * RANDN() AS `V11`, `S1` + 0.9797959 * RANDN() AS `V12`, `S1` + 0.9797959 * RANDN() AS `V13`, `S1` + 0.9797959 * RANDN() AS `V14`, `S1` + 0.9797959 * RANDN() AS `V15`, `S1` + 0.9797959 * RANDN() AS `V16`, `S1` + 0.9797959 * RANDN() AS `V17`, `S1` + 0.9797959 * RANDN() AS `V18`, `S1` + 0.9797959 * RANDN() AS `V19`, `S1` + 0.9797959 * RANDN() AS `V20`, `S1` + 0.9797959 * RANDN() AS `V21`, `S1` + 0.9797959 * RANDN() AS `V22`, `S1` + 0.9797959 * RANDN() AS `V23`, `S1` + 0.9797959 * RANDN() AS `V24`, `S1` + 0.9797959 * RANDN() AS `V25`, `S1` + 0.9797959 * RANDN() AS `V26`, `S1` + 0.9797959 * RANDN() AS `V27`, `S1` + 0.9797959 * RANDN() AS `V28`, `S1` + 0.9797959 * RANDN() AS `V29`, `S1` + 0.9797959 * RANDN() AS `V30`, `S1` + 0.9797959 * RANDN() AS `V31`, `S1` + 0.9797959 * RANDN() AS `V32`, `S1` + 0.9797959 * RANDN() AS `V33`, `S1` + 0.9797959 * RANDN() AS `V34`, `S1` + 0.9797959 * RANDN() AS `V35`, `S1` + 0.9797959 * RANDN() AS `V36`, `S1` + 0.9797959 * RANDN() AS `V37`, `S1` + 0.9797959 * RANDN() AS `V38`, `S1` + 0.9797959 * RANDN() AS `V39`, `S1` + 0.9797959 * RANDN() AS `V40`, `S1` + 0.9797959 * RANDN() AS `V41`, `S1` + 0.9797959 * RANDN() AS `V42`, `S1` + 0.9797959 * RANDN() AS `V43`, `S1` + 0.9797959 * RANDN() AS `V44`, `S1` + 0.9797959 * RANDN() AS `V45`, `S1` + 0.9797959 * RANDN() AS `V46`, `S1` + 0.9797959 * RANDN() AS `V47`, `S1` + 0.9797959 * RANDN() AS `V48`, `S1` + 0.9797959 * RANDN() AS `V49`, `S1` + 0.9797959 * RANDN() AS `V50`, `S1` + 0.9797959 * RANDN() AS `V51`, `S1` + 0.9797959 * RANDN() AS `V52`, `S1` + 0.9797959 * RANDN() AS `V53`, `S1` + 0.9797959 * RANDN() AS `V54`, `S1` + 0.9797959 * RANDN() AS `V55`, `S1` + 0.9797959 * RANDN() AS `V56`, `S1` + 0.9797959 * RANDN() AS `V57`, `S1` + 0.9797959 * RANDN() AS `V58`, `S1` + 0.9797959 * RANDN() AS `V59`, `S1` + 0.9797959 * RANDN() AS `V60`, `S1` + 0.9797959 * RANDN() AS `V61`, `S1` + 0.9797959 * RANDN() AS `V62`, `S1` + 0.9797959 * RANDN() AS `V63`, `S1` + 0.9797959 * RANDN() AS `V64`, `S1` + 0.9797959 * RANDN() AS `V65`, `S1` + 0.9797959 * RANDN() AS `V66`, `S1` + 0.9797959 * RANDN() AS `V67`, `S1` + 0.9797959 * RANDN() AS `V68`, `S1` + 0.9797959 * RANDN() AS `V69`, `S1` + 0.9797959 * RANDN() AS `V70`, `S1` + 0.9797959 * RANDN() AS `V71`, `S1` + 0.9797959 * RANDN() AS `V72`, `S1` + 0.9797959 * RANDN() AS `V73`, `S1` + 0.9797959 * RANDN() AS `V74`, `S1` + 0.9797959 * RANDN() AS `V75`, `S1` + 0.9797959 * RANDN() AS `V76`, `S1` + 0.9797959 * RANDN() AS `V77`, `S1` + 0.9797959 * RANDN() AS `V78`, `S1` + 0.9797959 * RANDN() AS `V79`, `S1` + 0.9797959 * RANDN() AS `V80`, `S1` + 0.9797959 * RANDN() AS `V81`, `S1` + 0.9797959 * RANDN() AS `V82`, `S1` + 0.9797959 * RANDN() AS `V83`, `S1` + 0.9797959 * RANDN() AS `V84`, `S1` + 0.9797959 * RANDN() AS `V85`, `S1` + 0.9797959 * RANDN() AS `V86`, `S1` + 0.9797959 * RANDN() AS `V87`, `S1` + 0.9797959 * RANDN() AS `V88`, `S1` + 0.9797959 * RANDN() AS `V89`, `S1` + 0.9797959 * RANDN() AS `V90`, `S1` + 0.9797959 * RANDN() AS `V91`, `S1` + 0.9797959 * RANDN() AS `V92`, `S1` + 0.9797959 * RANDN() AS `V93`, `S1` + 0.9797959 * RANDN() AS `V94`, `S1` + 0.9797959 * RANDN() AS `V95`, `S1` + 0.9797959 * RANDN() AS `V96`, `S1` + 0.9797959 * RANDN() AS `V97`, `S1` + 0.9797959 * RANDN() AS `V98`, `S1` + 0.9797959 * RANDN() AS `V99`, `S1` + 0.9797959 * RANDN() AS `V100`
FROM `analyis_tbl`) `epdzdjmdey`
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 50
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 51
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 57
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 58
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 59
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 60
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 61
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 62
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 63
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 64
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 65
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 66
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 67
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 68
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 69
17/12/20 21:53:43 INFO ContextCleaner: Cleaned shuffle 1
17/12/20 21:53:43 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:55361 in memory (size: 7.2 KB, free: 2004.5 MB)
17/12/20 21:53:43 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:55361 in memory (size: 5.6 KB, free: 2004.5 MB)
17/12/20 21:53:43 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 21:53:43 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:55361 in memory (size: 3.7 KB, free: 2004.5 MB)
17/12/20 21:53:43 INFO ContextCleaner: Cleaned accumulator 238
17/12/20 21:53:43 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:55361 in memory (size: 5.6 KB, free: 2004.5 MB)
17/12/20 21:53:43 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 127.0.0.1:55361 in memory (size: 3.7 KB, free: 2004.5 MB)
17/12/20 21:53:43 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:53:43 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz9`
WHERE (0 = 1)
17/12/20 21:53:43 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:53:43 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 21:53:43 INFO CodeGenerator: Code generated in 137.324288 ms
17/12/20 21:53:43 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 21:53:43 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 145 bytes
17/12/20 21:53:43 INFO DAGScheduler: Got job 3 (take at <unknown>:0) with 1 output partitions
17/12/20 21:53:43 INFO DAGScheduler: Final stage: ResultStage 8 (take at <unknown>:0)
17/12/20 21:53:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
17/12/20 21:53:43 INFO DAGScheduler: Missing parents: List()
17/12/20 21:53:43 INFO DAGScheduler: Submitting ResultStage 8 (WorkerRDD[31] at RDD at rdd.scala:18), which has no missing parents
17/12/20 21:53:43 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 275.3 KB, free 2003.9 MB)
17/12/20 21:53:43 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 136.1 KB, free 2003.8 MB)
17/12/20 21:53:43 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:55361 (size: 136.1 KB, free: 2004.4 MB)
17/12/20 21:53:43 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:996
17/12/20 21:53:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (WorkerRDD[31] at RDD at rdd.scala:18)
17/12/20 21:53:43 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/12/20 21:53:43 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5913 bytes)
17/12/20 21:53:43 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
17/12/20 21:53:43 INFO BlockManager: Found block rdd_12_0 locally
17/12/20 21:53:43 INFO CodeGenerator: Code generated in 18.058562 ms
17/12/20 21:53:43 INFO CodeGenerator: Code generated in 51.846767 ms
17/12/20 21:54:01 INFO MemoryStore: Block rdd_31_0 stored as values in memory (estimated size 13.5 MB, free 1990.3 MB)
17/12/20 21:54:01 INFO BlockManagerInfo: Added rdd_31_0 in memory on 127.0.0.1:55361 (size: 13.5 MB, free: 1990.8 MB)
17/12/20 21:54:01 WARN Executor: 1 block locks were not released by TID = 8:
[rdd_31_0]
17/12/20 21:54:01 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 16774 bytes result sent to driver
17/12/20 21:54:01 INFO DAGScheduler: ResultStage 8 (take at <unknown>:0) finished in 17.773 s
17/12/20 21:54:01 INFO DAGScheduler: Job 3 finished: take at <unknown>:0, took 17.785590 s
17/12/20 21:54:01 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 17773 ms on localhost (executor driver) (1/1)
17/12/20 21:54:01 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/12/20 21:54:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:54:01 INFO SparkSqlParser: Parsing command: sparklyr_tmp_17146e845067
17/12/20 21:54:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:54:01 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_17146e845067` AS `zzz10`
WHERE (0 = 1)
17/12/20 21:54:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:54:01 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_17146e845067`
17/12/20 21:54:01 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 21:54:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:54:01 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz11`
WHERE (0 = 1)
17/12/20 21:54:02 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:54:02 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.03) AS `V1`, (`V2` < 0.03) AS `V2`, (`V3` < 0.03) AS `V3`, (`V4` < 0.03) AS `V4`, (`V5` < 0.03) AS `V5`, (`V6` < 0.03) AS `V6`, (`V7` < 0.03) AS `V7`, (`V8` < 0.03) AS `V8`, (`V9` < 0.03) AS `V9`, (`V10` < 0.03) AS `V10`, (`V11` < 0.03) AS `V11`, (`V12` < 0.03) AS `V12`, (`V13` < 0.03) AS `V13`, (`V14` < 0.03) AS `V14`, (`V15` < 0.03) AS `V15`, (`V16` < 0.03) AS `V16`, (`V17` < 0.03) AS `V17`, (`V18` < 0.03) AS `V18`, (`V19` < 0.03) AS `V19`, (`V20` < 0.03) AS `V20`, (`V21` < 0.03) AS `V21`, (`V22` < 0.03) AS `V22`, (`V23` < 0.03) AS `V23`, (`V24` < 0.03) AS `V24`, (`V25` < 0.03) AS `V25`, (`V26` < 0.03) AS `V26`, (`V27` < 0.03) AS `V27`, (`V28` < 0.03) AS `V28`, (`V29` < 0.03) AS `V29`, (`V30` < 0.03) AS `V30`, (`V31` < 0.03) AS `V31`, (`V32` < 0.03) AS `V32`, (`V33` < 0.03) AS `V33`, (`V34` < 0.03) AS `V34`, (`V35` < 0.03) AS `V35`, (`V36` < 0.03) AS `V36`, (`V37` < 0.03) AS `V37`, (`V38` < 0.03) AS `V38`, (`V39` < 0.03) AS `V39`, (`V40` < 0.03) AS `V40`, (`V41` < 0.03) AS `V41`, (`V42` < 0.03) AS `V42`, (`V43` < 0.03) AS `V43`, (`V44` < 0.03) AS `V44`, (`V45` < 0.03) AS `V45`, (`V46` < 0.03) AS `V46`, (`V47` < 0.03) AS `V47`, (`V48` < 0.03) AS `V48`, (`V49` < 0.03) AS `V49`, (`V50` < 0.03) AS `V50`, (`V51` < 0.03) AS `V51`, (`V52` < 0.03) AS `V52`, (`V53` < 0.03) AS `V53`, (`V54` < 0.03) AS `V54`, (`V55` < 0.03) AS `V55`, (`V56` < 0.03) AS `V56`, (`V57` < 0.03) AS `V57`, (`V58` < 0.03) AS `V58`, (`V59` < 0.03) AS `V59`, (`V60` < 0.03) AS `V60`, (`V61` < 0.03) AS `V61`, (`V62` < 0.03) AS `V62`, (`V63` < 0.03) AS `V63`, (`V64` < 0.03) AS `V64`, (`V65` < 0.03) AS `V65`, (`V66` < 0.03) AS `V66`, (`V67` < 0.03) AS `V67`, (`V68` < 0.03) AS `V68`, (`V69` < 0.03) AS `V69`, (`V70` < 0.03) AS `V70`, (`V71` < 0.03) AS `V71`, (`V72` < 0.03) AS `V72`, (`V73` < 0.03) AS `V73`, (`V74` < 0.03) AS `V74`, (`V75` < 0.03) AS `V75`, (`V76` < 0.03) AS `V76`, (`V77` < 0.03) AS `V77`, (`V78` < 0.03) AS `V78`, (`V79` < 0.03) AS `V79`, (`V80` < 0.03) AS `V80`, (`V81` < 0.03) AS `V81`, (`V82` < 0.03) AS `V82`, (`V83` < 0.03) AS `V83`, (`V84` < 0.03) AS `V84`, (`V85` < 0.03) AS `V85`, (`V86` < 0.03) AS `V86`, (`V87` < 0.03) AS `V87`, (`V88` < 0.03) AS `V88`, (`V89` < 0.03) AS `V89`, (`V90` < 0.03) AS `V90`, (`V91` < 0.03) AS `V91`, (`V92` < 0.03) AS `V92`, (`V93` < 0.03) AS `V93`, (`V94` < 0.03) AS `V94`, (`V95` < 0.03) AS `V95`, (`V96` < 0.03) AS `V96`, (`V97` < 0.03) AS `V97`, (`V98` < 0.03) AS `V98`, (`V99` < 0.03) AS `V99`, (`V100` < 0.03) AS `V100`
FROM `analyis_tbl`
17/12/20 21:54:02 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 21:54:02 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:54:02 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz12`
WHERE (0 = 1)
17/12/20 21:54:02 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:54:02 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 21:54:02 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
17/12/20 21:54:02 INFO CodeGenerator: Code generated in 62.355598 ms
17/12/20 21:54:02 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 21:54:02 INFO DAGScheduler: Got job 4 (collect at utils.scala:196) with 2 output partitions
17/12/20 21:54:02 INFO DAGScheduler: Final stage: ResultStage 10 (collect at utils.scala:196)
17/12/20 21:54:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
17/12/20 21:54:02 INFO DAGScheduler: Missing parents: List()
17/12/20 21:54:02 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[38] at collect at utils.scala:196), which has no missing parents
17/12/20 21:54:02 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 369.2 KB, free 1989.9 MB)
17/12/20 21:54:02 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 155.8 KB, free 1989.8 MB)
17/12/20 21:54:02 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:55361 (size: 155.8 KB, free: 1990.7 MB)
17/12/20 21:54:02 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:996
17/12/20 21:54:02 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[38] at collect at utils.scala:196)
17/12/20 21:54:02 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
17/12/20 21:54:02 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 5945 bytes)
17/12/20 21:54:02 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 5945 bytes)
17/12/20 21:54:02 INFO Executor: Running task 0.0 in stage 10.0 (TID 9)
17/12/20 21:54:02 INFO Executor: Running task 1.0 in stage 10.0 (TID 10)
17/12/20 21:54:02 INFO BlockManager: Found block rdd_31_0 locally
17/12/20 21:54:02 INFO BlockManager: Found block rdd_12_1 locally
17/12/20 21:54:02 INFO CodeGenerator: Code generated in 85.036129 ms
17/12/20 21:54:02 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 127.0.0.1:55361 in memory (size: 136.1 KB, free: 1990.8 MB)
17/12/20 21:54:02 INFO CodeGenerator: Code generated in 253.524812 ms
17/12/20 21:54:03 INFO Executor: Finished task 0.0 in stage 10.0 (TID 9). 125519 bytes result sent to driver
17/12/20 21:54:03 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 772 ms on localhost (executor driver) (1/2)
17/12/20 21:54:20 INFO MemoryStore: Block rdd_31_1 stored as values in memory (estimated size 13.5 MB, free 1976.7 MB)
17/12/20 21:54:20 INFO BlockManagerInfo: Added rdd_31_1 in memory on 127.0.0.1:55361 (size: 13.5 MB, free: 1977.3 MB)
17/12/20 21:54:20 INFO Executor: Finished task 1.0 in stage 10.0 (TID 10). 127516 bytes result sent to driver
17/12/20 21:54:20 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 10) in 18076 ms on localhost (executor driver) (2/2)
17/12/20 21:54:20 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
17/12/20 21:54:20 INFO DAGScheduler: ResultStage 10 (collect at utils.scala:196) finished in 18.076 s
17/12/20 21:54:20 INFO DAGScheduler: Job 4 finished: collect at utils.scala:196, took 18.095730 s
17/12/20 21:54:20 INFO CodeGenerator: Code generated in 26.738392 ms
17/12/20 21:54:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:54:20 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:54:20 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:54:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:54:20 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:54:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:54:20 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:54:20 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:54:21 INFO CodeGenerator: Code generated in 72.614095 ms
17/12/20 21:54:21 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 127.0.0.1:55361 in memory (size: 155.8 KB, free: 1977.5 MB)
17/12/20 21:54:21 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:54:21 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:54:21 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:54:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:54:21 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:54:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:54:21 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:54:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:54:21 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:54:21 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:54:21 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:54:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:54:21 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:54:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:54:21 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:54:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:54:21 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:54:21 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:54:21 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:54:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:54:21 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:54:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:54:21 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:54:21 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:55:25 INFO SparkContext: Invoking stop() from shutdown hook
17/12/20 21:55:25 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/12/20 21:55:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/12/20 21:55:25 INFO MemoryStore: MemoryStore cleared
17/12/20 21:55:25 INFO BlockManager: BlockManager stopped
17/12/20 21:55:25 INFO BlockManagerMaster: BlockManagerMaster stopped
17/12/20 21:55:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/12/20 21:55:25 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-2a63e84e-bf59-46b0-a336-847453d03e3a\userFiles-b0f617c8-b9bd-4ce4-9918-91ec7dfd9191
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-2a63e84e-bf59-46b0-a336-847453d03e3a\userFiles-b0f617c8-b9bd-4ce4-9918-91ec7dfd9191
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:102)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1842)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1841)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 21:55:25 INFO SparkContext: Successfully stopped SparkContext
17/12/20 21:55:25 INFO ShutdownHookManager: Shutdown hook called
17/12/20 21:55:25 INFO ShutdownHookManager: Deleting directory C:\Users\conan\AppData\Local\Temp\spark-2a63e84e-bf59-46b0-a336-847453d03e3a
17/12/20 21:55:25 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-2a63e84e-bf59-46b0-a336-847453d03e3a
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-2a63e84e-bf59-46b0-a336-847453d03e3a
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 21:55:25 INFO ShutdownHookManager: Deleting directory C:\Users\conan\AppData\Local\Temp\spark-2a63e84e-bf59-46b0-a336-847453d03e3a\userFiles-b0f617c8-b9bd-4ce4-9918-91ec7dfd9191
17/12/20 21:55:25 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-2a63e84e-bf59-46b0-a336-847453d03e3a\userFiles-b0f617c8-b9bd-4ce4-9918-91ec7dfd9191
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-2a63e84e-bf59-46b0-a336-847453d03e3a\userFiles-b0f617c8-b9bd-4ce4-9918-91ec7dfd9191
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 21:56:04 INFO SparkContext: Running Spark version 2.1.0
17/12/20 21:56:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/12/20 21:56:05 INFO SecurityManager: Changing view acls to: conan
17/12/20 21:56:05 INFO SecurityManager: Changing modify acls to: conan
17/12/20 21:56:05 INFO SecurityManager: Changing view acls groups to: 
17/12/20 21:56:05 INFO SecurityManager: Changing modify acls groups to: 
17/12/20 21:56:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(conan); groups with view permissions: Set(); users  with modify permissions: Set(conan); groups with modify permissions: Set()
17/12/20 21:56:05 INFO Utils: Successfully started service 'sparkDriver' on port 55520.
17/12/20 21:56:05 INFO SparkEnv: Registering MapOutputTracker
17/12/20 21:56:05 INFO SparkEnv: Registering BlockManagerMaster
17/12/20 21:56:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/12/20 21:56:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/12/20 21:56:05 INFO DiskBlockManager: Created local directory at C:\Users\conan\AppData\Local\Temp\blockmgr-f2531cf5-6d23-45cd-b135-1f8e0dccbbc3
17/12/20 21:56:05 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
17/12/20 21:56:05 INFO SparkEnv: Registering OutputCommitCoordinator
17/12/20 21:56:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/12/20 21:56:05 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
17/12/20 21:56:05 INFO SparkContext: Added JAR file:/C:/Users/conan/Documents/R/win-library/3.4/sparklyr/java/sparklyr-2.1-2.11.jar at spark://127.0.0.1:55520/jars/sparklyr-2.1-2.11.jar with timestamp 1513806965851
17/12/20 21:56:05 INFO Executor: Starting executor ID driver on host localhost
17/12/20 21:56:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55541.
17/12/20 21:56:05 INFO NettyBlockTransferService: Server created on 127.0.0.1:55541
17/12/20 21:56:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/12/20 21:56:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 55541, None)
17/12/20 21:56:05 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:55541 with 2004.6 MB RAM, BlockManagerId(driver, 127.0.0.1, 55541, None)
17/12/20 21:56:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 55541, None)
17/12/20 21:56:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 55541, None)
17/12/20 21:56:06 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
17/12/20 21:56:06 INFO SharedState: Warehouse path is 'C:UsersconanAppDataLocalsparkspark-2.1.0-bin-hadoop2.7	mphive'.
17/12/20 21:56:06 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/12/20 21:56:07 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/12/20 21:56:07 INFO ObjectStore: ObjectStore, initialize called
17/12/20 21:56:07 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/12/20 21:56:07 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/12/20 21:56:08 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/12/20 21:56:09 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:56:09 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:56:10 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:56:10 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:56:10 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/12/20 21:56:10 INFO ObjectStore: Initialized ObjectStore
17/12/20 21:56:10 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/12/20 21:56:10 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/12/20 21:56:10 INFO HiveMetaStore: Added admin role in metastore
17/12/20 21:56:10 INFO HiveMetaStore: Added public role in metastore
17/12/20 21:56:10 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/12/20 21:56:10 INFO HiveMetaStore: 0: get_all_databases
17/12/20 21:56:10 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_all_databases	
17/12/20 21:56:10 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/12/20 21:56:10 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/12/20 21:56:10 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/12/20 21:56:11 INFO SessionState: Created local directory: C:/Users/conan/AppData/Local/Temp/09785964-41cb-4567-a864-44203919c70f_resources
17/12/20 21:56:11 INFO SessionState: Created HDFS directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/conan/09785964-41cb-4567-a864-44203919c70f
17/12/20 21:56:11 INFO SessionState: Created local directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/09785964-41cb-4567-a864-44203919c70f
17/12/20 21:56:11 INFO SessionState: Created HDFS directory: C:/Users/conan/AppData/Local/spark/spark-2.1.0-bin-hadoop2.7/tmp/hive/conan/09785964-41cb-4567-a864-44203919c70f/_tmp_space.db
17/12/20 21:56:11 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:UsersconanAppDataLocalsparkspark-2.1.0-bin-hadoop2.7	mphive
17/12/20 21:56:11 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:56:11 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:56:11 INFO HiveMetaStore: 0: get_database: global_temp
17/12/20 21:56:11 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: global_temp	
17/12/20 21:56:11 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
17/12/20 21:56:11 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:56:13 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:56:13 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:56:13 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:56:13 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:56:13 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:56:13 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:56:16 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:56:16 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 21:56:16 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:56:16 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:56:16 INFO HiveMetaStore: 0: get_database: default
17/12/20 21:56:16 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 21:56:16 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 21:56:16 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 21:56:16 INFO CodeGenerator: Code generated in 263.486529 ms
17/12/20 21:56:16 INFO SparkContext: Starting job: collect at utils.scala:58
17/12/20 21:56:16 INFO DAGScheduler: Got job 0 (collect at utils.scala:58) with 1 output partitions
17/12/20 21:56:16 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:58)
17/12/20 21:56:16 INFO DAGScheduler: Parents of final stage: List()
17/12/20 21:56:16 INFO DAGScheduler: Missing parents: List()
17/12/20 21:56:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at map at utils.scala:55), which has no missing parents
17/12/20 21:56:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 8.7 KB, free 2004.6 MB)
17/12/20 21:56:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2004.6 MB)
17/12/20 21:56:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:55541 (size: 4.6 KB, free: 2004.6 MB)
17/12/20 21:56:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996
17/12/20 21:56:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at map at utils.scala:55)
17/12/20 21:56:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/12/20 21:56:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6041 bytes)
17/12/20 21:56:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/12/20 21:56:16 INFO Executor: Fetching spark://127.0.0.1:55520/jars/sparklyr-2.1-2.11.jar with timestamp 1513806965851
17/12/20 21:56:16 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:55520 after 17 ms (0 ms spent in bootstraps)
17/12/20 21:56:16 INFO Utils: Fetching spark://127.0.0.1:55520/jars/sparklyr-2.1-2.11.jar to C:\Users\conan\AppData\Local\Temp\spark-e0657139-e685-443e-bb73-8129febf2cbf\userFiles-df549e01-61c0-4eb9-84b0-a2b84c9f1994\fetchFileTemp8515508354494895515.tmp
17/12/20 21:56:17 INFO Executor: Adding file:/C:/Users/conan/AppData/Local/Temp/spark-e0657139-e685-443e-bb73-8129febf2cbf/userFiles-df549e01-61c0-4eb9-84b0-a2b84c9f1994/sparklyr-2.1-2.11.jar to class loader
17/12/20 21:56:17 INFO CodeGenerator: Code generated in 14.800416 ms
17/12/20 21:56:17 INFO CodeGenerator: Code generated in 14.087921 ms
17/12/20 21:56:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1304 bytes result sent to driver
17/12/20 21:56:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 370 ms on localhost (executor driver) (1/1)
17/12/20 21:56:17 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:58) finished in 0.398 s
17/12/20 21:56:17 INFO DAGScheduler: Job 0 finished: collect at utils.scala:58, took 0.592904 s
17/12/20 21:56:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/12/20 21:56:17 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:56:17 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 21:56:17 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:56:17 INFO SparkSqlParser: Parsing command: CACHE TABLE `analyis_tbl`
17/12/20 21:56:17 INFO SparkSqlParser: Parsing command: `analyis_tbl`
17/12/20 21:56:17 INFO FileSourceStrategy: Pruning directories with: 
17/12/20 21:56:17 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/20 21:56:17 INFO FileSourceStrategy: Output Data Schema: struct<S1: double>
17/12/20 21:56:17 INFO FileSourceStrategy: Pushed Filters: 
17/12/20 21:56:17 INFO CodeGenerator: Code generated in 9.282072 ms
17/12/20 21:56:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 293.7 KB, free 2004.3 MB)
17/12/20 21:56:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 24.0 KB, free 2004.3 MB)
17/12/20 21:56:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:55541 (size: 24.0 KB, free: 2004.6 MB)
17/12/20 21:56:17 INFO SparkContext: Created broadcast 1 from sql at <unknown>:0
17/12/20 21:56:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/20 21:56:17 INFO CodeGenerator: Code generated in 14.008629 ms
17/12/20 21:56:17 INFO CodeGenerator: Code generated in 11.529055 ms
17/12/20 21:56:17 INFO SparkContext: Starting job: sql at <unknown>:0
17/12/20 21:56:17 INFO DAGScheduler: Registering RDD 10 (sql at <unknown>:0)
17/12/20 21:56:17 INFO DAGScheduler: Registering RDD 15 (sql at <unknown>:0)
17/12/20 21:56:17 INFO DAGScheduler: Got job 1 (sql at <unknown>:0) with 1 output partitions
17/12/20 21:56:17 INFO DAGScheduler: Final stage: ResultStage 3 (sql at <unknown>:0)
17/12/20 21:56:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
17/12/20 21:56:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
17/12/20 21:56:17 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[10] at sql at <unknown>:0), which has no missing parents
17/12/20 21:56:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.8 KB, free 2004.3 MB)
17/12/20 21:56:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.2 KB, free 2004.3 MB)
17/12/20 21:56:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:55541 (size: 7.2 KB, free: 2004.6 MB)
17/12/20 21:56:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
17/12/20 21:56:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[10] at sql at <unknown>:0)
17/12/20 21:56:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/12/20 21:56:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 6676 bytes)
17/12/20 21:56:17 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/12/20 21:56:18 INFO FileScanRDD: Reading File path: file:///C:/Users/conan/AppData/Local/Temp/Rtmp0CG1So/spark_serialize_299ff3d1bf658a1a753dd18532c5f9c3a92c8a5f9a27f8c3a67a906ec3a0eeb6.csv, range: 0-198074, partition values: [empty row]
17/12/20 21:56:18 INFO CodeGenerator: Code generated in 7.297506 ms
17/12/20 21:56:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1719 bytes result sent to driver
17/12/20 21:56:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 232 ms on localhost (executor driver) (1/1)
17/12/20 21:56:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/12/20 21:56:18 INFO DAGScheduler: ShuffleMapStage 1 (sql at <unknown>:0) finished in 0.232 s
17/12/20 21:56:18 INFO DAGScheduler: looking for newly runnable stages
17/12/20 21:56:18 INFO DAGScheduler: running: Set()
17/12/20 21:56:18 INFO DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)
17/12/20 21:56:18 INFO DAGScheduler: failed: Set()
17/12/20 21:56:18 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[15] at sql at <unknown>:0), which has no missing parents
17/12/20 21:56:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 2004.2 MB)
17/12/20 21:56:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.6 KB, free 2004.2 MB)
17/12/20 21:56:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:55541 (size: 5.6 KB, free: 2004.6 MB)
17/12/20 21:56:18 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
17/12/20 21:56:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[15] at sql at <unknown>:0)
17/12/20 21:56:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
17/12/20 21:56:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 5942 bytes)
17/12/20 21:56:18 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3, localhost, executor driver, partition 1, ANY, 5942 bytes)
17/12/20 21:56:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/12/20 21:56:18 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)
17/12/20 21:56:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 21:56:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
17/12/20 21:56:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 21:56:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 21:56:18 INFO MemoryStore: Block rdd_12_0 stored as values in memory (estimated size 39.3 KB, free 2004.2 MB)
17/12/20 21:56:18 INFO BlockManagerInfo: Added rdd_12_0 in memory on 127.0.0.1:55541 (size: 39.3 KB, free: 2004.5 MB)
17/12/20 21:56:18 INFO MemoryStore: Block rdd_12_1 stored as values in memory (estimated size 39.3 KB, free 2004.2 MB)
17/12/20 21:56:18 INFO BlockManagerInfo: Added rdd_12_1 in memory on 127.0.0.1:55541 (size: 39.3 KB, free: 2004.5 MB)
17/12/20 21:56:18 INFO CodeGenerator: Code generated in 3.876246 ms
17/12/20 21:56:18 INFO CodeGenerator: Code generated in 16.982078 ms
17/12/20 21:56:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3064 bytes result sent to driver
17/12/20 21:56:18 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 3064 bytes result sent to driver
17/12/20 21:56:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 163 ms on localhost (executor driver) (1/2)
17/12/20 21:56:18 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 147 ms on localhost (executor driver) (2/2)
17/12/20 21:56:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/12/20 21:56:18 INFO DAGScheduler: ShuffleMapStage 2 (sql at <unknown>:0) finished in 0.163 s
17/12/20 21:56:18 INFO DAGScheduler: looking for newly runnable stages
17/12/20 21:56:18 INFO DAGScheduler: running: Set()
17/12/20 21:56:18 INFO DAGScheduler: waiting: Set(ResultStage 3)
17/12/20 21:56:18 INFO DAGScheduler: failed: Set()
17/12/20 21:56:18 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at sql at <unknown>:0), which has no missing parents
17/12/20 21:56:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 2004.2 MB)
17/12/20 21:56:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2004.2 MB)
17/12/20 21:56:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:55541 (size: 3.7 KB, free: 2004.5 MB)
17/12/20 21:56:18 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:996
17/12/20 21:56:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at sql at <unknown>:0)
17/12/20 21:56:18 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/12/20 21:56:18 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4, localhost, executor driver, partition 0, ANY, 5953 bytes)
17/12/20 21:56:18 INFO Executor: Running task 0.0 in stage 3.0 (TID 4)
17/12/20 21:56:18 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 21:56:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 21:56:18 INFO Executor: Finished task 0.0 in stage 3.0 (TID 4). 1707 bytes result sent to driver
17/12/20 21:56:18 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 0 ms on localhost (executor driver) (1/1)
17/12/20 21:56:18 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/12/20 21:56:18 INFO DAGScheduler: ResultStage 3 (sql at <unknown>:0) finished in 0.015 s
17/12/20 21:56:18 INFO DAGScheduler: Job 1 finished: sql at <unknown>:0, took 0.456160 s
17/12/20 21:56:18 INFO CodeGenerator: Code generated in 6.185153 ms
17/12/20 21:56:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:56:18 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `analyis_tbl`
17/12/20 21:56:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:55541 in memory (size: 4.6 KB, free: 2004.5 MB)
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 50
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 51
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 57
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 58
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 59
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 60
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 61
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 62
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 63
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 64
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 65
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 66
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 67
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 68
17/12/20 21:56:18 INFO ContextCleaner: Cleaned accumulator 69
17/12/20 21:56:18 INFO ContextCleaner: Cleaned shuffle 1
17/12/20 21:56:18 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:55541 in memory (size: 7.2 KB, free: 2004.5 MB)
17/12/20 21:56:18 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 21:56:18 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 145 bytes
17/12/20 21:56:18 INFO DAGScheduler: Registering RDD 22 (collect at utils.scala:196)
17/12/20 21:56:18 INFO DAGScheduler: Got job 2 (collect at utils.scala:196) with 1 output partitions
17/12/20 21:56:18 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:196)
17/12/20 21:56:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/12/20 21:56:18 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:55541 in memory (size: 5.6 KB, free: 2004.5 MB)
17/12/20 21:56:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/12/20 21:56:18 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[22] at collect at utils.scala:196), which has no missing parents
17/12/20 21:56:18 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 10.9 KB, free 2004.2 MB)
17/12/20 21:56:18 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.6 KB, free 2004.2 MB)
17/12/20 21:56:18 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:55541 (size: 5.6 KB, free: 2004.5 MB)
17/12/20 21:56:18 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:996
17/12/20 21:56:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[22] at collect at utils.scala:196)
17/12/20 21:56:18 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/12/20 21:56:18 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:55541 in memory (size: 3.7 KB, free: 2004.5 MB)
17/12/20 21:56:18 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 5934 bytes)
17/12/20 21:56:18 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6, localhost, executor driver, partition 1, PROCESS_LOCAL, 5934 bytes)
17/12/20 21:56:18 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
17/12/20 21:56:18 INFO Executor: Running task 1.0 in stage 5.0 (TID 6)
17/12/20 21:56:18 INFO BlockManager: Found block rdd_12_1 locally
17/12/20 21:56:18 INFO BlockManager: Found block rdd_12_0 locally
17/12/20 21:56:18 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2037 bytes result sent to driver
17/12/20 21:56:18 INFO Executor: Finished task 1.0 in stage 5.0 (TID 6). 2048 bytes result sent to driver
17/12/20 21:56:18 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 31 ms on localhost (executor driver) (1/2)
17/12/20 21:56:18 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:196) finished in 0.031 s
17/12/20 21:56:18 INFO DAGScheduler: looking for newly runnable stages
17/12/20 21:56:18 INFO DAGScheduler: running: Set()
17/12/20 21:56:18 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/12/20 21:56:18 INFO DAGScheduler: failed: Set()
17/12/20 21:56:18 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[25] at collect at utils.scala:196), which has no missing parents
17/12/20 21:56:18 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 31 ms on localhost (executor driver) (2/2)
17/12/20 21:56:18 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/12/20 21:56:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.0 KB, free 2004.2 MB)
17/12/20 21:56:18 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2004.2 MB)
17/12/20 21:56:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:55541 (size: 3.7 KB, free: 2004.5 MB)
17/12/20 21:56:18 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:996
17/12/20 21:56:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[25] at collect at utils.scala:196)
17/12/20 21:56:18 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/12/20 21:56:18 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 7, localhost, executor driver, partition 0, ANY, 5945 bytes)
17/12/20 21:56:18 INFO Executor: Running task 0.0 in stage 6.0 (TID 7)
17/12/20 21:56:18 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/12/20 21:56:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/12/20 21:56:18 INFO Executor: Finished task 0.0 in stage 6.0 (TID 7). 1707 bytes result sent to driver
17/12/20 21:56:18 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:196) finished in 0.016 s
17/12/20 21:56:18 INFO DAGScheduler: Job 2 finished: collect at utils.scala:196, took 0.057958 s
17/12/20 21:56:18 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 7) in 16 ms on localhost (executor driver) (1/1)
17/12/20 21:56:18 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/12/20 21:56:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:56:18 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz13`
WHERE (0 = 1)
17/12/20 21:56:21 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:56:21 INFO SparkSqlParser: Parsing command: SELECT `V1` AS `V1`, `V2` AS `V2`, `V3` AS `V3`, `V4` AS `V4`, `V5` AS `V5`, `V6` AS `V6`, `V7` AS `V7`, `V8` AS `V8`, `V9` AS `V9`, `V10` AS `V10`, `V11` AS `V11`, `V12` AS `V12`, `V13` AS `V13`, `V14` AS `V14`, `V15` AS `V15`, `V16` AS `V16`, `V17` AS `V17`, `V18` AS `V18`, `V19` AS `V19`, `V20` AS `V20`, `V21` AS `V21`, `V22` AS `V22`, `V23` AS `V23`, `V24` AS `V24`, `V25` AS `V25`, `V26` AS `V26`, `V27` AS `V27`, `V28` AS `V28`, `V29` AS `V29`, `V30` AS `V30`, `V31` AS `V31`, `V32` AS `V32`, `V33` AS `V33`, `V34` AS `V34`, `V35` AS `V35`, `V36` AS `V36`, `V37` AS `V37`, `V38` AS `V38`, `V39` AS `V39`, `V40` AS `V40`, `V41` AS `V41`, `V42` AS `V42`, `V43` AS `V43`, `V44` AS `V44`, `V45` AS `V45`, `V46` AS `V46`, `V47` AS `V47`, `V48` AS `V48`, `V49` AS `V49`, `V50` AS `V50`, `V51` AS `V51`, `V52` AS `V52`, `V53` AS `V53`, `V54` AS `V54`, `V55` AS `V55`, `V56` AS `V56`, `V57` AS `V57`, `V58` AS `V58`, `V59` AS `V59`, `V60` AS `V60`, `V61` AS `V61`, `V62` AS `V62`, `V63` AS `V63`, `V64` AS `V64`, `V65` AS `V65`, `V66` AS `V66`, `V67` AS `V67`, `V68` AS `V68`, `V69` AS `V69`, `V70` AS `V70`, `V71` AS `V71`, `V72` AS `V72`, `V73` AS `V73`, `V74` AS `V74`, `V75` AS `V75`, `V76` AS `V76`, `V77` AS `V77`, `V78` AS `V78`, `V79` AS `V79`, `V80` AS `V80`, `V81` AS `V81`, `V82` AS `V82`, `V83` AS `V83`, `V84` AS `V84`, `V85` AS `V85`, `V86` AS `V86`, `V87` AS `V87`, `V88` AS `V88`, `V89` AS `V89`, `V90` AS `V90`, `V91` AS `V91`, `V92` AS `V92`, `V93` AS `V93`, `V94` AS `V94`, `V95` AS `V95`, `V96` AS `V96`, `V97` AS `V97`, `V98` AS `V98`, `V99` AS `V99`, `V100` AS `V100`, `V101` AS `V101`, `V102` AS `V102`, `V103` AS `V103`, `V104` AS `V104`, `V105` AS `V105`, `V106` AS `V106`, `V107` AS `V107`, `V108` AS `V108`, `V109` AS `V109`, `V110` AS `V110`, `V111` AS `V111`, `V112` AS `V112`, `V113` AS `V113`, `V114` AS `V114`, `V115` AS `V115`, `V116` AS `V116`, `V117` AS `V117`, `V118` AS `V118`, `V119` AS `V119`, `V120` AS `V120`, `V121` AS `V121`, `V122` AS `V122`, `V123` AS `V123`, `V124` AS `V124`, `V125` AS `V125`, `V126` AS `V126`, `V127` AS `V127`, `V128` AS `V128`, `V129` AS `V129`, `V130` AS `V130`, `V131` AS `V131`, `V132` AS `V132`, `V133` AS `V133`, `V134` AS `V134`, `V135` AS `V135`, `V136` AS `V136`, `V137` AS `V137`, `V138` AS `V138`, `V139` AS `V139`, `V140` AS `V140`, `V141` AS `V141`, `V142` AS `V142`, `V143` AS `V143`, `V144` AS `V144`, `V145` AS `V145`, `V146` AS `V146`, `V147` AS `V147`, `V148` AS `V148`, `V149` AS `V149`, `V150` AS `V150`, `V151` AS `V151`, `V152` AS `V152`, `V153` AS `V153`, `V154` AS `V154`, `V155` AS `V155`, `V156` AS `V156`, `V157` AS `V157`, `V158` AS `V158`, `V159` AS `V159`, `V160` AS `V160`, `V161` AS `V161`, `V162` AS `V162`, `V163` AS `V163`, `V164` AS `V164`, `V165` AS `V165`, `V166` AS `V166`, `V167` AS `V167`, `V168` AS `V168`, `V169` AS `V169`, `V170` AS `V170`, `V171` AS `V171`, `V172` AS `V172`, `V173` AS `V173`, `V174` AS `V174`, `V175` AS `V175`, `V176` AS `V176`, `V177` AS `V177`, `V178` AS `V178`, `V179` AS `V179`, `V180` AS `V180`, `V181` AS `V181`, `V182` AS `V182`, `V183` AS `V183`, `V184` AS `V184`, `V185` AS `V185`, `V186` AS `V186`, `V187` AS `V187`, `V188` AS `V188`, `V189` AS `V189`, `V190` AS `V190`, `V191` AS `V191`, `V192` AS `V192`, `V193` AS `V193`, `V194` AS `V194`, `V195` AS `V195`, `V196` AS `V196`, `V197` AS `V197`, `V198` AS `V198`, `V199` AS `V199`, `V200` AS `V200`, `V201` AS `V201`, `V202` AS `V202`, `V203` AS `V203`, `V204` AS `V204`, `V205` AS `V205`, `V206` AS `V206`, `V207` AS `V207`, `V208` AS `V208`, `V209` AS `V209`, `V210` AS `V210`, `V211` AS `V211`, `V212` AS `V212`, `V213` AS `V213`, `V214` AS `V214`, `V215` AS `V215`, `V216` AS `V216`, `V217` AS `V217`, `V218` AS `V218`, `V219` AS `V219`, `V220` AS `V220`, `V221` AS `V221`, `V222` AS `V222`, `V223` AS `V223`, `V224` AS `V224`, `V225` AS `V225`, `V226` AS `V226`, `V227` AS `V227`, `V228` AS `V228`, `V229` AS `V229`, `V230` AS `V230`, `V231` AS `V231`, `V232` AS `V232`, `V233` AS `V233`, `V234` AS `V234`, `V235` AS `V235`, `V236` AS `V236`, `V237` AS `V237`, `V238` AS `V238`, `V239` AS `V239`, `V240` AS `V240`, `V241` AS `V241`, `V242` AS `V242`, `V243` AS `V243`, `V244` AS `V244`, `V245` AS `V245`, `V246` AS `V246`, `V247` AS `V247`, `V248` AS `V248`, `V249` AS `V249`, `V250` AS `V250`, `V251` AS `V251`, `V252` AS `V252`, `V253` AS `V253`, `V254` AS `V254`, `V255` AS `V255`, `V256` AS `V256`, `V257` AS `V257`, `V258` AS `V258`, `V259` AS `V259`, `V260` AS `V260`, `V261` AS `V261`, `V262` AS `V262`, `V263` AS `V263`, `V264` AS `V264`, `V265` AS `V265`, `V266` AS `V266`, `V267` AS `V267`, `V268` AS `V268`, `V269` AS `V269`, `V270` AS `V270`, `V271` AS `V271`, `V272` AS `V272`, `V273` AS `V273`, `V274` AS `V274`, `V275` AS `V275`, `V276` AS `V276`, `V277` AS `V277`, `V278` AS `V278`, `V279` AS `V279`, `V280` AS `V280`, `V281` AS `V281`, `V282` AS `V282`, `V283` AS `V283`, `V284` AS `V284`, `V285` AS `V285`, `V286` AS `V286`, `V287` AS `V287`, `V288` AS `V288`, `V289` AS `V289`, `V290` AS `V290`, `V291` AS `V291`, `V292` AS `V292`, `V293` AS `V293`, `V294` AS `V294`, `V295` AS `V295`, `V296` AS `V296`, `V297` AS `V297`, `V298` AS `V298`, `V299` AS `V299`, `V300` AS `V300`, `V301` AS `V301`, `V302` AS `V302`, `V303` AS `V303`, `V304` AS `V304`, `V305` AS `V305`, `V306` AS `V306`, `V307` AS `V307`, `V308` AS `V308`, `V309` AS `V309`, `V310` AS `V310`, `V311` AS `V311`, `V312` AS `V312`, `V313` AS `V313`, `V314` AS `V314`, `V315` AS `V315`, `V316` AS `V316`, `V317` AS `V317`, `V318` AS `V318`, `V319` AS `V319`, `V320` AS `V320`, `V321` AS `V321`, `V322` AS `V322`, `V323` AS `V323`, `V324` AS `V324`, `V325` AS `V325`, `V326` AS `V326`, `V327` AS `V327`, `V328` AS `V328`, `V329` AS `V329`, `V330` AS `V330`, `V331` AS `V331`, `V332` AS `V332`, `V333` AS `V333`, `V334` AS `V334`, `V335` AS `V335`, `V336` AS `V336`, `V337` AS `V337`, `V338` AS `V338`, `V339` AS `V339`, `V340` AS `V340`, `V341` AS `V341`, `V342` AS `V342`, `V343` AS `V343`, `V344` AS `V344`, `V345` AS `V345`, `V346` AS `V346`, `V347` AS `V347`, `V348` AS `V348`, `V349` AS `V349`, `V350` AS `V350`, `V351` AS `V351`, `V352` AS `V352`, `V353` AS `V353`, `V354` AS `V354`, `V355` AS `V355`, `V356` AS `V356`, `V357` AS `V357`, `V358` AS `V358`, `V359` AS `V359`, `V360` AS `V360`, `V361` AS `V361`, `V362` AS `V362`, `V363` AS `V363`, `V364` AS `V364`, `V365` AS `V365`, `V366` AS `V366`, `V367` AS `V367`, `V368` AS `V368`, `V369` AS `V369`, `V370` AS `V370`, `V371` AS `V371`, `V372` AS `V372`, `V373` AS `V373`, `V374` AS `V374`, `V375` AS `V375`, `V376` AS `V376`, `V377` AS `V377`, `V378` AS `V378`, `V379` AS `V379`, `V380` AS `V380`, `V381` AS `V381`, `V382` AS `V382`, `V383` AS `V383`, `V384` AS `V384`, `V385` AS `V385`, `V386` AS `V386`, `V387` AS `V387`, `V388` AS `V388`, `V389` AS `V389`, `V390` AS `V390`, `V391` AS `V391`, `V392` AS `V392`, `V393` AS `V393`, `V394` AS `V394`, `V395` AS `V395`, `V396` AS `V396`, `V397` AS `V397`, `V398` AS `V398`, `V399` AS `V399`, `V400` AS `V400`, `V401` AS `V401`, `V402` AS `V402`, `V403` AS `V403`, `V404` AS `V404`, `V405` AS `V405`, `V406` AS `V406`, `V407` AS `V407`, `V408` AS `V408`, `V409` AS `V409`, `V410` AS `V410`, `V411` AS `V411`, `V412` AS `V412`, `V413` AS `V413`, `V414` AS `V414`, `V415` AS `V415`, `V416` AS `V416`, `V417` AS `V417`, `V418` AS `V418`, `V419` AS `V419`, `V420` AS `V420`, `V421` AS `V421`, `V422` AS `V422`, `V423` AS `V423`, `V424` AS `V424`, `V425` AS `V425`, `V426` AS `V426`, `V427` AS `V427`, `V428` AS `V428`, `V429` AS `V429`, `V430` AS `V430`, `V431` AS `V431`, `V432` AS `V432`, `V433` AS `V433`, `V434` AS `V434`, `V435` AS `V435`, `V436` AS `V436`, `V437` AS `V437`, `V438` AS `V438`, `V439` AS `V439`, `V440` AS `V440`, `V441` AS `V441`, `V442` AS `V442`, `V443` AS `V443`, `V444` AS `V444`, `V445` AS `V445`, `V446` AS `V446`, `V447` AS `V447`, `V448` AS `V448`, `V449` AS `V449`, `V450` AS `V450`, `V451` AS `V451`, `V452` AS `V452`, `V453` AS `V453`, `V454` AS `V454`, `V455` AS `V455`, `V456` AS `V456`, `V457` AS `V457`, `V458` AS `V458`, `V459` AS `V459`, `V460` AS `V460`, `V461` AS `V461`, `V462` AS `V462`, `V463` AS `V463`, `V464` AS `V464`, `V465` AS `V465`, `V466` AS `V466`, `V467` AS `V467`, `V468` AS `V468`, `V469` AS `V469`, `V470` AS `V470`, `V471` AS `V471`, `V472` AS `V472`, `V473` AS `V473`, `V474` AS `V474`, `V475` AS `V475`, `V476` AS `V476`, `V477` AS `V477`, `V478` AS `V478`, `V479` AS `V479`, `V480` AS `V480`, `V481` AS `V481`, `V482` AS `V482`, `V483` AS `V483`, `V484` AS `V484`, `V485` AS `V485`, `V486` AS `V486`, `V487` AS `V487`, `V488` AS `V488`, `V489` AS `V489`, `V490` AS `V490`, `V491` AS `V491`, `V492` AS `V492`, `V493` AS `V493`, `V494` AS `V494`, `V495` AS `V495`, `V496` AS `V496`, `V497` AS `V497`, `V498` AS `V498`, `V499` AS `V499`, `V500` AS `V500`, `V501` AS `V501`, `V502` AS `V502`, `V503` AS `V503`, `V504` AS `V504`, `V505` AS `V505`, `V506` AS `V506`, `V507` AS `V507`, `V508` AS `V508`, `V509` AS `V509`, `V510` AS `V510`, `V511` AS `V511`, `V512` AS `V512`, `V513` AS `V513`, `V514` AS `V514`, `V515` AS `V515`, `V516` AS `V516`, `V517` AS `V517`, `V518` AS `V518`, `V519` AS `V519`, `V520` AS `V520`, `V521` AS `V521`, `V522` AS `V522`, `V523` AS `V523`, `V524` AS `V524`, `V525` AS `V525`, `V526` AS `V526`, `V527` AS `V527`, `V528` AS `V528`, `V529` AS `V529`, `V530` AS `V530`, `V531` AS `V531`, `V532` AS `V532`, `V533` AS `V533`, `V534` AS `V534`, `V535` AS `V535`, `V536` AS `V536`, `V537` AS `V537`, `V538` AS `V538`, `V539` AS `V539`, `V540` AS `V540`, `V541` AS `V541`, `V542` AS `V542`, `V543` AS `V543`, `V544` AS `V544`, `V545` AS `V545`, `V546` AS `V546`, `V547` AS `V547`, `V548` AS `V548`, `V549` AS `V549`, `V550` AS `V550`, `V551` AS `V551`, `V552` AS `V552`, `V553` AS `V553`, `V554` AS `V554`, `V555` AS `V555`, `V556` AS `V556`, `V557` AS `V557`, `V558` AS `V558`, `V559` AS `V559`, `V560` AS `V560`, `V561` AS `V561`, `V562` AS `V562`, `V563` AS `V563`, `V564` AS `V564`, `V565` AS `V565`, `V566` AS `V566`, `V567` AS `V567`, `V568` AS `V568`, `V569` AS `V569`, `V570` AS `V570`, `V571` AS `V571`, `V572` AS `V572`, `V573` AS `V573`, `V574` AS `V574`, `V575` AS `V575`, `V576` AS `V576`, `V577` AS `V577`, `V578` AS `V578`, `V579` AS `V579`, `V580` AS `V580`, `V581` AS `V581`, `V582` AS `V582`, `V583` AS `V583`, `V584` AS `V584`, `V585` AS `V585`, `V586` AS `V586`, `V587` AS `V587`, `V588` AS `V588`, `V589` AS `V589`, `V590` AS `V590`, `V591` AS `V591`, `V592` AS `V592`, `V593` AS `V593`, `V594` AS `V594`, `V595` AS `V595`, `V596` AS `V596`, `V597` AS `V597`, `V598` AS `V598`, `V599` AS `V599`, `V600` AS `V600`, `V601` AS `V601`, `V602` AS `V602`, `V603` AS `V603`, `V604` AS `V604`, `V605` AS `V605`, `V606` AS `V606`, `V607` AS `V607`, `V608` AS `V608`, `V609` AS `V609`, `V610` AS `V610`, `V611` AS `V611`, `V612` AS `V612`, `V613` AS `V613`, `V614` AS `V614`, `V615` AS `V615`, `V616` AS `V616`, `V617` AS `V617`, `V618` AS `V618`, `V619` AS `V619`, `V620` AS `V620`, `V621` AS `V621`, `V622` AS `V622`, `V623` AS `V623`, `V624` AS `V624`, `V625` AS `V625`, `V626` AS `V626`, `V627` AS `V627`, `V628` AS `V628`, `V629` AS `V629`, `V630` AS `V630`, `V631` AS `V631`, `V632` AS `V632`, `V633` AS `V633`, `V634` AS `V634`, `V635` AS `V635`, `V636` AS `V636`, `V637` AS `V637`, `V638` AS `V638`, `V639` AS `V639`, `V640` AS `V640`, `V641` AS `V641`, `V642` AS `V642`, `V643` AS `V643`, `V644` AS `V644`, `V645` AS `V645`, `V646` AS `V646`, `V647` AS `V647`, `V648` AS `V648`, `V649` AS `V649`, `V650` AS `V650`, `V651` AS `V651`, `V652` AS `V652`, `V653` AS `V653`, `V654` AS `V654`, `V655` AS `V655`, `V656` AS `V656`, `V657` AS `V657`, `V658` AS `V658`, `V659` AS `V659`, `V660` AS `V660`, `V661` AS `V661`, `V662` AS `V662`, `V663` AS `V663`, `V664` AS `V664`, `V665` AS `V665`, `V666` AS `V666`, `V667` AS `V667`, `V668` AS `V668`, `V669` AS `V669`, `V670` AS `V670`, `V671` AS `V671`, `V672` AS `V672`, `V673` AS `V673`, `V674` AS `V674`, `V675` AS `V675`, `V676` AS `V676`, `V677` AS `V677`, `V678` AS `V678`, `V679` AS `V679`, `V680` AS `V680`, `V681` AS `V681`, `V682` AS `V682`, `V683` AS `V683`, `V684` AS `V684`, `V685` AS `V685`, `V686` AS `V686`, `V687` AS `V687`, `V688` AS `V688`, `V689` AS `V689`, `V690` AS `V690`, `V691` AS `V691`, `V692` AS `V692`, `V693` AS `V693`, `V694` AS `V694`, `V695` AS `V695`, `V696` AS `V696`, `V697` AS `V697`, `V698` AS `V698`, `V699` AS `V699`, `V700` AS `V700`, `V701` AS `V701`, `V702` AS `V702`, `V703` AS `V703`, `V704` AS `V704`, `V705` AS `V705`, `V706` AS `V706`, `V707` AS `V707`, `V708` AS `V708`, `V709` AS `V709`, `V710` AS `V710`, `V711` AS `V711`, `V712` AS `V712`, `V713` AS `V713`, `V714` AS `V714`, `V715` AS `V715`, `V716` AS `V716`, `V717` AS `V717`, `V718` AS `V718`, `V719` AS `V719`, `V720` AS `V720`, `V721` AS `V721`, `V722` AS `V722`, `V723` AS `V723`, `V724` AS `V724`, `V725` AS `V725`, `V726` AS `V726`, `V727` AS `V727`, `V728` AS `V728`, `V729` AS `V729`, `V730` AS `V730`, `V731` AS `V731`, `V732` AS `V732`, `V733` AS `V733`, `V734` AS `V734`, `V735` AS `V735`, `V736` AS `V736`, `V737` AS `V737`, `V738` AS `V738`, `V739` AS `V739`, `V740` AS `V740`, `V741` AS `V741`, `V742` AS `V742`, `V743` AS `V743`, `V744` AS `V744`, `V745` AS `V745`, `V746` AS `V746`, `V747` AS `V747`, `V748` AS `V748`, `V749` AS `V749`, `V750` AS `V750`, `V751` AS `V751`, `V752` AS `V752`, `V753` AS `V753`, `V754` AS `V754`, `V755` AS `V755`, `V756` AS `V756`, `V757` AS `V757`, `V758` AS `V758`, `V759` AS `V759`, `V760` AS `V760`, `V761` AS `V761`, `V762` AS `V762`, `V763` AS `V763`, `V764` AS `V764`, `V765` AS `V765`, `V766` AS `V766`, `V767` AS `V767`, `V768` AS `V768`, `V769` AS `V769`, `V770` AS `V770`, `V771` AS `V771`, `V772` AS `V772`, `V773` AS `V773`, `V774` AS `V774`, `V775` AS `V775`, `V776` AS `V776`, `V777` AS `V777`, `V778` AS `V778`, `V779` AS `V779`, `V780` AS `V780`, `V781` AS `V781`, `V782` AS `V782`, `V783` AS `V783`, `V784` AS `V784`, `V785` AS `V785`, `V786` AS `V786`, `V787` AS `V787`, `V788` AS `V788`, `V789` AS `V789`, `V790` AS `V790`, `V791` AS `V791`, `V792` AS `V792`, `V793` AS `V793`, `V794` AS `V794`, `V795` AS `V795`, `V796` AS `V796`, `V797` AS `V797`, `V798` AS `V798`, `V799` AS `V799`, `V800` AS `V800`, `V801` AS `V801`, `V802` AS `V802`, `V803` AS `V803`, `V804` AS `V804`, `V805` AS `V805`, `V806` AS `V806`, `V807` AS `V807`, `V808` AS `V808`, `V809` AS `V809`, `V810` AS `V810`, `V811` AS `V811`, `V812` AS `V812`, `V813` AS `V813`, `V814` AS `V814`, `V815` AS `V815`, `V816` AS `V816`, `V817` AS `V817`, `V818` AS `V818`, `V819` AS `V819`, `V820` AS `V820`, `V821` AS `V821`, `V822` AS `V822`, `V823` AS `V823`, `V824` AS `V824`, `V825` AS `V825`, `V826` AS `V826`, `V827` AS `V827`, `V828` AS `V828`, `V829` AS `V829`, `V830` AS `V830`, `V831` AS `V831`, `V832` AS `V832`, `V833` AS `V833`, `V834` AS `V834`, `V835` AS `V835`, `V836` AS `V836`, `V837` AS `V837`, `V838` AS `V838`, `V839` AS `V839`, `V840` AS `V840`, `V841` AS `V841`, `V842` AS `V842`, `V843` AS `V843`, `V844` AS `V844`, `V845` AS `V845`, `V846` AS `V846`, `V847` AS `V847`, `V848` AS `V848`, `V849` AS `V849`, `V850` AS `V850`, `V851` AS `V851`, `V852` AS `V852`, `V853` AS `V853`, `V854` AS `V854`, `V855` AS `V855`, `V856` AS `V856`, `V857` AS `V857`, `V858` AS `V858`, `V859` AS `V859`, `V860` AS `V860`, `V861` AS `V861`, `V862` AS `V862`, `V863` AS `V863`, `V864` AS `V864`, `V865` AS `V865`, `V866` AS `V866`, `V867` AS `V867`, `V868` AS `V868`, `V869` AS `V869`, `V870` AS `V870`, `V871` AS `V871`, `V872` AS `V872`, `V873` AS `V873`, `V874` AS `V874`, `V875` AS `V875`, `V876` AS `V876`, `V877` AS `V877`, `V878` AS `V878`, `V879` AS `V879`, `V880` AS `V880`, `V881` AS `V881`, `V882` AS `V882`, `V883` AS `V883`, `V884` AS `V884`, `V885` AS `V885`, `V886` AS `V886`, `V887` AS `V887`, `V888` AS `V888`, `V889` AS `V889`, `V890` AS `V890`, `V891` AS `V891`, `V892` AS `V892`, `V893` AS `V893`, `V894` AS `V894`, `V895` AS `V895`, `V896` AS `V896`, `V897` AS `V897`, `V898` AS `V898`, `V899` AS `V899`, `V900` AS `V900`, `V901` AS `V901`, `V902` AS `V902`, `V903` AS `V903`, `V904` AS `V904`, `V905` AS `V905`, `V906` AS `V906`, `V907` AS `V907`, `V908` AS `V908`, `V909` AS `V909`, `V910` AS `V910`, `V911` AS `V911`, `V912` AS `V912`, `V913` AS `V913`, `V914` AS `V914`, `V915` AS `V915`, `V916` AS `V916`, `V917` AS `V917`, `V918` AS `V918`, `V919` AS `V919`, `V920` AS `V920`, `V921` AS `V921`, `V922` AS `V922`, `V923` AS `V923`, `V924` AS `V924`, `V925` AS `V925`, `V926` AS `V926`, `V927` AS `V927`, `V928` AS `V928`, `V929` AS `V929`, `V930` AS `V930`, `V931` AS `V931`, `V932` AS `V932`, `V933` AS `V933`, `V934` AS `V934`, `V935` AS `V935`, `V936` AS `V936`, `V937` AS `V937`, `V938` AS `V938`, `V939` AS `V939`, `V940` AS `V940`, `V941` AS `V941`, `V942` AS `V942`, `V943` AS `V943`, `V944` AS `V944`, `V945` AS `V945`, `V946` AS `V946`, `V947` AS `V947`, `V948` AS `V948`, `V949` AS `V949`, `V950` AS `V950`, `V951` AS `V951`, `V952` AS `V952`, `V953` AS `V953`, `V954` AS `V954`, `V955` AS `V955`, `V956` AS `V956`, `V957` AS `V957`, `V958` AS `V958`, `V959` AS `V959`, `V960` AS `V960`, `V961` AS `V961`, `V962` AS `V962`, `V963` AS `V963`, `V964` AS `V964`, `V965` AS `V965`, `V966` AS `V966`, `V967` AS `V967`, `V968` AS `V968`, `V969` AS `V969`, `V970` AS `V970`, `V971` AS `V971`, `V972` AS `V972`, `V973` AS `V973`, `V974` AS `V974`, `V975` AS `V975`, `V976` AS `V976`, `V977` AS `V977`, `V978` AS `V978`, `V979` AS `V979`, `V980` AS `V980`, `V981` AS `V981`, `V982` AS `V982`, `V983` AS `V983`, `V984` AS `V984`, `V985` AS `V985`, `V986` AS `V986`, `V987` AS `V987`, `V988` AS `V988`, `V989` AS `V989`, `V990` AS `V990`, `V991` AS `V991`, `V992` AS `V992`, `V993` AS `V993`, `V994` AS `V994`, `V995` AS `V995`, `V996` AS `V996`, `V997` AS `V997`, `V998` AS `V998`, `V999` AS `V999`, `V1000` AS `V1000`
FROM (SELECT `S1`, `S1` + 0.9797959 * RANDN() AS `V1`, `S1` + 0.9797959 * RANDN() AS `V2`, `S1` + 0.9797959 * RANDN() AS `V3`, `S1` + 0.9797959 * RANDN() AS `V4`, `S1` + 0.9797959 * RANDN() AS `V5`, `S1` + 0.9797959 * RANDN() AS `V6`, `S1` + 0.9797959 * RANDN() AS `V7`, `S1` + 0.9797959 * RANDN() AS `V8`, `S1` + 0.9797959 * RANDN() AS `V9`, `S1` + 0.9797959 * RANDN() AS `V10`, `S1` + 0.9797959 * RANDN() AS `V11`, `S1` + 0.9797959 * RANDN() AS `V12`, `S1` + 0.9797959 * RANDN() AS `V13`, `S1` + 0.9797959 * RANDN() AS `V14`, `S1` + 0.9797959 * RANDN() AS `V15`, `S1` + 0.9797959 * RANDN() AS `V16`, `S1` + 0.9797959 * RANDN() AS `V17`, `S1` + 0.9797959 * RANDN() AS `V18`, `S1` + 0.9797959 * RANDN() AS `V19`, `S1` + 0.9797959 * RANDN() AS `V20`, `S1` + 0.9797959 * RANDN() AS `V21`, `S1` + 0.9797959 * RANDN() AS `V22`, `S1` + 0.9797959 * RANDN() AS `V23`, `S1` + 0.9797959 * RANDN() AS `V24`, `S1` + 0.9797959 * RANDN() AS `V25`, `S1` + 0.9797959 * RANDN() AS `V26`, `S1` + 0.9797959 * RANDN() AS `V27`, `S1` + 0.9797959 * RANDN() AS `V28`, `S1` + 0.9797959 * RANDN() AS `V29`, `S1` + 0.9797959 * RANDN() AS `V30`, `S1` + 0.9797959 * RANDN() AS `V31`, `S1` + 0.9797959 * RANDN() AS `V32`, `S1` + 0.9797959 * RANDN() AS `V33`, `S1` + 0.9797959 * RANDN() AS `V34`, `S1` + 0.9797959 * RANDN() AS `V35`, `S1` + 0.9797959 * RANDN() AS `V36`, `S1` + 0.9797959 * RANDN() AS `V37`, `S1` + 0.9797959 * RANDN() AS `V38`, `S1` + 0.9797959 * RANDN() AS `V39`, `S1` + 0.9797959 * RANDN() AS `V40`, `S1` + 0.9797959 * RANDN() AS `V41`, `S1` + 0.9797959 * RANDN() AS `V42`, `S1` + 0.9797959 * RANDN() AS `V43`, `S1` + 0.9797959 * RANDN() AS `V44`, `S1` + 0.9797959 * RANDN() AS `V45`, `S1` + 0.9797959 * RANDN() AS `V46`, `S1` + 0.9797959 * RANDN() AS `V47`, `S1` + 0.9797959 * RANDN() AS `V48`, `S1` + 0.9797959 * RANDN() AS `V49`, `S1` + 0.9797959 * RANDN() AS `V50`, `S1` + 0.9797959 * RANDN() AS `V51`, `S1` + 0.9797959 * RANDN() AS `V52`, `S1` + 0.9797959 * RANDN() AS `V53`, `S1` + 0.9797959 * RANDN() AS `V54`, `S1` + 0.9797959 * RANDN() AS `V55`, `S1` + 0.9797959 * RANDN() AS `V56`, `S1` + 0.9797959 * RANDN() AS `V57`, `S1` + 0.9797959 * RANDN() AS `V58`, `S1` + 0.9797959 * RANDN() AS `V59`, `S1` + 0.9797959 * RANDN() AS `V60`, `S1` + 0.9797959 * RANDN() AS `V61`, `S1` + 0.9797959 * RANDN() AS `V62`, `S1` + 0.9797959 * RANDN() AS `V63`, `S1` + 0.9797959 * RANDN() AS `V64`, `S1` + 0.9797959 * RANDN() AS `V65`, `S1` + 0.9797959 * RANDN() AS `V66`, `S1` + 0.9797959 * RANDN() AS `V67`, `S1` + 0.9797959 * RANDN() AS `V68`, `S1` + 0.9797959 * RANDN() AS `V69`, `S1` + 0.9797959 * RANDN() AS `V70`, `S1` + 0.9797959 * RANDN() AS `V71`, `S1` + 0.9797959 * RANDN() AS `V72`, `S1` + 0.9797959 * RANDN() AS `V73`, `S1` + 0.9797959 * RANDN() AS `V74`, `S1` + 0.9797959 * RANDN() AS `V75`, `S1` + 0.9797959 * RANDN() AS `V76`, `S1` + 0.9797959 * RANDN() AS `V77`, `S1` + 0.9797959 * RANDN() AS `V78`, `S1` + 0.9797959 * RANDN() AS `V79`, `S1` + 0.9797959 * RANDN() AS `V80`, `S1` + 0.9797959 * RANDN() AS `V81`, `S1` + 0.9797959 * RANDN() AS `V82`, `S1` + 0.9797959 * RANDN() AS `V83`, `S1` + 0.9797959 * RANDN() AS `V84`, `S1` + 0.9797959 * RANDN() AS `V85`, `S1` + 0.9797959 * RANDN() AS `V86`, `S1` + 0.9797959 * RANDN() AS `V87`, `S1` + 0.9797959 * RANDN() AS `V88`, `S1` + 0.9797959 * RANDN() AS `V89`, `S1` + 0.9797959 * RANDN() AS `V90`, `S1` + 0.9797959 * RANDN() AS `V91`, `S1` + 0.9797959 * RANDN() AS `V92`, `S1` + 0.9797959 * RANDN() AS `V93`, `S1` + 0.9797959 * RANDN() AS `V94`, `S1` + 0.9797959 * RANDN() AS `V95`, `S1` + 0.9797959 * RANDN() AS `V96`, `S1` + 0.9797959 * RANDN() AS `V97`, `S1` + 0.9797959 * RANDN() AS `V98`, `S1` + 0.9797959 * RANDN() AS `V99`, `S1` + 0.9797959 * RANDN() AS `V100`, `S1` + 0.9797959 * RANDN() AS `V101`, `S1` + 0.9797959 * RANDN() AS `V102`, `S1` + 0.9797959 * RANDN() AS `V103`, `S1` + 0.9797959 * RANDN() AS `V104`, `S1` + 0.9797959 * RANDN() AS `V105`, `S1` + 0.9797959 * RANDN() AS `V106`, `S1` + 0.9797959 * RANDN() AS `V107`, `S1` + 0.9797959 * RANDN() AS `V108`, `S1` + 0.9797959 * RANDN() AS `V109`, `S1` + 0.9797959 * RANDN() AS `V110`, `S1` + 0.9797959 * RANDN() AS `V111`, `S1` + 0.9797959 * RANDN() AS `V112`, `S1` + 0.9797959 * RANDN() AS `V113`, `S1` + 0.9797959 * RANDN() AS `V114`, `S1` + 0.9797959 * RANDN() AS `V115`, `S1` + 0.9797959 * RANDN() AS `V116`, `S1` + 0.9797959 * RANDN() AS `V117`, `S1` + 0.9797959 * RANDN() AS `V118`, `S1` + 0.9797959 * RANDN() AS `V119`, `S1` + 0.9797959 * RANDN() AS `V120`, `S1` + 0.9797959 * RANDN() AS `V121`, `S1` + 0.9797959 * RANDN() AS `V122`, `S1` + 0.9797959 * RANDN() AS `V123`, `S1` + 0.9797959 * RANDN() AS `V124`, `S1` + 0.9797959 * RANDN() AS `V125`, `S1` + 0.9797959 * RANDN() AS `V126`, `S1` + 0.9797959 * RANDN() AS `V127`, `S1` + 0.9797959 * RANDN() AS `V128`, `S1` + 0.9797959 * RANDN() AS `V129`, `S1` + 0.9797959 * RANDN() AS `V130`, `S1` + 0.9797959 * RANDN() AS `V131`, `S1` + 0.9797959 * RANDN() AS `V132`, `S1` + 0.9797959 * RANDN() AS `V133`, `S1` + 0.9797959 * RANDN() AS `V134`, `S1` + 0.9797959 * RANDN() AS `V135`, `S1` + 0.9797959 * RANDN() AS `V136`, `S1` + 0.9797959 * RANDN() AS `V137`, `S1` + 0.9797959 * RANDN() AS `V138`, `S1` + 0.9797959 * RANDN() AS `V139`, `S1` + 0.9797959 * RANDN() AS `V140`, `S1` + 0.9797959 * RANDN() AS `V141`, `S1` + 0.9797959 * RANDN() AS `V142`, `S1` + 0.9797959 * RANDN() AS `V143`, `S1` + 0.9797959 * RANDN() AS `V144`, `S1` + 0.9797959 * RANDN() AS `V145`, `S1` + 0.9797959 * RANDN() AS `V146`, `S1` + 0.9797959 * RANDN() AS `V147`, `S1` + 0.9797959 * RANDN() AS `V148`, `S1` + 0.9797959 * RANDN() AS `V149`, `S1` + 0.9797959 * RANDN() AS `V150`, `S1` + 0.9797959 * RANDN() AS `V151`, `S1` + 0.9797959 * RANDN() AS `V152`, `S1` + 0.9797959 * RANDN() AS `V153`, `S1` + 0.9797959 * RANDN() AS `V154`, `S1` + 0.9797959 * RANDN() AS `V155`, `S1` + 0.9797959 * RANDN() AS `V156`, `S1` + 0.9797959 * RANDN() AS `V157`, `S1` + 0.9797959 * RANDN() AS `V158`, `S1` + 0.9797959 * RANDN() AS `V159`, `S1` + 0.9797959 * RANDN() AS `V160`, `S1` + 0.9797959 * RANDN() AS `V161`, `S1` + 0.9797959 * RANDN() AS `V162`, `S1` + 0.9797959 * RANDN() AS `V163`, `S1` + 0.9797959 * RANDN() AS `V164`, `S1` + 0.9797959 * RANDN() AS `V165`, `S1` + 0.9797959 * RANDN() AS `V166`, `S1` + 0.9797959 * RANDN() AS `V167`, `S1` + 0.9797959 * RANDN() AS `V168`, `S1` + 0.9797959 * RANDN() AS `V169`, `S1` + 0.9797959 * RANDN() AS `V170`, `S1` + 0.9797959 * RANDN() AS `V171`, `S1` + 0.9797959 * RANDN() AS `V172`, `S1` + 0.9797959 * RANDN() AS `V173`, `S1` + 0.9797959 * RANDN() AS `V174`, `S1` + 0.9797959 * RANDN() AS `V175`, `S1` + 0.9797959 * RANDN() AS `V176`, `S1` + 0.9797959 * RANDN() AS `V177`, `S1` + 0.9797959 * RANDN() AS `V178`, `S1` + 0.9797959 * RANDN() AS `V179`, `S1` + 0.9797959 * RANDN() AS `V180`, `S1` + 0.9797959 * RANDN() AS `V181`, `S1` + 0.9797959 * RANDN() AS `V182`, `S1` + 0.9797959 * RANDN() AS `V183`, `S1` + 0.9797959 * RANDN() AS `V184`, `S1` + 0.9797959 * RANDN() AS `V185`, `S1` + 0.9797959 * RANDN() AS `V186`, `S1` + 0.9797959 * RANDN() AS `V187`, `S1` + 0.9797959 * RANDN() AS `V188`, `S1` + 0.9797959 * RANDN() AS `V189`, `S1` + 0.9797959 * RANDN() AS `V190`, `S1` + 0.9797959 * RANDN() AS `V191`, `S1` + 0.9797959 * RANDN() AS `V192`, `S1` + 0.9797959 * RANDN() AS `V193`, `S1` + 0.9797959 * RANDN() AS `V194`, `S1` + 0.9797959 * RANDN() AS `V195`, `S1` + 0.9797959 * RANDN() AS `V196`, `S1` + 0.9797959 * RANDN() AS `V197`, `S1` + 0.9797959 * RANDN() AS `V198`, `S1` + 0.9797959 * RANDN() AS `V199`, `S1` + 0.9797959 * RANDN() AS `V200`, `S1` + 0.9797959 * RANDN() AS `V201`, `S1` + 0.9797959 * RANDN() AS `V202`, `S1` + 0.9797959 * RANDN() AS `V203`, `S1` + 0.9797959 * RANDN() AS `V204`, `S1` + 0.9797959 * RANDN() AS `V205`, `S1` + 0.9797959 * RANDN() AS `V206`, `S1` + 0.9797959 * RANDN() AS `V207`, `S1` + 0.9797959 * RANDN() AS `V208`, `S1` + 0.9797959 * RANDN() AS `V209`, `S1` + 0.9797959 * RANDN() AS `V210`, `S1` + 0.9797959 * RANDN() AS `V211`, `S1` + 0.9797959 * RANDN() AS `V212`, `S1` + 0.9797959 * RANDN() AS `V213`, `S1` + 0.9797959 * RANDN() AS `V214`, `S1` + 0.9797959 * RANDN() AS `V215`, `S1` + 0.9797959 * RANDN() AS `V216`, `S1` + 0.9797959 * RANDN() AS `V217`, `S1` + 0.9797959 * RANDN() AS `V218`, `S1` + 0.9797959 * RANDN() AS `V219`, `S1` + 0.9797959 * RANDN() AS `V220`, `S1` + 0.9797959 * RANDN() AS `V221`, `S1` + 0.9797959 * RANDN() AS `V222`, `S1` + 0.9797959 * RANDN() AS `V223`, `S1` + 0.9797959 * RANDN() AS `V224`, `S1` + 0.9797959 * RANDN() AS `V225`, `S1` + 0.9797959 * RANDN() AS `V226`, `S1` + 0.9797959 * RANDN() AS `V227`, `S1` + 0.9797959 * RANDN() AS `V228`, `S1` + 0.9797959 * RANDN() AS `V229`, `S1` + 0.9797959 * RANDN() AS `V230`, `S1` + 0.9797959 * RANDN() AS `V231`, `S1` + 0.9797959 * RANDN() AS `V232`, `S1` + 0.9797959 * RANDN() AS `V233`, `S1` + 0.9797959 * RANDN() AS `V234`, `S1` + 0.9797959 * RANDN() AS `V235`, `S1` + 0.9797959 * RANDN() AS `V236`, `S1` + 0.9797959 * RANDN() AS `V237`, `S1` + 0.9797959 * RANDN() AS `V238`, `S1` + 0.9797959 * RANDN() AS `V239`, `S1` + 0.9797959 * RANDN() AS `V240`, `S1` + 0.9797959 * RANDN() AS `V241`, `S1` + 0.9797959 * RANDN() AS `V242`, `S1` + 0.9797959 * RANDN() AS `V243`, `S1` + 0.9797959 * RANDN() AS `V244`, `S1` + 0.9797959 * RANDN() AS `V245`, `S1` + 0.9797959 * RANDN() AS `V246`, `S1` + 0.9797959 * RANDN() AS `V247`, `S1` + 0.9797959 * RANDN() AS `V248`, `S1` + 0.9797959 * RANDN() AS `V249`, `S1` + 0.9797959 * RANDN() AS `V250`, `S1` + 0.9797959 * RANDN() AS `V251`, `S1` + 0.9797959 * RANDN() AS `V252`, `S1` + 0.9797959 * RANDN() AS `V253`, `S1` + 0.9797959 * RANDN() AS `V254`, `S1` + 0.9797959 * RANDN() AS `V255`, `S1` + 0.9797959 * RANDN() AS `V256`, `S1` + 0.9797959 * RANDN() AS `V257`, `S1` + 0.9797959 * RANDN() AS `V258`, `S1` + 0.9797959 * RANDN() AS `V259`, `S1` + 0.9797959 * RANDN() AS `V260`, `S1` + 0.9797959 * RANDN() AS `V261`, `S1` + 0.9797959 * RANDN() AS `V262`, `S1` + 0.9797959 * RANDN() AS `V263`, `S1` + 0.9797959 * RANDN() AS `V264`, `S1` + 0.9797959 * RANDN() AS `V265`, `S1` + 0.9797959 * RANDN() AS `V266`, `S1` + 0.9797959 * RANDN() AS `V267`, `S1` + 0.9797959 * RANDN() AS `V268`, `S1` + 0.9797959 * RANDN() AS `V269`, `S1` + 0.9797959 * RANDN() AS `V270`, `S1` + 0.9797959 * RANDN() AS `V271`, `S1` + 0.9797959 * RANDN() AS `V272`, `S1` + 0.9797959 * RANDN() AS `V273`, `S1` + 0.9797959 * RANDN() AS `V274`, `S1` + 0.9797959 * RANDN() AS `V275`, `S1` + 0.9797959 * RANDN() AS `V276`, `S1` + 0.9797959 * RANDN() AS `V277`, `S1` + 0.9797959 * RANDN() AS `V278`, `S1` + 0.9797959 * RANDN() AS `V279`, `S1` + 0.9797959 * RANDN() AS `V280`, `S1` + 0.9797959 * RANDN() AS `V281`, `S1` + 0.9797959 * RANDN() AS `V282`, `S1` + 0.9797959 * RANDN() AS `V283`, `S1` + 0.9797959 * RANDN() AS `V284`, `S1` + 0.9797959 * RANDN() AS `V285`, `S1` + 0.9797959 * RANDN() AS `V286`, `S1` + 0.9797959 * RANDN() AS `V287`, `S1` + 0.9797959 * RANDN() AS `V288`, `S1` + 0.9797959 * RANDN() AS `V289`, `S1` + 0.9797959 * RANDN() AS `V290`, `S1` + 0.9797959 * RANDN() AS `V291`, `S1` + 0.9797959 * RANDN() AS `V292`, `S1` + 0.9797959 * RANDN() AS `V293`, `S1` + 0.9797959 * RANDN() AS `V294`, `S1` + 0.9797959 * RANDN() AS `V295`, `S1` + 0.9797959 * RANDN() AS `V296`, `S1` + 0.9797959 * RANDN() AS `V297`, `S1` + 0.9797959 * RANDN() AS `V298`, `S1` + 0.9797959 * RANDN() AS `V299`, `S1` + 0.9797959 * RANDN() AS `V300`, `S1` + 0.9797959 * RANDN() AS `V301`, `S1` + 0.9797959 * RANDN() AS `V302`, `S1` + 0.9797959 * RANDN() AS `V303`, `S1` + 0.9797959 * RANDN() AS `V304`, `S1` + 0.9797959 * RANDN() AS `V305`, `S1` + 0.9797959 * RANDN() AS `V306`, `S1` + 0.9797959 * RANDN() AS `V307`, `S1` + 0.9797959 * RANDN() AS `V308`, `S1` + 0.9797959 * RANDN() AS `V309`, `S1` + 0.9797959 * RANDN() AS `V310`, `S1` + 0.9797959 * RANDN() AS `V311`, `S1` + 0.9797959 * RANDN() AS `V312`, `S1` + 0.9797959 * RANDN() AS `V313`, `S1` + 0.9797959 * RANDN() AS `V314`, `S1` + 0.9797959 * RANDN() AS `V315`, `S1` + 0.9797959 * RANDN() AS `V316`, `S1` + 0.9797959 * RANDN() AS `V317`, `S1` + 0.9797959 * RANDN() AS `V318`, `S1` + 0.9797959 * RANDN() AS `V319`, `S1` + 0.9797959 * RANDN() AS `V320`, `S1` + 0.9797959 * RANDN() AS `V321`, `S1` + 0.9797959 * RANDN() AS `V322`, `S1` + 0.9797959 * RANDN() AS `V323`, `S1` + 0.9797959 * RANDN() AS `V324`, `S1` + 0.9797959 * RANDN() AS `V325`, `S1` + 0.9797959 * RANDN() AS `V326`, `S1` + 0.9797959 * RANDN() AS `V327`, `S1` + 0.9797959 * RANDN() AS `V328`, `S1` + 0.9797959 * RANDN() AS `V329`, `S1` + 0.9797959 * RANDN() AS `V330`, `S1` + 0.9797959 * RANDN() AS `V331`, `S1` + 0.9797959 * RANDN() AS `V332`, `S1` + 0.9797959 * RANDN() AS `V333`, `S1` + 0.9797959 * RANDN() AS `V334`, `S1` + 0.9797959 * RANDN() AS `V335`, `S1` + 0.9797959 * RANDN() AS `V336`, `S1` + 0.9797959 * RANDN() AS `V337`, `S1` + 0.9797959 * RANDN() AS `V338`, `S1` + 0.9797959 * RANDN() AS `V339`, `S1` + 0.9797959 * RANDN() AS `V340`, `S1` + 0.9797959 * RANDN() AS `V341`, `S1` + 0.9797959 * RANDN() AS `V342`, `S1` + 0.9797959 * RANDN() AS `V343`, `S1` + 0.9797959 * RANDN() AS `V344`, `S1` + 0.9797959 * RANDN() AS `V345`, `S1` + 0.9797959 * RANDN() AS `V346`, `S1` + 0.9797959 * RANDN() AS `V347`, `S1` + 0.9797959 * RANDN() AS `V348`, `S1` + 0.9797959 * RANDN() AS `V349`, `S1` + 0.9797959 * RANDN() AS `V350`, `S1` + 0.9797959 * RANDN() AS `V351`, `S1` + 0.9797959 * RANDN() AS `V352`, `S1` + 0.9797959 * RANDN() AS `V353`, `S1` + 0.9797959 * RANDN() AS `V354`, `S1` + 0.9797959 * RANDN() AS `V355`, `S1` + 0.9797959 * RANDN() AS `V356`, `S1` + 0.9797959 * RANDN() AS `V357`, `S1` + 0.9797959 * RANDN() AS `V358`, `S1` + 0.9797959 * RANDN() AS `V359`, `S1` + 0.9797959 * RANDN() AS `V360`, `S1` + 0.9797959 * RANDN() AS `V361`, `S1` + 0.9797959 * RANDN() AS `V362`, `S1` + 0.9797959 * RANDN() AS `V363`, `S1` + 0.9797959 * RANDN() AS `V364`, `S1` + 0.9797959 * RANDN() AS `V365`, `S1` + 0.9797959 * RANDN() AS `V366`, `S1` + 0.9797959 * RANDN() AS `V367`, `S1` + 0.9797959 * RANDN() AS `V368`, `S1` + 0.9797959 * RANDN() AS `V369`, `S1` + 0.9797959 * RANDN() AS `V370`, `S1` + 0.9797959 * RANDN() AS `V371`, `S1` + 0.9797959 * RANDN() AS `V372`, `S1` + 0.9797959 * RANDN() AS `V373`, `S1` + 0.9797959 * RANDN() AS `V374`, `S1` + 0.9797959 * RANDN() AS `V375`, `S1` + 0.9797959 * RANDN() AS `V376`, `S1` + 0.9797959 * RANDN() AS `V377`, `S1` + 0.9797959 * RANDN() AS `V378`, `S1` + 0.9797959 * RANDN() AS `V379`, `S1` + 0.9797959 * RANDN() AS `V380`, `S1` + 0.9797959 * RANDN() AS `V381`, `S1` + 0.9797959 * RANDN() AS `V382`, `S1` + 0.9797959 * RANDN() AS `V383`, `S1` + 0.9797959 * RANDN() AS `V384`, `S1` + 0.9797959 * RANDN() AS `V385`, `S1` + 0.9797959 * RANDN() AS `V386`, `S1` + 0.9797959 * RANDN() AS `V387`, `S1` + 0.9797959 * RANDN() AS `V388`, `S1` + 0.9797959 * RANDN() AS `V389`, `S1` + 0.9797959 * RANDN() AS `V390`, `S1` + 0.9797959 * RANDN() AS `V391`, `S1` + 0.9797959 * RANDN() AS `V392`, `S1` + 0.9797959 * RANDN() AS `V393`, `S1` + 0.9797959 * RANDN() AS `V394`, `S1` + 0.9797959 * RANDN() AS `V395`, `S1` + 0.9797959 * RANDN() AS `V396`, `S1` + 0.9797959 * RANDN() AS `V397`, `S1` + 0.9797959 * RANDN() AS `V398`, `S1` + 0.9797959 * RANDN() AS `V399`, `S1` + 0.9797959 * RANDN() AS `V400`, `S1` + 0.9797959 * RANDN() AS `V401`, `S1` + 0.9797959 * RANDN() AS `V402`, `S1` + 0.9797959 * RANDN() AS `V403`, `S1` + 0.9797959 * RANDN() AS `V404`, `S1` + 0.9797959 * RANDN() AS `V405`, `S1` + 0.9797959 * RANDN() AS `V406`, `S1` + 0.9797959 * RANDN() AS `V407`, `S1` + 0.9797959 * RANDN() AS `V408`, `S1` + 0.9797959 * RANDN() AS `V409`, `S1` + 0.9797959 * RANDN() AS `V410`, `S1` + 0.9797959 * RANDN() AS `V411`, `S1` + 0.9797959 * RANDN() AS `V412`, `S1` + 0.9797959 * RANDN() AS `V413`, `S1` + 0.9797959 * RANDN() AS `V414`, `S1` + 0.9797959 * RANDN() AS `V415`, `S1` + 0.9797959 * RANDN() AS `V416`, `S1` + 0.9797959 * RANDN() AS `V417`, `S1` + 0.9797959 * RANDN() AS `V418`, `S1` + 0.9797959 * RANDN() AS `V419`, `S1` + 0.9797959 * RANDN() AS `V420`, `S1` + 0.9797959 * RANDN() AS `V421`, `S1` + 0.9797959 * RANDN() AS `V422`, `S1` + 0.9797959 * RANDN() AS `V423`, `S1` + 0.9797959 * RANDN() AS `V424`, `S1` + 0.9797959 * RANDN() AS `V425`, `S1` + 0.9797959 * RANDN() AS `V426`, `S1` + 0.9797959 * RANDN() AS `V427`, `S1` + 0.9797959 * RANDN() AS `V428`, `S1` + 0.9797959 * RANDN() AS `V429`, `S1` + 0.9797959 * RANDN() AS `V430`, `S1` + 0.9797959 * RANDN() AS `V431`, `S1` + 0.9797959 * RANDN() AS `V432`, `S1` + 0.9797959 * RANDN() AS `V433`, `S1` + 0.9797959 * RANDN() AS `V434`, `S1` + 0.9797959 * RANDN() AS `V435`, `S1` + 0.9797959 * RANDN() AS `V436`, `S1` + 0.9797959 * RANDN() AS `V437`, `S1` + 0.9797959 * RANDN() AS `V438`, `S1` + 0.9797959 * RANDN() AS `V439`, `S1` + 0.9797959 * RANDN() AS `V440`, `S1` + 0.9797959 * RANDN() AS `V441`, `S1` + 0.9797959 * RANDN() AS `V442`, `S1` + 0.9797959 * RANDN() AS `V443`, `S1` + 0.9797959 * RANDN() AS `V444`, `S1` + 0.9797959 * RANDN() AS `V445`, `S1` + 0.9797959 * RANDN() AS `V446`, `S1` + 0.9797959 * RANDN() AS `V447`, `S1` + 0.9797959 * RANDN() AS `V448`, `S1` + 0.9797959 * RANDN() AS `V449`, `S1` + 0.9797959 * RANDN() AS `V450`, `S1` + 0.9797959 * RANDN() AS `V451`, `S1` + 0.9797959 * RANDN() AS `V452`, `S1` + 0.9797959 * RANDN() AS `V453`, `S1` + 0.9797959 * RANDN() AS `V454`, `S1` + 0.9797959 * RANDN() AS `V455`, `S1` + 0.9797959 * RANDN() AS `V456`, `S1` + 0.9797959 * RANDN() AS `V457`, `S1` + 0.9797959 * RANDN() AS `V458`, `S1` + 0.9797959 * RANDN() AS `V459`, `S1` + 0.9797959 * RANDN() AS `V460`, `S1` + 0.9797959 * RANDN() AS `V461`, `S1` + 0.9797959 * RANDN() AS `V462`, `S1` + 0.9797959 * RANDN() AS `V463`, `S1` + 0.9797959 * RANDN() AS `V464`, `S1` + 0.9797959 * RANDN() AS `V465`, `S1` + 0.9797959 * RANDN() AS `V466`, `S1` + 0.9797959 * RANDN() AS `V467`, `S1` + 0.9797959 * RANDN() AS `V468`, `S1` + 0.9797959 * RANDN() AS `V469`, `S1` + 0.9797959 * RANDN() AS `V470`, `S1` + 0.9797959 * RANDN() AS `V471`, `S1` + 0.9797959 * RANDN() AS `V472`, `S1` + 0.9797959 * RANDN() AS `V473`, `S1` + 0.9797959 * RANDN() AS `V474`, `S1` + 0.9797959 * RANDN() AS `V475`, `S1` + 0.9797959 * RANDN() AS `V476`, `S1` + 0.9797959 * RANDN() AS `V477`, `S1` + 0.9797959 * RANDN() AS `V478`, `S1` + 0.9797959 * RANDN() AS `V479`, `S1` + 0.9797959 * RANDN() AS `V480`, `S1` + 0.9797959 * RANDN() AS `V481`, `S1` + 0.9797959 * RANDN() AS `V482`, `S1` + 0.9797959 * RANDN() AS `V483`, `S1` + 0.9797959 * RANDN() AS `V484`, `S1` + 0.9797959 * RANDN() AS `V485`, `S1` + 0.9797959 * RANDN() AS `V486`, `S1` + 0.9797959 * RANDN() AS `V487`, `S1` + 0.9797959 * RANDN() AS `V488`, `S1` + 0.9797959 * RANDN() AS `V489`, `S1` + 0.9797959 * RANDN() AS `V490`, `S1` + 0.9797959 * RANDN() AS `V491`, `S1` + 0.9797959 * RANDN() AS `V492`, `S1` + 0.9797959 * RANDN() AS `V493`, `S1` + 0.9797959 * RANDN() AS `V494`, `S1` + 0.9797959 * RANDN() AS `V495`, `S1` + 0.9797959 * RANDN() AS `V496`, `S1` + 0.9797959 * RANDN() AS `V497`, `S1` + 0.9797959 * RANDN() AS `V498`, `S1` + 0.9797959 * RANDN() AS `V499`, `S1` + 0.9797959 * RANDN() AS `V500`, `S1` + 0.9797959 * RANDN() AS `V501`, `S1` + 0.9797959 * RANDN() AS `V502`, `S1` + 0.9797959 * RANDN() AS `V503`, `S1` + 0.9797959 * RANDN() AS `V504`, `S1` + 0.9797959 * RANDN() AS `V505`, `S1` + 0.9797959 * RANDN() AS `V506`, `S1` + 0.9797959 * RANDN() AS `V507`, `S1` + 0.9797959 * RANDN() AS `V508`, `S1` + 0.9797959 * RANDN() AS `V509`, `S1` + 0.9797959 * RANDN() AS `V510`, `S1` + 0.9797959 * RANDN() AS `V511`, `S1` + 0.9797959 * RANDN() AS `V512`, `S1` + 0.9797959 * RANDN() AS `V513`, `S1` + 0.9797959 * RANDN() AS `V514`, `S1` + 0.9797959 * RANDN() AS `V515`, `S1` + 0.9797959 * RANDN() AS `V516`, `S1` + 0.9797959 * RANDN() AS `V517`, `S1` + 0.9797959 * RANDN() AS `V518`, `S1` + 0.9797959 * RANDN() AS `V519`, `S1` + 0.9797959 * RANDN() AS `V520`, `S1` + 0.9797959 * RANDN() AS `V521`, `S1` + 0.9797959 * RANDN() AS `V522`, `S1` + 0.9797959 * RANDN() AS `V523`, `S1` + 0.9797959 * RANDN() AS `V524`, `S1` + 0.9797959 * RANDN() AS `V525`, `S1` + 0.9797959 * RANDN() AS `V526`, `S1` + 0.9797959 * RANDN() AS `V527`, `S1` + 0.9797959 * RANDN() AS `V528`, `S1` + 0.9797959 * RANDN() AS `V529`, `S1` + 0.9797959 * RANDN() AS `V530`, `S1` + 0.9797959 * RANDN() AS `V531`, `S1` + 0.9797959 * RANDN() AS `V532`, `S1` + 0.9797959 * RANDN() AS `V533`, `S1` + 0.9797959 * RANDN() AS `V534`, `S1` + 0.9797959 * RANDN() AS `V535`, `S1` + 0.9797959 * RANDN() AS `V536`, `S1` + 0.9797959 * RANDN() AS `V537`, `S1` + 0.9797959 * RANDN() AS `V538`, `S1` + 0.9797959 * RANDN() AS `V539`, `S1` + 0.9797959 * RANDN() AS `V540`, `S1` + 0.9797959 * RANDN() AS `V541`, `S1` + 0.9797959 * RANDN() AS `V542`, `S1` + 0.9797959 * RANDN() AS `V543`, `S1` + 0.9797959 * RANDN() AS `V544`, `S1` + 0.9797959 * RANDN() AS `V545`, `S1` + 0.9797959 * RANDN() AS `V546`, `S1` + 0.9797959 * RANDN() AS `V547`, `S1` + 0.9797959 * RANDN() AS `V548`, `S1` + 0.9797959 * RANDN() AS `V549`, `S1` + 0.9797959 * RANDN() AS `V550`, `S1` + 0.9797959 * RANDN() AS `V551`, `S1` + 0.9797959 * RANDN() AS `V552`, `S1` + 0.9797959 * RANDN() AS `V553`, `S1` + 0.9797959 * RANDN() AS `V554`, `S1` + 0.9797959 * RANDN() AS `V555`, `S1` + 0.9797959 * RANDN() AS `V556`, `S1` + 0.9797959 * RANDN() AS `V557`, `S1` + 0.9797959 * RANDN() AS `V558`, `S1` + 0.9797959 * RANDN() AS `V559`, `S1` + 0.9797959 * RANDN() AS `V560`, `S1` + 0.9797959 * RANDN() AS `V561`, `S1` + 0.9797959 * RANDN() AS `V562`, `S1` + 0.9797959 * RANDN() AS `V563`, `S1` + 0.9797959 * RANDN() AS `V564`, `S1` + 0.9797959 * RANDN() AS `V565`, `S1` + 0.9797959 * RANDN() AS `V566`, `S1` + 0.9797959 * RANDN() AS `V567`, `S1` + 0.9797959 * RANDN() AS `V568`, `S1` + 0.9797959 * RANDN() AS `V569`, `S1` + 0.9797959 * RANDN() AS `V570`, `S1` + 0.9797959 * RANDN() AS `V571`, `S1` + 0.9797959 * RANDN() AS `V572`, `S1` + 0.9797959 * RANDN() AS `V573`, `S1` + 0.9797959 * RANDN() AS `V574`, `S1` + 0.9797959 * RANDN() AS `V575`, `S1` + 0.9797959 * RANDN() AS `V576`, `S1` + 0.9797959 * RANDN() AS `V577`, `S1` + 0.9797959 * RANDN() AS `V578`, `S1` + 0.9797959 * RANDN() AS `V579`, `S1` + 0.9797959 * RANDN() AS `V580`, `S1` + 0.9797959 * RANDN() AS `V581`, `S1` + 0.9797959 * RANDN() AS `V582`, `S1` + 0.9797959 * RANDN() AS `V583`, `S1` + 0.9797959 * RANDN() AS `V584`, `S1` + 0.9797959 * RANDN() AS `V585`, `S1` + 0.9797959 * RANDN() AS `V586`, `S1` + 0.9797959 * RANDN() AS `V587`, `S1` + 0.9797959 * RANDN() AS `V588`, `S1` + 0.9797959 * RANDN() AS `V589`, `S1` + 0.9797959 * RANDN() AS `V590`, `S1` + 0.9797959 * RANDN() AS `V591`, `S1` + 0.9797959 * RANDN() AS `V592`, `S1` + 0.9797959 * RANDN() AS `V593`, `S1` + 0.9797959 * RANDN() AS `V594`, `S1` + 0.9797959 * RANDN() AS `V595`, `S1` + 0.9797959 * RANDN() AS `V596`, `S1` + 0.9797959 * RANDN() AS `V597`, `S1` + 0.9797959 * RANDN() AS `V598`, `S1` + 0.9797959 * RANDN() AS `V599`, `S1` + 0.9797959 * RANDN() AS `V600`, `S1` + 0.9797959 * RANDN() AS `V601`, `S1` + 0.9797959 * RANDN() AS `V602`, `S1` + 0.9797959 * RANDN() AS `V603`, `S1` + 0.9797959 * RANDN() AS `V604`, `S1` + 0.9797959 * RANDN() AS `V605`, `S1` + 0.9797959 * RANDN() AS `V606`, `S1` + 0.9797959 * RANDN() AS `V607`, `S1` + 0.9797959 * RANDN() AS `V608`, `S1` + 0.9797959 * RANDN() AS `V609`, `S1` + 0.9797959 * RANDN() AS `V610`, `S1` + 0.9797959 * RANDN() AS `V611`, `S1` + 0.9797959 * RANDN() AS `V612`, `S1` + 0.9797959 * RANDN() AS `V613`, `S1` + 0.9797959 * RANDN() AS `V614`, `S1` + 0.9797959 * RANDN() AS `V615`, `S1` + 0.9797959 * RANDN() AS `V616`, `S1` + 0.9797959 * RANDN() AS `V617`, `S1` + 0.9797959 * RANDN() AS `V618`, `S1` + 0.9797959 * RANDN() AS `V619`, `S1` + 0.9797959 * RANDN() AS `V620`, `S1` + 0.9797959 * RANDN() AS `V621`, `S1` + 0.9797959 * RANDN() AS `V622`, `S1` + 0.9797959 * RANDN() AS `V623`, `S1` + 0.9797959 * RANDN() AS `V624`, `S1` + 0.9797959 * RANDN() AS `V625`, `S1` + 0.9797959 * RANDN() AS `V626`, `S1` + 0.9797959 * RANDN() AS `V627`, `S1` + 0.9797959 * RANDN() AS `V628`, `S1` + 0.9797959 * RANDN() AS `V629`, `S1` + 0.9797959 * RANDN() AS `V630`, `S1` + 0.9797959 * RANDN() AS `V631`, `S1` + 0.9797959 * RANDN() AS `V632`, `S1` + 0.9797959 * RANDN() AS `V633`, `S1` + 0.9797959 * RANDN() AS `V634`, `S1` + 0.9797959 * RANDN() AS `V635`, `S1` + 0.9797959 * RANDN() AS `V636`, `S1` + 0.9797959 * RANDN() AS `V637`, `S1` + 0.9797959 * RANDN() AS `V638`, `S1` + 0.9797959 * RANDN() AS `V639`, `S1` + 0.9797959 * RANDN() AS `V640`, `S1` + 0.9797959 * RANDN() AS `V641`, `S1` + 0.9797959 * RANDN() AS `V642`, `S1` + 0.9797959 * RANDN() AS `V643`, `S1` + 0.9797959 * RANDN() AS `V644`, `S1` + 0.9797959 * RANDN() AS `V645`, `S1` + 0.9797959 * RANDN() AS `V646`, `S1` + 0.9797959 * RANDN() AS `V647`, `S1` + 0.9797959 * RANDN() AS `V648`, `S1` + 0.9797959 * RANDN() AS `V649`, `S1` + 0.9797959 * RANDN() AS `V650`, `S1` + 0.9797959 * RANDN() AS `V651`, `S1` + 0.9797959 * RANDN() AS `V652`, `S1` + 0.9797959 * RANDN() AS `V653`, `S1` + 0.9797959 * RANDN() AS `V654`, `S1` + 0.9797959 * RANDN() AS `V655`, `S1` + 0.9797959 * RANDN() AS `V656`, `S1` + 0.9797959 * RANDN() AS `V657`, `S1` + 0.9797959 * RANDN() AS `V658`, `S1` + 0.9797959 * RANDN() AS `V659`, `S1` + 0.9797959 * RANDN() AS `V660`, `S1` + 0.9797959 * RANDN() AS `V661`, `S1` + 0.9797959 * RANDN() AS `V662`, `S1` + 0.9797959 * RANDN() AS `V663`, `S1` + 0.9797959 * RANDN() AS `V664`, `S1` + 0.9797959 * RANDN() AS `V665`, `S1` + 0.9797959 * RANDN() AS `V666`, `S1` + 0.9797959 * RANDN() AS `V667`, `S1` + 0.9797959 * RANDN() AS `V668`, `S1` + 0.9797959 * RANDN() AS `V669`, `S1` + 0.9797959 * RANDN() AS `V670`, `S1` + 0.9797959 * RANDN() AS `V671`, `S1` + 0.9797959 * RANDN() AS `V672`, `S1` + 0.9797959 * RANDN() AS `V673`, `S1` + 0.9797959 * RANDN() AS `V674`, `S1` + 0.9797959 * RANDN() AS `V675`, `S1` + 0.9797959 * RANDN() AS `V676`, `S1` + 0.9797959 * RANDN() AS `V677`, `S1` + 0.9797959 * RANDN() AS `V678`, `S1` + 0.9797959 * RANDN() AS `V679`, `S1` + 0.9797959 * RANDN() AS `V680`, `S1` + 0.9797959 * RANDN() AS `V681`, `S1` + 0.9797959 * RANDN() AS `V682`, `S1` + 0.9797959 * RANDN() AS `V683`, `S1` + 0.9797959 * RANDN() AS `V684`, `S1` + 0.9797959 * RANDN() AS `V685`, `S1` + 0.9797959 * RANDN() AS `V686`, `S1` + 0.9797959 * RANDN() AS `V687`, `S1` + 0.9797959 * RANDN() AS `V688`, `S1` + 0.9797959 * RANDN() AS `V689`, `S1` + 0.9797959 * RANDN() AS `V690`, `S1` + 0.9797959 * RANDN() AS `V691`, `S1` + 0.9797959 * RANDN() AS `V692`, `S1` + 0.9797959 * RANDN() AS `V693`, `S1` + 0.9797959 * RANDN() AS `V694`, `S1` + 0.9797959 * RANDN() AS `V695`, `S1` + 0.9797959 * RANDN() AS `V696`, `S1` + 0.9797959 * RANDN() AS `V697`, `S1` + 0.9797959 * RANDN() AS `V698`, `S1` + 0.9797959 * RANDN() AS `V699`, `S1` + 0.9797959 * RANDN() AS `V700`, `S1` + 0.9797959 * RANDN() AS `V701`, `S1` + 0.9797959 * RANDN() AS `V702`, `S1` + 0.9797959 * RANDN() AS `V703`, `S1` + 0.9797959 * RANDN() AS `V704`, `S1` + 0.9797959 * RANDN() AS `V705`, `S1` + 0.9797959 * RANDN() AS `V706`, `S1` + 0.9797959 * RANDN() AS `V707`, `S1` + 0.9797959 * RANDN() AS `V708`, `S1` + 0.9797959 * RANDN() AS `V709`, `S1` + 0.9797959 * RANDN() AS `V710`, `S1` + 0.9797959 * RANDN() AS `V711`, `S1` + 0.9797959 * RANDN() AS `V712`, `S1` + 0.9797959 * RANDN() AS `V713`, `S1` + 0.9797959 * RANDN() AS `V714`, `S1` + 0.9797959 * RANDN() AS `V715`, `S1` + 0.9797959 * RANDN() AS `V716`, `S1` + 0.9797959 * RANDN() AS `V717`, `S1` + 0.9797959 * RANDN() AS `V718`, `S1` + 0.9797959 * RANDN() AS `V719`, `S1` + 0.9797959 * RANDN() AS `V720`, `S1` + 0.9797959 * RANDN() AS `V721`, `S1` + 0.9797959 * RANDN() AS `V722`, `S1` + 0.9797959 * RANDN() AS `V723`, `S1` + 0.9797959 * RANDN() AS `V724`, `S1` + 0.9797959 * RANDN() AS `V725`, `S1` + 0.9797959 * RANDN() AS `V726`, `S1` + 0.9797959 * RANDN() AS `V727`, `S1` + 0.9797959 * RANDN() AS `V728`, `S1` + 0.9797959 * RANDN() AS `V729`, `S1` + 0.9797959 * RANDN() AS `V730`, `S1` + 0.9797959 * RANDN() AS `V731`, `S1` + 0.9797959 * RANDN() AS `V732`, `S1` + 0.9797959 * RANDN() AS `V733`, `S1` + 0.9797959 * RANDN() AS `V734`, `S1` + 0.9797959 * RANDN() AS `V735`, `S1` + 0.9797959 * RANDN() AS `V736`, `S1` + 0.9797959 * RANDN() AS `V737`, `S1` + 0.9797959 * RANDN() AS `V738`, `S1` + 0.9797959 * RANDN() AS `V739`, `S1` + 0.9797959 * RANDN() AS `V740`, `S1` + 0.9797959 * RANDN() AS `V741`, `S1` + 0.9797959 * RANDN() AS `V742`, `S1` + 0.9797959 * RANDN() AS `V743`, `S1` + 0.9797959 * RANDN() AS `V744`, `S1` + 0.9797959 * RANDN() AS `V745`, `S1` + 0.9797959 * RANDN() AS `V746`, `S1` + 0.9797959 * RANDN() AS `V747`, `S1` + 0.9797959 * RANDN() AS `V748`, `S1` + 0.9797959 * RANDN() AS `V749`, `S1` + 0.9797959 * RANDN() AS `V750`, `S1` + 0.9797959 * RANDN() AS `V751`, `S1` + 0.9797959 * RANDN() AS `V752`, `S1` + 0.9797959 * RANDN() AS `V753`, `S1` + 0.9797959 * RANDN() AS `V754`, `S1` + 0.9797959 * RANDN() AS `V755`, `S1` + 0.9797959 * RANDN() AS `V756`, `S1` + 0.9797959 * RANDN() AS `V757`, `S1` + 0.9797959 * RANDN() AS `V758`, `S1` + 0.9797959 * RANDN() AS `V759`, `S1` + 0.9797959 * RANDN() AS `V760`, `S1` + 0.9797959 * RANDN() AS `V761`, `S1` + 0.9797959 * RANDN() AS `V762`, `S1` + 0.9797959 * RANDN() AS `V763`, `S1` + 0.9797959 * RANDN() AS `V764`, `S1` + 0.9797959 * RANDN() AS `V765`, `S1` + 0.9797959 * RANDN() AS `V766`, `S1` + 0.9797959 * RANDN() AS `V767`, `S1` + 0.9797959 * RANDN() AS `V768`, `S1` + 0.9797959 * RANDN() AS `V769`, `S1` + 0.9797959 * RANDN() AS `V770`, `S1` + 0.9797959 * RANDN() AS `V771`, `S1` + 0.9797959 * RANDN() AS `V772`, `S1` + 0.9797959 * RANDN() AS `V773`, `S1` + 0.9797959 * RANDN() AS `V774`, `S1` + 0.9797959 * RANDN() AS `V775`, `S1` + 0.9797959 * RANDN() AS `V776`, `S1` + 0.9797959 * RANDN() AS `V777`, `S1` + 0.9797959 * RANDN() AS `V778`, `S1` + 0.9797959 * RANDN() AS `V779`, `S1` + 0.9797959 * RANDN() AS `V780`, `S1` + 0.9797959 * RANDN() AS `V781`, `S1` + 0.9797959 * RANDN() AS `V782`, `S1` + 0.9797959 * RANDN() AS `V783`, `S1` + 0.9797959 * RANDN() AS `V784`, `S1` + 0.9797959 * RANDN() AS `V785`, `S1` + 0.9797959 * RANDN() AS `V786`, `S1` + 0.9797959 * RANDN() AS `V787`, `S1` + 0.9797959 * RANDN() AS `V788`, `S1` + 0.9797959 * RANDN() AS `V789`, `S1` + 0.9797959 * RANDN() AS `V790`, `S1` + 0.9797959 * RANDN() AS `V791`, `S1` + 0.9797959 * RANDN() AS `V792`, `S1` + 0.9797959 * RANDN() AS `V793`, `S1` + 0.9797959 * RANDN() AS `V794`, `S1` + 0.9797959 * RANDN() AS `V795`, `S1` + 0.9797959 * RANDN() AS `V796`, `S1` + 0.9797959 * RANDN() AS `V797`, `S1` + 0.9797959 * RANDN() AS `V798`, `S1` + 0.9797959 * RANDN() AS `V799`, `S1` + 0.9797959 * RANDN() AS `V800`, `S1` + 0.9797959 * RANDN() AS `V801`, `S1` + 0.9797959 * RANDN() AS `V802`, `S1` + 0.9797959 * RANDN() AS `V803`, `S1` + 0.9797959 * RANDN() AS `V804`, `S1` + 0.9797959 * RANDN() AS `V805`, `S1` + 0.9797959 * RANDN() AS `V806`, `S1` + 0.9797959 * RANDN() AS `V807`, `S1` + 0.9797959 * RANDN() AS `V808`, `S1` + 0.9797959 * RANDN() AS `V809`, `S1` + 0.9797959 * RANDN() AS `V810`, `S1` + 0.9797959 * RANDN() AS `V811`, `S1` + 0.9797959 * RANDN() AS `V812`, `S1` + 0.9797959 * RANDN() AS `V813`, `S1` + 0.9797959 * RANDN() AS `V814`, `S1` + 0.9797959 * RANDN() AS `V815`, `S1` + 0.9797959 * RANDN() AS `V816`, `S1` + 0.9797959 * RANDN() AS `V817`, `S1` + 0.9797959 * RANDN() AS `V818`, `S1` + 0.9797959 * RANDN() AS `V819`, `S1` + 0.9797959 * RANDN() AS `V820`, `S1` + 0.9797959 * RANDN() AS `V821`, `S1` + 0.9797959 * RANDN() AS `V822`, `S1` + 0.9797959 * RANDN() AS `V823`, `S1` + 0.9797959 * RANDN() AS `V824`, `S1` + 0.9797959 * RANDN() AS `V825`, `S1` + 0.9797959 * RANDN() AS `V826`, `S1` + 0.9797959 * RANDN() AS `V827`, `S1` + 0.9797959 * RANDN() AS `V828`, `S1` + 0.9797959 * RANDN() AS `V829`, `S1` + 0.9797959 * RANDN() AS `V830`, `S1` + 0.9797959 * RANDN() AS `V831`, `S1` + 0.9797959 * RANDN() AS `V832`, `S1` + 0.9797959 * RANDN() AS `V833`, `S1` + 0.9797959 * RANDN() AS `V834`, `S1` + 0.9797959 * RANDN() AS `V835`, `S1` + 0.9797959 * RANDN() AS `V836`, `S1` + 0.9797959 * RANDN() AS `V837`, `S1` + 0.9797959 * RANDN() AS `V838`, `S1` + 0.9797959 * RANDN() AS `V839`, `S1` + 0.9797959 * RANDN() AS `V840`, `S1` + 0.9797959 * RANDN() AS `V841`, `S1` + 0.9797959 * RANDN() AS `V842`, `S1` + 0.9797959 * RANDN() AS `V843`, `S1` + 0.9797959 * RANDN() AS `V844`, `S1` + 0.9797959 * RANDN() AS `V845`, `S1` + 0.9797959 * RANDN() AS `V846`, `S1` + 0.9797959 * RANDN() AS `V847`, `S1` + 0.9797959 * RANDN() AS `V848`, `S1` + 0.9797959 * RANDN() AS `V849`, `S1` + 0.9797959 * RANDN() AS `V850`, `S1` + 0.9797959 * RANDN() AS `V851`, `S1` + 0.9797959 * RANDN() AS `V852`, `S1` + 0.9797959 * RANDN() AS `V853`, `S1` + 0.9797959 * RANDN() AS `V854`, `S1` + 0.9797959 * RANDN() AS `V855`, `S1` + 0.9797959 * RANDN() AS `V856`, `S1` + 0.9797959 * RANDN() AS `V857`, `S1` + 0.9797959 * RANDN() AS `V858`, `S1` + 0.9797959 * RANDN() AS `V859`, `S1` + 0.9797959 * RANDN() AS `V860`, `S1` + 0.9797959 * RANDN() AS `V861`, `S1` + 0.9797959 * RANDN() AS `V862`, `S1` + 0.9797959 * RANDN() AS `V863`, `S1` + 0.9797959 * RANDN() AS `V864`, `S1` + 0.9797959 * RANDN() AS `V865`, `S1` + 0.9797959 * RANDN() AS `V866`, `S1` + 0.9797959 * RANDN() AS `V867`, `S1` + 0.9797959 * RANDN() AS `V868`, `S1` + 0.9797959 * RANDN() AS `V869`, `S1` + 0.9797959 * RANDN() AS `V870`, `S1` + 0.9797959 * RANDN() AS `V871`, `S1` + 0.9797959 * RANDN() AS `V872`, `S1` + 0.9797959 * RANDN() AS `V873`, `S1` + 0.9797959 * RANDN() AS `V874`, `S1` + 0.9797959 * RANDN() AS `V875`, `S1` + 0.9797959 * RANDN() AS `V876`, `S1` + 0.9797959 * RANDN() AS `V877`, `S1` + 0.9797959 * RANDN() AS `V878`, `S1` + 0.9797959 * RANDN() AS `V879`, `S1` + 0.9797959 * RANDN() AS `V880`, `S1` + 0.9797959 * RANDN() AS `V881`, `S1` + 0.9797959 * RANDN() AS `V882`, `S1` + 0.9797959 * RANDN() AS `V883`, `S1` + 0.9797959 * RANDN() AS `V884`, `S1` + 0.9797959 * RANDN() AS `V885`, `S1` + 0.9797959 * RANDN() AS `V886`, `S1` + 0.9797959 * RANDN() AS `V887`, `S1` + 0.9797959 * RANDN() AS `V888`, `S1` + 0.9797959 * RANDN() AS `V889`, `S1` + 0.9797959 * RANDN() AS `V890`, `S1` + 0.9797959 * RANDN() AS `V891`, `S1` + 0.9797959 * RANDN() AS `V892`, `S1` + 0.9797959 * RANDN() AS `V893`, `S1` + 0.9797959 * RANDN() AS `V894`, `S1` + 0.9797959 * RANDN() AS `V895`, `S1` + 0.9797959 * RANDN() AS `V896`, `S1` + 0.9797959 * RANDN() AS `V897`, `S1` + 0.9797959 * RANDN() AS `V898`, `S1` + 0.9797959 * RANDN() AS `V899`, `S1` + 0.9797959 * RANDN() AS `V900`, `S1` + 0.9797959 * RANDN() AS `V901`, `S1` + 0.9797959 * RANDN() AS `V902`, `S1` + 0.9797959 * RANDN() AS `V903`, `S1` + 0.9797959 * RANDN() AS `V904`, `S1` + 0.9797959 * RANDN() AS `V905`, `S1` + 0.9797959 * RANDN() AS `V906`, `S1` + 0.9797959 * RANDN() AS `V907`, `S1` + 0.9797959 * RANDN() AS `V908`, `S1` + 0.9797959 * RANDN() AS `V909`, `S1` + 0.9797959 * RANDN() AS `V910`, `S1` + 0.9797959 * RANDN() AS `V911`, `S1` + 0.9797959 * RANDN() AS `V912`, `S1` + 0.9797959 * RANDN() AS `V913`, `S1` + 0.9797959 * RANDN() AS `V914`, `S1` + 0.9797959 * RANDN() AS `V915`, `S1` + 0.9797959 * RANDN() AS `V916`, `S1` + 0.9797959 * RANDN() AS `V917`, `S1` + 0.9797959 * RANDN() AS `V918`, `S1` + 0.9797959 * RANDN() AS `V919`, `S1` + 0.9797959 * RANDN() AS `V920`, `S1` + 0.9797959 * RANDN() AS `V921`, `S1` + 0.9797959 * RANDN() AS `V922`, `S1` + 0.9797959 * RANDN() AS `V923`, `S1` + 0.9797959 * RANDN() AS `V924`, `S1` + 0.9797959 * RANDN() AS `V925`, `S1` + 0.9797959 * RANDN() AS `V926`, `S1` + 0.9797959 * RANDN() AS `V927`, `S1` + 0.9797959 * RANDN() AS `V928`, `S1` + 0.9797959 * RANDN() AS `V929`, `S1` + 0.9797959 * RANDN() AS `V930`, `S1` + 0.9797959 * RANDN() AS `V931`, `S1` + 0.9797959 * RANDN() AS `V932`, `S1` + 0.9797959 * RANDN() AS `V933`, `S1` + 0.9797959 * RANDN() AS `V934`, `S1` + 0.9797959 * RANDN() AS `V935`, `S1` + 0.9797959 * RANDN() AS `V936`, `S1` + 0.9797959 * RANDN() AS `V937`, `S1` + 0.9797959 * RANDN() AS `V938`, `S1` + 0.9797959 * RANDN() AS `V939`, `S1` + 0.9797959 * RANDN() AS `V940`, `S1` + 0.9797959 * RANDN() AS `V941`, `S1` + 0.9797959 * RANDN() AS `V942`, `S1` + 0.9797959 * RANDN() AS `V943`, `S1` + 0.9797959 * RANDN() AS `V944`, `S1` + 0.9797959 * RANDN() AS `V945`, `S1` + 0.9797959 * RANDN() AS `V946`, `S1` + 0.9797959 * RANDN() AS `V947`, `S1` + 0.9797959 * RANDN() AS `V948`, `S1` + 0.9797959 * RANDN() AS `V949`, `S1` + 0.9797959 * RANDN() AS `V950`, `S1` + 0.9797959 * RANDN() AS `V951`, `S1` + 0.9797959 * RANDN() AS `V952`, `S1` + 0.9797959 * RANDN() AS `V953`, `S1` + 0.9797959 * RANDN() AS `V954`, `S1` + 0.9797959 * RANDN() AS `V955`, `S1` + 0.9797959 * RANDN() AS `V956`, `S1` + 0.9797959 * RANDN() AS `V957`, `S1` + 0.9797959 * RANDN() AS `V958`, `S1` + 0.9797959 * RANDN() AS `V959`, `S1` + 0.9797959 * RANDN() AS `V960`, `S1` + 0.9797959 * RANDN() AS `V961`, `S1` + 0.9797959 * RANDN() AS `V962`, `S1` + 0.9797959 * RANDN() AS `V963`, `S1` + 0.9797959 * RANDN() AS `V964`, `S1` + 0.9797959 * RANDN() AS `V965`, `S1` + 0.9797959 * RANDN() AS `V966`, `S1` + 0.9797959 * RANDN() AS `V967`, `S1` + 0.9797959 * RANDN() AS `V968`, `S1` + 0.9797959 * RANDN() AS `V969`, `S1` + 0.9797959 * RANDN() AS `V970`, `S1` + 0.9797959 * RANDN() AS `V971`, `S1` + 0.9797959 * RANDN() AS `V972`, `S1` + 0.9797959 * RANDN() AS `V973`, `S1` + 0.9797959 * RANDN() AS `V974`, `S1` + 0.9797959 * RANDN() AS `V975`, `S1` + 0.9797959 * RANDN() AS `V976`, `S1` + 0.9797959 * RANDN() AS `V977`, `S1` + 0.9797959 * RANDN() AS `V978`, `S1` + 0.9797959 * RANDN() AS `V979`, `S1` + 0.9797959 * RANDN() AS `V980`, `S1` + 0.9797959 * RANDN() AS `V981`, `S1` + 0.9797959 * RANDN() AS `V982`, `S1` + 0.9797959 * RANDN() AS `V983`, `S1` + 0.9797959 * RANDN() AS `V984`, `S1` + 0.9797959 * RANDN() AS `V985`, `S1` + 0.9797959 * RANDN() AS `V986`, `S1` + 0.9797959 * RANDN() AS `V987`, `S1` + 0.9797959 * RANDN() AS `V988`, `S1` + 0.9797959 * RANDN() AS `V989`, `S1` + 0.9797959 * RANDN() AS `V990`, `S1` + 0.9797959 * RANDN() AS `V991`, `S1` + 0.9797959 * RANDN() AS `V992`, `S1` + 0.9797959 * RANDN() AS `V993`, `S1` + 0.9797959 * RANDN() AS `V994`, `S1` + 0.9797959 * RANDN() AS `V995`, `S1` + 0.9797959 * RANDN() AS `V996`, `S1` + 0.9797959 * RANDN() AS `V997`, `S1` + 0.9797959 * RANDN() AS `V998`, `S1` + 0.9797959 * RANDN() AS `V999`, `S1` + 0.9797959 * RANDN() AS `V1000`
FROM `analyis_tbl`) `jiyzvcoxqq`
17/12/20 21:56:22 INFO ContextCleaner: Cleaned accumulator 238
17/12/20 21:56:22 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:55541 in memory (size: 5.6 KB, free: 2004.5 MB)
17/12/20 21:56:22 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 127.0.0.1:55541 in memory (size: 3.7 KB, free: 2004.5 MB)
17/12/20 21:56:22 INFO ContextCleaner: Cleaned accumulator 1
17/12/20 21:56:22 INFO ContextCleaner: Cleaned accumulator 0
17/12/20 21:56:23 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 21:56:23 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:56:23 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz14`
WHERE (0 = 1)
17/12/20 21:56:23 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:56:23 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 21:56:24 INFO SparkContext: Starting job: take at <unknown>:0
17/12/20 21:56:24 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 145 bytes
17/12/20 21:56:24 INFO DAGScheduler: Got job 3 (take at <unknown>:0) with 1 output partitions
17/12/20 21:56:24 INFO DAGScheduler: Final stage: ResultStage 8 (take at <unknown>:0)
17/12/20 21:56:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
17/12/20 21:56:24 INFO DAGScheduler: Missing parents: List()
17/12/20 21:56:24 INFO DAGScheduler: Submitting ResultStage 8 (WorkerRDD[31] at RDD at rdd.scala:18), which has no missing parents
17/12/20 21:56:24 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 567.9 KB, free 2003.7 MB)
17/12/20 21:56:24 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 189.3 KB, free 2003.5 MB)
17/12/20 21:56:24 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:55541 (size: 189.3 KB, free: 2004.3 MB)
17/12/20 21:56:24 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:996
17/12/20 21:56:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (WorkerRDD[31] at RDD at rdd.scala:18)
17/12/20 21:56:24 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/12/20 21:56:24 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5913 bytes)
17/12/20 21:56:24 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
17/12/20 21:56:24 INFO BlockManager: Found block rdd_12_0 locally
17/12/20 21:56:24 INFO CodeGenerator: Code generated in 16.262409 ms
17/12/20 21:56:25 INFO CodeGenerator: Code generated in 621.843282 ms
17/12/20 21:56:25 INFO CodeGenerator: Code generated in 117.876228 ms
17/12/20 21:59:20 INFO MemoryStore: Block rdd_31_0 stored as values in memory (estimated size 133.7 MB, free 1869.8 MB)
17/12/20 21:59:20 INFO BlockManagerInfo: Added rdd_31_0 in memory on 127.0.0.1:55541 (size: 133.7 MB, free: 1870.6 MB)
17/12/20 21:59:20 WARN Executor: 1 block locks were not released by TID = 8:
[rdd_31_0]
17/12/20 21:59:20 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 143298 bytes result sent to driver
17/12/20 21:59:20 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 175872 ms on localhost (executor driver) (1/1)
17/12/20 21:59:20 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/12/20 21:59:20 INFO DAGScheduler: ResultStage 8 (take at <unknown>:0) finished in 175.872 s
17/12/20 21:59:20 INFO DAGScheduler: Job 3 finished: take at <unknown>:0, took 175.902486 s
17/12/20 21:59:21 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:59:21 INFO SparkSqlParser: Parsing command: sparklyr_tmp_171410226c66
17/12/20 21:59:21 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:59:21 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_171410226c66` AS `zzz15`
WHERE (0 = 1)
17/12/20 21:59:21 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:59:21 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_171410226c66`
17/12/20 21:59:21 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 21:59:21 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:59:21 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz16`
WHERE (0 = 1)
17/12/20 21:59:24 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:59:24 INFO SparkSqlParser: Parsing command: SELECT (`V1` < 0.03) AS `V1`, (`V2` < 0.03) AS `V2`, (`V3` < 0.03) AS `V3`, (`V4` < 0.03) AS `V4`, (`V5` < 0.03) AS `V5`, (`V6` < 0.03) AS `V6`, (`V7` < 0.03) AS `V7`, (`V8` < 0.03) AS `V8`, (`V9` < 0.03) AS `V9`, (`V10` < 0.03) AS `V10`, (`V11` < 0.03) AS `V11`, (`V12` < 0.03) AS `V12`, (`V13` < 0.03) AS `V13`, (`V14` < 0.03) AS `V14`, (`V15` < 0.03) AS `V15`, (`V16` < 0.03) AS `V16`, (`V17` < 0.03) AS `V17`, (`V18` < 0.03) AS `V18`, (`V19` < 0.03) AS `V19`, (`V20` < 0.03) AS `V20`, (`V21` < 0.03) AS `V21`, (`V22` < 0.03) AS `V22`, (`V23` < 0.03) AS `V23`, (`V24` < 0.03) AS `V24`, (`V25` < 0.03) AS `V25`, (`V26` < 0.03) AS `V26`, (`V27` < 0.03) AS `V27`, (`V28` < 0.03) AS `V28`, (`V29` < 0.03) AS `V29`, (`V30` < 0.03) AS `V30`, (`V31` < 0.03) AS `V31`, (`V32` < 0.03) AS `V32`, (`V33` < 0.03) AS `V33`, (`V34` < 0.03) AS `V34`, (`V35` < 0.03) AS `V35`, (`V36` < 0.03) AS `V36`, (`V37` < 0.03) AS `V37`, (`V38` < 0.03) AS `V38`, (`V39` < 0.03) AS `V39`, (`V40` < 0.03) AS `V40`, (`V41` < 0.03) AS `V41`, (`V42` < 0.03) AS `V42`, (`V43` < 0.03) AS `V43`, (`V44` < 0.03) AS `V44`, (`V45` < 0.03) AS `V45`, (`V46` < 0.03) AS `V46`, (`V47` < 0.03) AS `V47`, (`V48` < 0.03) AS `V48`, (`V49` < 0.03) AS `V49`, (`V50` < 0.03) AS `V50`, (`V51` < 0.03) AS `V51`, (`V52` < 0.03) AS `V52`, (`V53` < 0.03) AS `V53`, (`V54` < 0.03) AS `V54`, (`V55` < 0.03) AS `V55`, (`V56` < 0.03) AS `V56`, (`V57` < 0.03) AS `V57`, (`V58` < 0.03) AS `V58`, (`V59` < 0.03) AS `V59`, (`V60` < 0.03) AS `V60`, (`V61` < 0.03) AS `V61`, (`V62` < 0.03) AS `V62`, (`V63` < 0.03) AS `V63`, (`V64` < 0.03) AS `V64`, (`V65` < 0.03) AS `V65`, (`V66` < 0.03) AS `V66`, (`V67` < 0.03) AS `V67`, (`V68` < 0.03) AS `V68`, (`V69` < 0.03) AS `V69`, (`V70` < 0.03) AS `V70`, (`V71` < 0.03) AS `V71`, (`V72` < 0.03) AS `V72`, (`V73` < 0.03) AS `V73`, (`V74` < 0.03) AS `V74`, (`V75` < 0.03) AS `V75`, (`V76` < 0.03) AS `V76`, (`V77` < 0.03) AS `V77`, (`V78` < 0.03) AS `V78`, (`V79` < 0.03) AS `V79`, (`V80` < 0.03) AS `V80`, (`V81` < 0.03) AS `V81`, (`V82` < 0.03) AS `V82`, (`V83` < 0.03) AS `V83`, (`V84` < 0.03) AS `V84`, (`V85` < 0.03) AS `V85`, (`V86` < 0.03) AS `V86`, (`V87` < 0.03) AS `V87`, (`V88` < 0.03) AS `V88`, (`V89` < 0.03) AS `V89`, (`V90` < 0.03) AS `V90`, (`V91` < 0.03) AS `V91`, (`V92` < 0.03) AS `V92`, (`V93` < 0.03) AS `V93`, (`V94` < 0.03) AS `V94`, (`V95` < 0.03) AS `V95`, (`V96` < 0.03) AS `V96`, (`V97` < 0.03) AS `V97`, (`V98` < 0.03) AS `V98`, (`V99` < 0.03) AS `V99`, (`V100` < 0.03) AS `V100`, (`V101` < 0.03) AS `V101`, (`V102` < 0.03) AS `V102`, (`V103` < 0.03) AS `V103`, (`V104` < 0.03) AS `V104`, (`V105` < 0.03) AS `V105`, (`V106` < 0.03) AS `V106`, (`V107` < 0.03) AS `V107`, (`V108` < 0.03) AS `V108`, (`V109` < 0.03) AS `V109`, (`V110` < 0.03) AS `V110`, (`V111` < 0.03) AS `V111`, (`V112` < 0.03) AS `V112`, (`V113` < 0.03) AS `V113`, (`V114` < 0.03) AS `V114`, (`V115` < 0.03) AS `V115`, (`V116` < 0.03) AS `V116`, (`V117` < 0.03) AS `V117`, (`V118` < 0.03) AS `V118`, (`V119` < 0.03) AS `V119`, (`V120` < 0.03) AS `V120`, (`V121` < 0.03) AS `V121`, (`V122` < 0.03) AS `V122`, (`V123` < 0.03) AS `V123`, (`V124` < 0.03) AS `V124`, (`V125` < 0.03) AS `V125`, (`V126` < 0.03) AS `V126`, (`V127` < 0.03) AS `V127`, (`V128` < 0.03) AS `V128`, (`V129` < 0.03) AS `V129`, (`V130` < 0.03) AS `V130`, (`V131` < 0.03) AS `V131`, (`V132` < 0.03) AS `V132`, (`V133` < 0.03) AS `V133`, (`V134` < 0.03) AS `V134`, (`V135` < 0.03) AS `V135`, (`V136` < 0.03) AS `V136`, (`V137` < 0.03) AS `V137`, (`V138` < 0.03) AS `V138`, (`V139` < 0.03) AS `V139`, (`V140` < 0.03) AS `V140`, (`V141` < 0.03) AS `V141`, (`V142` < 0.03) AS `V142`, (`V143` < 0.03) AS `V143`, (`V144` < 0.03) AS `V144`, (`V145` < 0.03) AS `V145`, (`V146` < 0.03) AS `V146`, (`V147` < 0.03) AS `V147`, (`V148` < 0.03) AS `V148`, (`V149` < 0.03) AS `V149`, (`V150` < 0.03) AS `V150`, (`V151` < 0.03) AS `V151`, (`V152` < 0.03) AS `V152`, (`V153` < 0.03) AS `V153`, (`V154` < 0.03) AS `V154`, (`V155` < 0.03) AS `V155`, (`V156` < 0.03) AS `V156`, (`V157` < 0.03) AS `V157`, (`V158` < 0.03) AS `V158`, (`V159` < 0.03) AS `V159`, (`V160` < 0.03) AS `V160`, (`V161` < 0.03) AS `V161`, (`V162` < 0.03) AS `V162`, (`V163` < 0.03) AS `V163`, (`V164` < 0.03) AS `V164`, (`V165` < 0.03) AS `V165`, (`V166` < 0.03) AS `V166`, (`V167` < 0.03) AS `V167`, (`V168` < 0.03) AS `V168`, (`V169` < 0.03) AS `V169`, (`V170` < 0.03) AS `V170`, (`V171` < 0.03) AS `V171`, (`V172` < 0.03) AS `V172`, (`V173` < 0.03) AS `V173`, (`V174` < 0.03) AS `V174`, (`V175` < 0.03) AS `V175`, (`V176` < 0.03) AS `V176`, (`V177` < 0.03) AS `V177`, (`V178` < 0.03) AS `V178`, (`V179` < 0.03) AS `V179`, (`V180` < 0.03) AS `V180`, (`V181` < 0.03) AS `V181`, (`V182` < 0.03) AS `V182`, (`V183` < 0.03) AS `V183`, (`V184` < 0.03) AS `V184`, (`V185` < 0.03) AS `V185`, (`V186` < 0.03) AS `V186`, (`V187` < 0.03) AS `V187`, (`V188` < 0.03) AS `V188`, (`V189` < 0.03) AS `V189`, (`V190` < 0.03) AS `V190`, (`V191` < 0.03) AS `V191`, (`V192` < 0.03) AS `V192`, (`V193` < 0.03) AS `V193`, (`V194` < 0.03) AS `V194`, (`V195` < 0.03) AS `V195`, (`V196` < 0.03) AS `V196`, (`V197` < 0.03) AS `V197`, (`V198` < 0.03) AS `V198`, (`V199` < 0.03) AS `V199`, (`V200` < 0.03) AS `V200`, (`V201` < 0.03) AS `V201`, (`V202` < 0.03) AS `V202`, (`V203` < 0.03) AS `V203`, (`V204` < 0.03) AS `V204`, (`V205` < 0.03) AS `V205`, (`V206` < 0.03) AS `V206`, (`V207` < 0.03) AS `V207`, (`V208` < 0.03) AS `V208`, (`V209` < 0.03) AS `V209`, (`V210` < 0.03) AS `V210`, (`V211` < 0.03) AS `V211`, (`V212` < 0.03) AS `V212`, (`V213` < 0.03) AS `V213`, (`V214` < 0.03) AS `V214`, (`V215` < 0.03) AS `V215`, (`V216` < 0.03) AS `V216`, (`V217` < 0.03) AS `V217`, (`V218` < 0.03) AS `V218`, (`V219` < 0.03) AS `V219`, (`V220` < 0.03) AS `V220`, (`V221` < 0.03) AS `V221`, (`V222` < 0.03) AS `V222`, (`V223` < 0.03) AS `V223`, (`V224` < 0.03) AS `V224`, (`V225` < 0.03) AS `V225`, (`V226` < 0.03) AS `V226`, (`V227` < 0.03) AS `V227`, (`V228` < 0.03) AS `V228`, (`V229` < 0.03) AS `V229`, (`V230` < 0.03) AS `V230`, (`V231` < 0.03) AS `V231`, (`V232` < 0.03) AS `V232`, (`V233` < 0.03) AS `V233`, (`V234` < 0.03) AS `V234`, (`V235` < 0.03) AS `V235`, (`V236` < 0.03) AS `V236`, (`V237` < 0.03) AS `V237`, (`V238` < 0.03) AS `V238`, (`V239` < 0.03) AS `V239`, (`V240` < 0.03) AS `V240`, (`V241` < 0.03) AS `V241`, (`V242` < 0.03) AS `V242`, (`V243` < 0.03) AS `V243`, (`V244` < 0.03) AS `V244`, (`V245` < 0.03) AS `V245`, (`V246` < 0.03) AS `V246`, (`V247` < 0.03) AS `V247`, (`V248` < 0.03) AS `V248`, (`V249` < 0.03) AS `V249`, (`V250` < 0.03) AS `V250`, (`V251` < 0.03) AS `V251`, (`V252` < 0.03) AS `V252`, (`V253` < 0.03) AS `V253`, (`V254` < 0.03) AS `V254`, (`V255` < 0.03) AS `V255`, (`V256` < 0.03) AS `V256`, (`V257` < 0.03) AS `V257`, (`V258` < 0.03) AS `V258`, (`V259` < 0.03) AS `V259`, (`V260` < 0.03) AS `V260`, (`V261` < 0.03) AS `V261`, (`V262` < 0.03) AS `V262`, (`V263` < 0.03) AS `V263`, (`V264` < 0.03) AS `V264`, (`V265` < 0.03) AS `V265`, (`V266` < 0.03) AS `V266`, (`V267` < 0.03) AS `V267`, (`V268` < 0.03) AS `V268`, (`V269` < 0.03) AS `V269`, (`V270` < 0.03) AS `V270`, (`V271` < 0.03) AS `V271`, (`V272` < 0.03) AS `V272`, (`V273` < 0.03) AS `V273`, (`V274` < 0.03) AS `V274`, (`V275` < 0.03) AS `V275`, (`V276` < 0.03) AS `V276`, (`V277` < 0.03) AS `V277`, (`V278` < 0.03) AS `V278`, (`V279` < 0.03) AS `V279`, (`V280` < 0.03) AS `V280`, (`V281` < 0.03) AS `V281`, (`V282` < 0.03) AS `V282`, (`V283` < 0.03) AS `V283`, (`V284` < 0.03) AS `V284`, (`V285` < 0.03) AS `V285`, (`V286` < 0.03) AS `V286`, (`V287` < 0.03) AS `V287`, (`V288` < 0.03) AS `V288`, (`V289` < 0.03) AS `V289`, (`V290` < 0.03) AS `V290`, (`V291` < 0.03) AS `V291`, (`V292` < 0.03) AS `V292`, (`V293` < 0.03) AS `V293`, (`V294` < 0.03) AS `V294`, (`V295` < 0.03) AS `V295`, (`V296` < 0.03) AS `V296`, (`V297` < 0.03) AS `V297`, (`V298` < 0.03) AS `V298`, (`V299` < 0.03) AS `V299`, (`V300` < 0.03) AS `V300`, (`V301` < 0.03) AS `V301`, (`V302` < 0.03) AS `V302`, (`V303` < 0.03) AS `V303`, (`V304` < 0.03) AS `V304`, (`V305` < 0.03) AS `V305`, (`V306` < 0.03) AS `V306`, (`V307` < 0.03) AS `V307`, (`V308` < 0.03) AS `V308`, (`V309` < 0.03) AS `V309`, (`V310` < 0.03) AS `V310`, (`V311` < 0.03) AS `V311`, (`V312` < 0.03) AS `V312`, (`V313` < 0.03) AS `V313`, (`V314` < 0.03) AS `V314`, (`V315` < 0.03) AS `V315`, (`V316` < 0.03) AS `V316`, (`V317` < 0.03) AS `V317`, (`V318` < 0.03) AS `V318`, (`V319` < 0.03) AS `V319`, (`V320` < 0.03) AS `V320`, (`V321` < 0.03) AS `V321`, (`V322` < 0.03) AS `V322`, (`V323` < 0.03) AS `V323`, (`V324` < 0.03) AS `V324`, (`V325` < 0.03) AS `V325`, (`V326` < 0.03) AS `V326`, (`V327` < 0.03) AS `V327`, (`V328` < 0.03) AS `V328`, (`V329` < 0.03) AS `V329`, (`V330` < 0.03) AS `V330`, (`V331` < 0.03) AS `V331`, (`V332` < 0.03) AS `V332`, (`V333` < 0.03) AS `V333`, (`V334` < 0.03) AS `V334`, (`V335` < 0.03) AS `V335`, (`V336` < 0.03) AS `V336`, (`V337` < 0.03) AS `V337`, (`V338` < 0.03) AS `V338`, (`V339` < 0.03) AS `V339`, (`V340` < 0.03) AS `V340`, (`V341` < 0.03) AS `V341`, (`V342` < 0.03) AS `V342`, (`V343` < 0.03) AS `V343`, (`V344` < 0.03) AS `V344`, (`V345` < 0.03) AS `V345`, (`V346` < 0.03) AS `V346`, (`V347` < 0.03) AS `V347`, (`V348` < 0.03) AS `V348`, (`V349` < 0.03) AS `V349`, (`V350` < 0.03) AS `V350`, (`V351` < 0.03) AS `V351`, (`V352` < 0.03) AS `V352`, (`V353` < 0.03) AS `V353`, (`V354` < 0.03) AS `V354`, (`V355` < 0.03) AS `V355`, (`V356` < 0.03) AS `V356`, (`V357` < 0.03) AS `V357`, (`V358` < 0.03) AS `V358`, (`V359` < 0.03) AS `V359`, (`V360` < 0.03) AS `V360`, (`V361` < 0.03) AS `V361`, (`V362` < 0.03) AS `V362`, (`V363` < 0.03) AS `V363`, (`V364` < 0.03) AS `V364`, (`V365` < 0.03) AS `V365`, (`V366` < 0.03) AS `V366`, (`V367` < 0.03) AS `V367`, (`V368` < 0.03) AS `V368`, (`V369` < 0.03) AS `V369`, (`V370` < 0.03) AS `V370`, (`V371` < 0.03) AS `V371`, (`V372` < 0.03) AS `V372`, (`V373` < 0.03) AS `V373`, (`V374` < 0.03) AS `V374`, (`V375` < 0.03) AS `V375`, (`V376` < 0.03) AS `V376`, (`V377` < 0.03) AS `V377`, (`V378` < 0.03) AS `V378`, (`V379` < 0.03) AS `V379`, (`V380` < 0.03) AS `V380`, (`V381` < 0.03) AS `V381`, (`V382` < 0.03) AS `V382`, (`V383` < 0.03) AS `V383`, (`V384` < 0.03) AS `V384`, (`V385` < 0.03) AS `V385`, (`V386` < 0.03) AS `V386`, (`V387` < 0.03) AS `V387`, (`V388` < 0.03) AS `V388`, (`V389` < 0.03) AS `V389`, (`V390` < 0.03) AS `V390`, (`V391` < 0.03) AS `V391`, (`V392` < 0.03) AS `V392`, (`V393` < 0.03) AS `V393`, (`V394` < 0.03) AS `V394`, (`V395` < 0.03) AS `V395`, (`V396` < 0.03) AS `V396`, (`V397` < 0.03) AS `V397`, (`V398` < 0.03) AS `V398`, (`V399` < 0.03) AS `V399`, (`V400` < 0.03) AS `V400`, (`V401` < 0.03) AS `V401`, (`V402` < 0.03) AS `V402`, (`V403` < 0.03) AS `V403`, (`V404` < 0.03) AS `V404`, (`V405` < 0.03) AS `V405`, (`V406` < 0.03) AS `V406`, (`V407` < 0.03) AS `V407`, (`V408` < 0.03) AS `V408`, (`V409` < 0.03) AS `V409`, (`V410` < 0.03) AS `V410`, (`V411` < 0.03) AS `V411`, (`V412` < 0.03) AS `V412`, (`V413` < 0.03) AS `V413`, (`V414` < 0.03) AS `V414`, (`V415` < 0.03) AS `V415`, (`V416` < 0.03) AS `V416`, (`V417` < 0.03) AS `V417`, (`V418` < 0.03) AS `V418`, (`V419` < 0.03) AS `V419`, (`V420` < 0.03) AS `V420`, (`V421` < 0.03) AS `V421`, (`V422` < 0.03) AS `V422`, (`V423` < 0.03) AS `V423`, (`V424` < 0.03) AS `V424`, (`V425` < 0.03) AS `V425`, (`V426` < 0.03) AS `V426`, (`V427` < 0.03) AS `V427`, (`V428` < 0.03) AS `V428`, (`V429` < 0.03) AS `V429`, (`V430` < 0.03) AS `V430`, (`V431` < 0.03) AS `V431`, (`V432` < 0.03) AS `V432`, (`V433` < 0.03) AS `V433`, (`V434` < 0.03) AS `V434`, (`V435` < 0.03) AS `V435`, (`V436` < 0.03) AS `V436`, (`V437` < 0.03) AS `V437`, (`V438` < 0.03) AS `V438`, (`V439` < 0.03) AS `V439`, (`V440` < 0.03) AS `V440`, (`V441` < 0.03) AS `V441`, (`V442` < 0.03) AS `V442`, (`V443` < 0.03) AS `V443`, (`V444` < 0.03) AS `V444`, (`V445` < 0.03) AS `V445`, (`V446` < 0.03) AS `V446`, (`V447` < 0.03) AS `V447`, (`V448` < 0.03) AS `V448`, (`V449` < 0.03) AS `V449`, (`V450` < 0.03) AS `V450`, (`V451` < 0.03) AS `V451`, (`V452` < 0.03) AS `V452`, (`V453` < 0.03) AS `V453`, (`V454` < 0.03) AS `V454`, (`V455` < 0.03) AS `V455`, (`V456` < 0.03) AS `V456`, (`V457` < 0.03) AS `V457`, (`V458` < 0.03) AS `V458`, (`V459` < 0.03) AS `V459`, (`V460` < 0.03) AS `V460`, (`V461` < 0.03) AS `V461`, (`V462` < 0.03) AS `V462`, (`V463` < 0.03) AS `V463`, (`V464` < 0.03) AS `V464`, (`V465` < 0.03) AS `V465`, (`V466` < 0.03) AS `V466`, (`V467` < 0.03) AS `V467`, (`V468` < 0.03) AS `V468`, (`V469` < 0.03) AS `V469`, (`V470` < 0.03) AS `V470`, (`V471` < 0.03) AS `V471`, (`V472` < 0.03) AS `V472`, (`V473` < 0.03) AS `V473`, (`V474` < 0.03) AS `V474`, (`V475` < 0.03) AS `V475`, (`V476` < 0.03) AS `V476`, (`V477` < 0.03) AS `V477`, (`V478` < 0.03) AS `V478`, (`V479` < 0.03) AS `V479`, (`V480` < 0.03) AS `V480`, (`V481` < 0.03) AS `V481`, (`V482` < 0.03) AS `V482`, (`V483` < 0.03) AS `V483`, (`V484` < 0.03) AS `V484`, (`V485` < 0.03) AS `V485`, (`V486` < 0.03) AS `V486`, (`V487` < 0.03) AS `V487`, (`V488` < 0.03) AS `V488`, (`V489` < 0.03) AS `V489`, (`V490` < 0.03) AS `V490`, (`V491` < 0.03) AS `V491`, (`V492` < 0.03) AS `V492`, (`V493` < 0.03) AS `V493`, (`V494` < 0.03) AS `V494`, (`V495` < 0.03) AS `V495`, (`V496` < 0.03) AS `V496`, (`V497` < 0.03) AS `V497`, (`V498` < 0.03) AS `V498`, (`V499` < 0.03) AS `V499`, (`V500` < 0.03) AS `V500`, (`V501` < 0.03) AS `V501`, (`V502` < 0.03) AS `V502`, (`V503` < 0.03) AS `V503`, (`V504` < 0.03) AS `V504`, (`V505` < 0.03) AS `V505`, (`V506` < 0.03) AS `V506`, (`V507` < 0.03) AS `V507`, (`V508` < 0.03) AS `V508`, (`V509` < 0.03) AS `V509`, (`V510` < 0.03) AS `V510`, (`V511` < 0.03) AS `V511`, (`V512` < 0.03) AS `V512`, (`V513` < 0.03) AS `V513`, (`V514` < 0.03) AS `V514`, (`V515` < 0.03) AS `V515`, (`V516` < 0.03) AS `V516`, (`V517` < 0.03) AS `V517`, (`V518` < 0.03) AS `V518`, (`V519` < 0.03) AS `V519`, (`V520` < 0.03) AS `V520`, (`V521` < 0.03) AS `V521`, (`V522` < 0.03) AS `V522`, (`V523` < 0.03) AS `V523`, (`V524` < 0.03) AS `V524`, (`V525` < 0.03) AS `V525`, (`V526` < 0.03) AS `V526`, (`V527` < 0.03) AS `V527`, (`V528` < 0.03) AS `V528`, (`V529` < 0.03) AS `V529`, (`V530` < 0.03) AS `V530`, (`V531` < 0.03) AS `V531`, (`V532` < 0.03) AS `V532`, (`V533` < 0.03) AS `V533`, (`V534` < 0.03) AS `V534`, (`V535` < 0.03) AS `V535`, (`V536` < 0.03) AS `V536`, (`V537` < 0.03) AS `V537`, (`V538` < 0.03) AS `V538`, (`V539` < 0.03) AS `V539`, (`V540` < 0.03) AS `V540`, (`V541` < 0.03) AS `V541`, (`V542` < 0.03) AS `V542`, (`V543` < 0.03) AS `V543`, (`V544` < 0.03) AS `V544`, (`V545` < 0.03) AS `V545`, (`V546` < 0.03) AS `V546`, (`V547` < 0.03) AS `V547`, (`V548` < 0.03) AS `V548`, (`V549` < 0.03) AS `V549`, (`V550` < 0.03) AS `V550`, (`V551` < 0.03) AS `V551`, (`V552` < 0.03) AS `V552`, (`V553` < 0.03) AS `V553`, (`V554` < 0.03) AS `V554`, (`V555` < 0.03) AS `V555`, (`V556` < 0.03) AS `V556`, (`V557` < 0.03) AS `V557`, (`V558` < 0.03) AS `V558`, (`V559` < 0.03) AS `V559`, (`V560` < 0.03) AS `V560`, (`V561` < 0.03) AS `V561`, (`V562` < 0.03) AS `V562`, (`V563` < 0.03) AS `V563`, (`V564` < 0.03) AS `V564`, (`V565` < 0.03) AS `V565`, (`V566` < 0.03) AS `V566`, (`V567` < 0.03) AS `V567`, (`V568` < 0.03) AS `V568`, (`V569` < 0.03) AS `V569`, (`V570` < 0.03) AS `V570`, (`V571` < 0.03) AS `V571`, (`V572` < 0.03) AS `V572`, (`V573` < 0.03) AS `V573`, (`V574` < 0.03) AS `V574`, (`V575` < 0.03) AS `V575`, (`V576` < 0.03) AS `V576`, (`V577` < 0.03) AS `V577`, (`V578` < 0.03) AS `V578`, (`V579` < 0.03) AS `V579`, (`V580` < 0.03) AS `V580`, (`V581` < 0.03) AS `V581`, (`V582` < 0.03) AS `V582`, (`V583` < 0.03) AS `V583`, (`V584` < 0.03) AS `V584`, (`V585` < 0.03) AS `V585`, (`V586` < 0.03) AS `V586`, (`V587` < 0.03) AS `V587`, (`V588` < 0.03) AS `V588`, (`V589` < 0.03) AS `V589`, (`V590` < 0.03) AS `V590`, (`V591` < 0.03) AS `V591`, (`V592` < 0.03) AS `V592`, (`V593` < 0.03) AS `V593`, (`V594` < 0.03) AS `V594`, (`V595` < 0.03) AS `V595`, (`V596` < 0.03) AS `V596`, (`V597` < 0.03) AS `V597`, (`V598` < 0.03) AS `V598`, (`V599` < 0.03) AS `V599`, (`V600` < 0.03) AS `V600`, (`V601` < 0.03) AS `V601`, (`V602` < 0.03) AS `V602`, (`V603` < 0.03) AS `V603`, (`V604` < 0.03) AS `V604`, (`V605` < 0.03) AS `V605`, (`V606` < 0.03) AS `V606`, (`V607` < 0.03) AS `V607`, (`V608` < 0.03) AS `V608`, (`V609` < 0.03) AS `V609`, (`V610` < 0.03) AS `V610`, (`V611` < 0.03) AS `V611`, (`V612` < 0.03) AS `V612`, (`V613` < 0.03) AS `V613`, (`V614` < 0.03) AS `V614`, (`V615` < 0.03) AS `V615`, (`V616` < 0.03) AS `V616`, (`V617` < 0.03) AS `V617`, (`V618` < 0.03) AS `V618`, (`V619` < 0.03) AS `V619`, (`V620` < 0.03) AS `V620`, (`V621` < 0.03) AS `V621`, (`V622` < 0.03) AS `V622`, (`V623` < 0.03) AS `V623`, (`V624` < 0.03) AS `V624`, (`V625` < 0.03) AS `V625`, (`V626` < 0.03) AS `V626`, (`V627` < 0.03) AS `V627`, (`V628` < 0.03) AS `V628`, (`V629` < 0.03) AS `V629`, (`V630` < 0.03) AS `V630`, (`V631` < 0.03) AS `V631`, (`V632` < 0.03) AS `V632`, (`V633` < 0.03) AS `V633`, (`V634` < 0.03) AS `V634`, (`V635` < 0.03) AS `V635`, (`V636` < 0.03) AS `V636`, (`V637` < 0.03) AS `V637`, (`V638` < 0.03) AS `V638`, (`V639` < 0.03) AS `V639`, (`V640` < 0.03) AS `V640`, (`V641` < 0.03) AS `V641`, (`V642` < 0.03) AS `V642`, (`V643` < 0.03) AS `V643`, (`V644` < 0.03) AS `V644`, (`V645` < 0.03) AS `V645`, (`V646` < 0.03) AS `V646`, (`V647` < 0.03) AS `V647`, (`V648` < 0.03) AS `V648`, (`V649` < 0.03) AS `V649`, (`V650` < 0.03) AS `V650`, (`V651` < 0.03) AS `V651`, (`V652` < 0.03) AS `V652`, (`V653` < 0.03) AS `V653`, (`V654` < 0.03) AS `V654`, (`V655` < 0.03) AS `V655`, (`V656` < 0.03) AS `V656`, (`V657` < 0.03) AS `V657`, (`V658` < 0.03) AS `V658`, (`V659` < 0.03) AS `V659`, (`V660` < 0.03) AS `V660`, (`V661` < 0.03) AS `V661`, (`V662` < 0.03) AS `V662`, (`V663` < 0.03) AS `V663`, (`V664` < 0.03) AS `V664`, (`V665` < 0.03) AS `V665`, (`V666` < 0.03) AS `V666`, (`V667` < 0.03) AS `V667`, (`V668` < 0.03) AS `V668`, (`V669` < 0.03) AS `V669`, (`V670` < 0.03) AS `V670`, (`V671` < 0.03) AS `V671`, (`V672` < 0.03) AS `V672`, (`V673` < 0.03) AS `V673`, (`V674` < 0.03) AS `V674`, (`V675` < 0.03) AS `V675`, (`V676` < 0.03) AS `V676`, (`V677` < 0.03) AS `V677`, (`V678` < 0.03) AS `V678`, (`V679` < 0.03) AS `V679`, (`V680` < 0.03) AS `V680`, (`V681` < 0.03) AS `V681`, (`V682` < 0.03) AS `V682`, (`V683` < 0.03) AS `V683`, (`V684` < 0.03) AS `V684`, (`V685` < 0.03) AS `V685`, (`V686` < 0.03) AS `V686`, (`V687` < 0.03) AS `V687`, (`V688` < 0.03) AS `V688`, (`V689` < 0.03) AS `V689`, (`V690` < 0.03) AS `V690`, (`V691` < 0.03) AS `V691`, (`V692` < 0.03) AS `V692`, (`V693` < 0.03) AS `V693`, (`V694` < 0.03) AS `V694`, (`V695` < 0.03) AS `V695`, (`V696` < 0.03) AS `V696`, (`V697` < 0.03) AS `V697`, (`V698` < 0.03) AS `V698`, (`V699` < 0.03) AS `V699`, (`V700` < 0.03) AS `V700`, (`V701` < 0.03) AS `V701`, (`V702` < 0.03) AS `V702`, (`V703` < 0.03) AS `V703`, (`V704` < 0.03) AS `V704`, (`V705` < 0.03) AS `V705`, (`V706` < 0.03) AS `V706`, (`V707` < 0.03) AS `V707`, (`V708` < 0.03) AS `V708`, (`V709` < 0.03) AS `V709`, (`V710` < 0.03) AS `V710`, (`V711` < 0.03) AS `V711`, (`V712` < 0.03) AS `V712`, (`V713` < 0.03) AS `V713`, (`V714` < 0.03) AS `V714`, (`V715` < 0.03) AS `V715`, (`V716` < 0.03) AS `V716`, (`V717` < 0.03) AS `V717`, (`V718` < 0.03) AS `V718`, (`V719` < 0.03) AS `V719`, (`V720` < 0.03) AS `V720`, (`V721` < 0.03) AS `V721`, (`V722` < 0.03) AS `V722`, (`V723` < 0.03) AS `V723`, (`V724` < 0.03) AS `V724`, (`V725` < 0.03) AS `V725`, (`V726` < 0.03) AS `V726`, (`V727` < 0.03) AS `V727`, (`V728` < 0.03) AS `V728`, (`V729` < 0.03) AS `V729`, (`V730` < 0.03) AS `V730`, (`V731` < 0.03) AS `V731`, (`V732` < 0.03) AS `V732`, (`V733` < 0.03) AS `V733`, (`V734` < 0.03) AS `V734`, (`V735` < 0.03) AS `V735`, (`V736` < 0.03) AS `V736`, (`V737` < 0.03) AS `V737`, (`V738` < 0.03) AS `V738`, (`V739` < 0.03) AS `V739`, (`V740` < 0.03) AS `V740`, (`V741` < 0.03) AS `V741`, (`V742` < 0.03) AS `V742`, (`V743` < 0.03) AS `V743`, (`V744` < 0.03) AS `V744`, (`V745` < 0.03) AS `V745`, (`V746` < 0.03) AS `V746`, (`V747` < 0.03) AS `V747`, (`V748` < 0.03) AS `V748`, (`V749` < 0.03) AS `V749`, (`V750` < 0.03) AS `V750`, (`V751` < 0.03) AS `V751`, (`V752` < 0.03) AS `V752`, (`V753` < 0.03) AS `V753`, (`V754` < 0.03) AS `V754`, (`V755` < 0.03) AS `V755`, (`V756` < 0.03) AS `V756`, (`V757` < 0.03) AS `V757`, (`V758` < 0.03) AS `V758`, (`V759` < 0.03) AS `V759`, (`V760` < 0.03) AS `V760`, (`V761` < 0.03) AS `V761`, (`V762` < 0.03) AS `V762`, (`V763` < 0.03) AS `V763`, (`V764` < 0.03) AS `V764`, (`V765` < 0.03) AS `V765`, (`V766` < 0.03) AS `V766`, (`V767` < 0.03) AS `V767`, (`V768` < 0.03) AS `V768`, (`V769` < 0.03) AS `V769`, (`V770` < 0.03) AS `V770`, (`V771` < 0.03) AS `V771`, (`V772` < 0.03) AS `V772`, (`V773` < 0.03) AS `V773`, (`V774` < 0.03) AS `V774`, (`V775` < 0.03) AS `V775`, (`V776` < 0.03) AS `V776`, (`V777` < 0.03) AS `V777`, (`V778` < 0.03) AS `V778`, (`V779` < 0.03) AS `V779`, (`V780` < 0.03) AS `V780`, (`V781` < 0.03) AS `V781`, (`V782` < 0.03) AS `V782`, (`V783` < 0.03) AS `V783`, (`V784` < 0.03) AS `V784`, (`V785` < 0.03) AS `V785`, (`V786` < 0.03) AS `V786`, (`V787` < 0.03) AS `V787`, (`V788` < 0.03) AS `V788`, (`V789` < 0.03) AS `V789`, (`V790` < 0.03) AS `V790`, (`V791` < 0.03) AS `V791`, (`V792` < 0.03) AS `V792`, (`V793` < 0.03) AS `V793`, (`V794` < 0.03) AS `V794`, (`V795` < 0.03) AS `V795`, (`V796` < 0.03) AS `V796`, (`V797` < 0.03) AS `V797`, (`V798` < 0.03) AS `V798`, (`V799` < 0.03) AS `V799`, (`V800` < 0.03) AS `V800`, (`V801` < 0.03) AS `V801`, (`V802` < 0.03) AS `V802`, (`V803` < 0.03) AS `V803`, (`V804` < 0.03) AS `V804`, (`V805` < 0.03) AS `V805`, (`V806` < 0.03) AS `V806`, (`V807` < 0.03) AS `V807`, (`V808` < 0.03) AS `V808`, (`V809` < 0.03) AS `V809`, (`V810` < 0.03) AS `V810`, (`V811` < 0.03) AS `V811`, (`V812` < 0.03) AS `V812`, (`V813` < 0.03) AS `V813`, (`V814` < 0.03) AS `V814`, (`V815` < 0.03) AS `V815`, (`V816` < 0.03) AS `V816`, (`V817` < 0.03) AS `V817`, (`V818` < 0.03) AS `V818`, (`V819` < 0.03) AS `V819`, (`V820` < 0.03) AS `V820`, (`V821` < 0.03) AS `V821`, (`V822` < 0.03) AS `V822`, (`V823` < 0.03) AS `V823`, (`V824` < 0.03) AS `V824`, (`V825` < 0.03) AS `V825`, (`V826` < 0.03) AS `V826`, (`V827` < 0.03) AS `V827`, (`V828` < 0.03) AS `V828`, (`V829` < 0.03) AS `V829`, (`V830` < 0.03) AS `V830`, (`V831` < 0.03) AS `V831`, (`V832` < 0.03) AS `V832`, (`V833` < 0.03) AS `V833`, (`V834` < 0.03) AS `V834`, (`V835` < 0.03) AS `V835`, (`V836` < 0.03) AS `V836`, (`V837` < 0.03) AS `V837`, (`V838` < 0.03) AS `V838`, (`V839` < 0.03) AS `V839`, (`V840` < 0.03) AS `V840`, (`V841` < 0.03) AS `V841`, (`V842` < 0.03) AS `V842`, (`V843` < 0.03) AS `V843`, (`V844` < 0.03) AS `V844`, (`V845` < 0.03) AS `V845`, (`V846` < 0.03) AS `V846`, (`V847` < 0.03) AS `V847`, (`V848` < 0.03) AS `V848`, (`V849` < 0.03) AS `V849`, (`V850` < 0.03) AS `V850`, (`V851` < 0.03) AS `V851`, (`V852` < 0.03) AS `V852`, (`V853` < 0.03) AS `V853`, (`V854` < 0.03) AS `V854`, (`V855` < 0.03) AS `V855`, (`V856` < 0.03) AS `V856`, (`V857` < 0.03) AS `V857`, (`V858` < 0.03) AS `V858`, (`V859` < 0.03) AS `V859`, (`V860` < 0.03) AS `V860`, (`V861` < 0.03) AS `V861`, (`V862` < 0.03) AS `V862`, (`V863` < 0.03) AS `V863`, (`V864` < 0.03) AS `V864`, (`V865` < 0.03) AS `V865`, (`V866` < 0.03) AS `V866`, (`V867` < 0.03) AS `V867`, (`V868` < 0.03) AS `V868`, (`V869` < 0.03) AS `V869`, (`V870` < 0.03) AS `V870`, (`V871` < 0.03) AS `V871`, (`V872` < 0.03) AS `V872`, (`V873` < 0.03) AS `V873`, (`V874` < 0.03) AS `V874`, (`V875` < 0.03) AS `V875`, (`V876` < 0.03) AS `V876`, (`V877` < 0.03) AS `V877`, (`V878` < 0.03) AS `V878`, (`V879` < 0.03) AS `V879`, (`V880` < 0.03) AS `V880`, (`V881` < 0.03) AS `V881`, (`V882` < 0.03) AS `V882`, (`V883` < 0.03) AS `V883`, (`V884` < 0.03) AS `V884`, (`V885` < 0.03) AS `V885`, (`V886` < 0.03) AS `V886`, (`V887` < 0.03) AS `V887`, (`V888` < 0.03) AS `V888`, (`V889` < 0.03) AS `V889`, (`V890` < 0.03) AS `V890`, (`V891` < 0.03) AS `V891`, (`V892` < 0.03) AS `V892`, (`V893` < 0.03) AS `V893`, (`V894` < 0.03) AS `V894`, (`V895` < 0.03) AS `V895`, (`V896` < 0.03) AS `V896`, (`V897` < 0.03) AS `V897`, (`V898` < 0.03) AS `V898`, (`V899` < 0.03) AS `V899`, (`V900` < 0.03) AS `V900`, (`V901` < 0.03) AS `V901`, (`V902` < 0.03) AS `V902`, (`V903` < 0.03) AS `V903`, (`V904` < 0.03) AS `V904`, (`V905` < 0.03) AS `V905`, (`V906` < 0.03) AS `V906`, (`V907` < 0.03) AS `V907`, (`V908` < 0.03) AS `V908`, (`V909` < 0.03) AS `V909`, (`V910` < 0.03) AS `V910`, (`V911` < 0.03) AS `V911`, (`V912` < 0.03) AS `V912`, (`V913` < 0.03) AS `V913`, (`V914` < 0.03) AS `V914`, (`V915` < 0.03) AS `V915`, (`V916` < 0.03) AS `V916`, (`V917` < 0.03) AS `V917`, (`V918` < 0.03) AS `V918`, (`V919` < 0.03) AS `V919`, (`V920` < 0.03) AS `V920`, (`V921` < 0.03) AS `V921`, (`V922` < 0.03) AS `V922`, (`V923` < 0.03) AS `V923`, (`V924` < 0.03) AS `V924`, (`V925` < 0.03) AS `V925`, (`V926` < 0.03) AS `V926`, (`V927` < 0.03) AS `V927`, (`V928` < 0.03) AS `V928`, (`V929` < 0.03) AS `V929`, (`V930` < 0.03) AS `V930`, (`V931` < 0.03) AS `V931`, (`V932` < 0.03) AS `V932`, (`V933` < 0.03) AS `V933`, (`V934` < 0.03) AS `V934`, (`V935` < 0.03) AS `V935`, (`V936` < 0.03) AS `V936`, (`V937` < 0.03) AS `V937`, (`V938` < 0.03) AS `V938`, (`V939` < 0.03) AS `V939`, (`V940` < 0.03) AS `V940`, (`V941` < 0.03) AS `V941`, (`V942` < 0.03) AS `V942`, (`V943` < 0.03) AS `V943`, (`V944` < 0.03) AS `V944`, (`V945` < 0.03) AS `V945`, (`V946` < 0.03) AS `V946`, (`V947` < 0.03) AS `V947`, (`V948` < 0.03) AS `V948`, (`V949` < 0.03) AS `V949`, (`V950` < 0.03) AS `V950`, (`V951` < 0.03) AS `V951`, (`V952` < 0.03) AS `V952`, (`V953` < 0.03) AS `V953`, (`V954` < 0.03) AS `V954`, (`V955` < 0.03) AS `V955`, (`V956` < 0.03) AS `V956`, (`V957` < 0.03) AS `V957`, (`V958` < 0.03) AS `V958`, (`V959` < 0.03) AS `V959`, (`V960` < 0.03) AS `V960`, (`V961` < 0.03) AS `V961`, (`V962` < 0.03) AS `V962`, (`V963` < 0.03) AS `V963`, (`V964` < 0.03) AS `V964`, (`V965` < 0.03) AS `V965`, (`V966` < 0.03) AS `V966`, (`V967` < 0.03) AS `V967`, (`V968` < 0.03) AS `V968`, (`V969` < 0.03) AS `V969`, (`V970` < 0.03) AS `V970`, (`V971` < 0.03) AS `V971`, (`V972` < 0.03) AS `V972`, (`V973` < 0.03) AS `V973`, (`V974` < 0.03) AS `V974`, (`V975` < 0.03) AS `V975`, (`V976` < 0.03) AS `V976`, (`V977` < 0.03) AS `V977`, (`V978` < 0.03) AS `V978`, (`V979` < 0.03) AS `V979`, (`V980` < 0.03) AS `V980`, (`V981` < 0.03) AS `V981`, (`V982` < 0.03) AS `V982`, (`V983` < 0.03) AS `V983`, (`V984` < 0.03) AS `V984`, (`V985` < 0.03) AS `V985`, (`V986` < 0.03) AS `V986`, (`V987` < 0.03) AS `V987`, (`V988` < 0.03) AS `V988`, (`V989` < 0.03) AS `V989`, (`V990` < 0.03) AS `V990`, (`V991` < 0.03) AS `V991`, (`V992` < 0.03) AS `V992`, (`V993` < 0.03) AS `V993`, (`V994` < 0.03) AS `V994`, (`V995` < 0.03) AS `V995`, (`V996` < 0.03) AS `V996`, (`V997` < 0.03) AS `V997`, (`V998` < 0.03) AS `V998`, (`V999` < 0.03) AS `V999`, (`V1000` < 0.03) AS `V1000`
FROM `analyis_tbl`
17/12/20 21:59:25 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 127.0.0.1:55541 in memory (size: 189.3 KB, free: 1870.8 MB)
17/12/20 21:59:26 INFO SparkSqlParser: Parsing command: analyis_tbl
17/12/20 21:59:26 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:59:26 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl` AS `zzz17`
WHERE (0 = 1)
17/12/20 21:59:26 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 21:59:26 INFO SparkSqlParser: Parsing command: SELECT *
FROM `analyis_tbl`
17/12/20 21:59:26 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
17/12/20 21:59:27 INFO SparkContext: Starting job: collect at utils.scala:196
17/12/20 21:59:27 INFO DAGScheduler: Got job 4 (collect at utils.scala:196) with 2 output partitions
17/12/20 21:59:27 INFO DAGScheduler: Final stage: ResultStage 10 (collect at utils.scala:196)
17/12/20 21:59:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
17/12/20 21:59:27 INFO DAGScheduler: Missing parents: List()
17/12/20 21:59:27 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[38] at collect at utils.scala:196), which has no missing parents
17/12/20 21:59:27 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 1064.2 KB, free 1869.5 MB)
17/12/20 21:59:27 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 279.5 KB, free 1869.2 MB)
17/12/20 21:59:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:55541 (size: 279.5 KB, free: 1870.5 MB)
17/12/20 21:59:27 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:996
17/12/20 21:59:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[38] at collect at utils.scala:196)
17/12/20 21:59:27 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
17/12/20 21:59:27 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 5945 bytes)
17/12/20 21:59:27 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 5945 bytes)
17/12/20 21:59:27 INFO Executor: Running task 0.0 in stage 10.0 (TID 9)
17/12/20 21:59:27 INFO Executor: Running task 1.0 in stage 10.0 (TID 10)
17/12/20 21:59:27 INFO BlockManager: Found block rdd_31_0 locally
17/12/20 21:59:27 INFO BlockManager: Found block rdd_12_1 locally
17/12/20 21:59:27 INFO CodeGenerator: Code generated in 229.63036 ms
17/12/20 21:59:27 INFO CodeGenerator: Code generated in 316.73752 ms
17/12/20 21:59:30 WARN CodeGenerator: Error calculating stats of compiled class.
java.io.EOFException
	at java.io.DataInputStream.readFully(Unknown Source)
	at java.io.DataInputStream.readFully(Unknown Source)
	at org.codehaus.janino.util.ClassFile.loadAttribute(ClassFile.java:1509)
	at org.codehaus.janino.util.ClassFile.loadAttributes(ClassFile.java:644)
	at org.codehaus.janino.util.ClassFile.loadFields(ClassFile.java:623)
	at org.codehaus.janino.util.ClassFile.<init>(ClassFile.java:280)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anonfun$recordCompilationStats$1.apply(CodeGenerator.scala:967)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anonfun$recordCompilationStats$1.apply(CodeGenerator.scala:964)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.recordCompilationStats(CodeGenerator.scala:964)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:936)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:998)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:995)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:890)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:405)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:359)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:874)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.extractProjection$lzycompute(ExpressionEncoder.scala:266)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.extractProjection(ExpressionEncoder.scala:266)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:547)
	at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:547)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:232)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
17/12/20 21:59:30 INFO CodeGenerator: Code generated in 2380.058971 ms
17/12/20 21:59:39 INFO Executor: Finished task 0.0 in stage 10.0 (TID 9). 1008327 bytes result sent to driver
17/12/20 21:59:39 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 12397 ms on localhost (executor driver) (1/2)
17/12/20 22:02:25 INFO MemoryStore: Block rdd_31_1 stored as values in memory (estimated size 133.7 MB, free 1735.5 MB)
17/12/20 22:02:25 INFO BlockManagerInfo: Added rdd_31_1 in memory on 127.0.0.1:55541 (size: 133.7 MB, free: 1736.9 MB)
17/12/20 22:02:32 INFO Executor: Finished task 1.0 in stage 10.0 (TID 10). 1005724 bytes result sent to driver
17/12/20 22:02:32 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 10) in 185514 ms on localhost (executor driver) (2/2)
17/12/20 22:02:32 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
17/12/20 22:02:32 INFO DAGScheduler: ResultStage 10 (collect at utils.scala:196) finished in 185.514 s
17/12/20 22:02:32 INFO DAGScheduler: Job 4 finished: collect at utils.scala:196, took 185.537679 s
17/12/20 22:02:32 INFO CodeGenerator: Code generated in 87.879691 ms
17/12/20 22:02:34 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 22:02:34 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 22:02:34 INFO HiveMetaStore: 0: get_database: default
17/12/20 22:02:34 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 22:02:34 INFO HiveMetaStore: 0: get_database: default
17/12/20 22:02:34 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 22:02:34 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 22:02:34 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 22:02:34 INFO CodeGenerator: Code generated in 4.86513 ms
17/12/20 22:02:34 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 22:02:34 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 22:02:34 INFO HiveMetaStore: 0: get_database: default
17/12/20 22:02:35 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 22:02:35 INFO HiveMetaStore: 0: get_database: default
17/12/20 22:02:35 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 22:02:35 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 22:02:35 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 22:02:35 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 22:02:35 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 22:02:35 INFO HiveMetaStore: 0: get_database: default
17/12/20 22:02:35 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 22:02:35 INFO HiveMetaStore: 0: get_database: default
17/12/20 22:02:35 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 22:02:35 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 22:02:35 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 22:02:35 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/12/20 22:02:35 INFO SparkSqlParser: Parsing command: SHOW TABLES
17/12/20 22:02:35 INFO HiveMetaStore: 0: get_database: default
17/12/20 22:02:35 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 22:02:35 INFO HiveMetaStore: 0: get_database: default
17/12/20 22:02:35 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_database: default	
17/12/20 22:02:35 INFO HiveMetaStore: 0: get_tables: db=default pat=*
17/12/20 22:02:35 INFO audit: ugi=conan	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
17/12/20 22:07:33 INFO SparkContext: Invoking stop() from shutdown hook
17/12/20 22:07:33 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/12/20 22:07:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/12/20 22:07:34 INFO MemoryStore: MemoryStore cleared
17/12/20 22:07:34 INFO BlockManager: BlockManager stopped
17/12/20 22:07:34 INFO BlockManagerMaster: BlockManagerMaster stopped
17/12/20 22:07:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/12/20 22:07:34 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-e0657139-e685-443e-bb73-8129febf2cbf\userFiles-df549e01-61c0-4eb9-84b0-a2b84c9f1994
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-e0657139-e685-443e-bb73-8129febf2cbf\userFiles-df549e01-61c0-4eb9-84b0-a2b84c9f1994
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:102)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1842)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1841)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 22:07:34 INFO SparkContext: Successfully stopped SparkContext
17/12/20 22:07:34 INFO ShutdownHookManager: Shutdown hook called
17/12/20 22:07:34 INFO ShutdownHookManager: Deleting directory C:\Users\conan\AppData\Local\Temp\spark-e0657139-e685-443e-bb73-8129febf2cbf\userFiles-df549e01-61c0-4eb9-84b0-a2b84c9f1994
17/12/20 22:07:34 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-e0657139-e685-443e-bb73-8129febf2cbf\userFiles-df549e01-61c0-4eb9-84b0-a2b84c9f1994
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-e0657139-e685-443e-bb73-8129febf2cbf\userFiles-df549e01-61c0-4eb9-84b0-a2b84c9f1994
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/12/20 22:07:34 INFO ShutdownHookManager: Deleting directory C:\Users\conan\AppData\Local\Temp\spark-e0657139-e685-443e-bb73-8129febf2cbf
17/12/20 22:07:34 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\conan\AppData\Local\Temp\spark-e0657139-e685-443e-bb73-8129febf2cbf
java.io.IOException: Failed to delete: C:\Users\conan\AppData\Local\Temp\spark-e0657139-e685-443e-bb73-8129febf2cbf
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
